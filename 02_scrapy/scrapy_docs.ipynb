{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "968ee719-bf68-4e29-9408-ec844121f4c9",
   "metadata": {},
   "source": [
    "# SCRAPY [DOCUMENTATION](https://docs.scrapy.org/en/latest/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01be5cd1-1600-4414-a5de-aab700913727",
   "metadata": {},
   "source": [
    "Scrapy is a fast high-level web crawling and web scraping framework, used to crawl websites and extract structured data from their pages. It can be used for a wide range of purposes, from data mining to monitoring and automated testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920f46d7-496c-4360-b7a5-451108e6d521",
   "metadata": {},
   "source": [
    "# Commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dcc10f-fcb3-4ac9-8f77-872712283f6c",
   "metadata": {},
   "source": [
    "## Scrapy command line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b22b9f-ec2a-4403-b3d9-806eeaa5c25c",
   "metadata": {},
   "source": [
    "|Scrapy command line|Description|\n",
    "|-|-|\n",
    "|**Help**||\n",
    "|`scrapy -h`|(`--help`) list all the available commands (run from the project's directory)|\n",
    "|`scrapy <command> -h`|help on the given command|\n",
    "|**List info**||\n",
    "|`scrapy list`|list all available crawlers (run from the project's deirectory)|\n",
    "|**Crawl a random webpage in shell**||\n",
    "|`scrapy shell <url>`||\n",
    "|(inside shell) `view(response)`|open the resonse object in your browser|\n",
    "|**Project**||\n",
    "|`scrapy startproject project_name [project_dir]`|create a new project|\n",
    "|`scrapy genspider [-t template] <name> <domain or URL>`|Create a new spider in the current folder or in the current project’s spiders folder,|\n",
    "|`scrapy crawl spider_name`|run the spider (from the project's top level dir)|\n",
    "|`scrapy crawl spider_name -s JOBDIR=crawls/spider_name-1`|pausing and resuming crawls|\n",
    "|||\n",
    "|||\n",
    "|||\n",
    "\n",
    "```\n",
    "scrapy --help\n",
    "Scrapy 2.11.0 - active project: tutorial\n",
    "\n",
    "Usage:\n",
    "  scrapy <command> [options] [args]\n",
    "\n",
    "Available commands:\n",
    "  bench         Run quick benchmark test\n",
    "  check         Check spider contracts\n",
    "  crawl         Run a spider\n",
    "  edit          Edit spider\n",
    "  fetch         Fetch a URL using the Scrapy downloader\n",
    "  genspider     Generate new spider using pre-defined templates\n",
    "  list          List available spiders\n",
    "  parse         Parse URL (using its spider) and print the results\n",
    "  runspider     Run a self-contained spider (without creating a project)\n",
    "  settings      Get settings values\n",
    "  shell         Interactive scraping console\n",
    "  startproject  Create new project\n",
    "  version       Print Scrapy version\n",
    "  view          Open URL in browser, as seen by Scrapy\n",
    "\n",
    "Use \"scrapy <command> -h\" to see more info about a command\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d33a9d-eda2-4f44-b287-6512d7cef5ff",
   "metadata": {},
   "source": [
    "## Scrapy extraction most common tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558e8530-49b4-4ec5-b347-b719697157af",
   "metadata": {},
   "source": [
    "|Scrapy extraction tools|Description|\n",
    "|-|-|\n",
    "|`view(response)`|open the response page from the shell in your web browser|\n",
    "|**Response status codes**||\n",
    "|`response.status`||\n",
    "|**CSS selectors**||\n",
    "|`response.css`||\n",
    "|`response.css(\"title::text\").getall()`|get only text from the SelectorList|\n",
    "|||\n",
    "|||\n",
    "|||"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00a0466-e563-49de-a9d1-b380ced0c1f9",
   "metadata": {},
   "source": [
    "# <b>1. Scrapy tutorial</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1eb925-6bb2-4b85-8261-56c0b924f4b6",
   "metadata": {},
   "source": [
    "This tutorial will walk you through these tasks:\n",
    "\n",
    "- Creating a new Scrapy project\n",
    "- Writing a spider to crawl a site and extract data\n",
    "- Exporting the scraped data using the command line\n",
    "- Changing spider to recursively follow links\n",
    "- Using spider arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ca1bcf-ddc7-4728-8b21-6f94070a72b1",
   "metadata": {},
   "source": [
    "# 1.0 Installation guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe05960c-b29b-4731-aae8-35713f2b0606",
   "metadata": {},
   "source": [
    "We strongly recommend that you install Scrapy in a dedicated virtualenv, to avoid conflicting with your system packages.\n",
    "\n",
    "```sh\n",
    "(venv) $ pip install Scrapy\n",
    "```\n",
    "\n",
    "Scrapy is written in pure Python and depends on a few key Python packages (among others):\n",
    "\n",
    "- lxml, an efficient XML and HTML parser\n",
    "- parsel, an HTML/XML data extraction library written on top of lxml,\n",
    "- w3lib, a multi-purpose helper for dealing with URLs and web page encodings\n",
    "- twisted, an asynchronous networking framework\n",
    "- cryptography and pyOpenSSL, to deal with various network-level security needs\n",
    "\n",
    "Some of these packages themselves depend on non-Python packages that might require additional installation steps depending on your platform. Please check [platform-specific guides](https://docs.scrapy.org/en/latest/intro/install.html#intro-install-platform-notes).\n",
    "\n",
    "In case of any trouble related to these dependencies, please refer to their respective installation instructions:\n",
    "\n",
    "- [lxml installation](https://lxml.de/installation.html)\n",
    "- [cryptography installation](https://cryptography.io/en/latest/installation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122d7112-8b38-4950-a190-6272f74f6903",
   "metadata": {},
   "source": [
    "# 1.1 Creating a project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7312e67a-4e29-47dd-9799-01db5661b765",
   "metadata": {},
   "source": [
    "Before you start scraping, you will have to set up a new Scrapy project. Enter a directory where you’d like to store your code and run:\n",
    "\n",
    "```sh\n",
    "scrapy startproject tutorial\n",
    "```\n",
    "```\n",
    "New Scrapy project 'tutorial', using template directory '/home/commi/venv/venv3.11/lib/python3.11/site-packages/scrapy/templates/project', created in:\n",
    "    /home/commi/Yandex.Disk/it_learning/08_web_scraping/02_scrapy/data/tutorial\n",
    "\n",
    "You can start your first spider with:\n",
    "    cd tutorial\n",
    "    scrapy genspider example example.com\n",
    "```\n",
    "\n",
    "This will create a `tutorial` directory with the following contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8e616c4-6e21-4f23-8aef-6a47dd54ed32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-04T20:41:14.321005Z",
     "iopub.status.busy": "2024-02-04T20:41:14.320026Z",
     "iopub.status.idle": "2024-02-04T20:41:14.486706Z",
     "shell.execute_reply": "2024-02-04T20:41:14.484683Z",
     "shell.execute_reply.started": "2024-02-04T20:41:14.320929Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtutorial\u001b[0m\n",
      "├── scrapy.cfg\n",
      "└── \u001b[01;34mtutorial\u001b[0m\n",
      "    ├── __init__.py\n",
      "    ├── items.py\n",
      "    ├── middlewares.py\n",
      "    ├── pipelines.py\n",
      "    ├── settings.py\n",
      "    └── \u001b[01;34mspiders\u001b[0m\n",
      "        └── __init__.py\n",
      "\n",
      "3 directories, 7 files\n"
     ]
    }
   ],
   "source": [
    "cd ./data\n",
    "tree tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f47d0d-42a0-45d1-81bc-19405b2282cd",
   "metadata": {},
   "source": [
    "```\n",
    "tutorial/\n",
    "    scrapy.cfg            # deploy configuration file\n",
    "\n",
    "    tutorial/             # project's Python module, you'll import your code from here\n",
    "        __init__.py\n",
    "\n",
    "        items.py          # project items definition file\n",
    "\n",
    "        middlewares.py    # project middlewares file\n",
    "\n",
    "        pipelines.py      # project pipelines file\n",
    "\n",
    "        settings.py       # project settings file\n",
    "\n",
    "        spiders/          # a directory where you'll later put your spiders\n",
    "            __init__.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4fa774-eea7-4e39-992d-743fb81519f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1.2 Our first Spider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1932bf53-e3c9-4209-9533-3590627dd4b7",
   "metadata": {},
   "source": [
    "**Spiders** are classes that you define and that Scrapy uses to scrape information from a website (or a group of websites). They must subclass `Spider` and define the initial requests to make, optionally \n",
    "- how to follow links in the pages, and \n",
    "- how to parse the downloaded page content to extract data.\n",
    "\n",
    "This is the code for our first `Spider`. Save it in a file named `quotes_spider.py` under the `tutorial/spiders` directory in your project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d31f0595-b402-48ee-8f1b-fbffacb2d778",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-04T20:43:53.627476Z",
     "iopub.status.busy": "2024-02-04T20:43:53.625433Z",
     "iopub.status.idle": "2024-02-04T20:43:53.742875Z",
     "shell.execute_reply": "2024-02-04T20:43:53.741806Z",
     "shell.execute_reply.started": "2024-02-04T20:43:53.627403Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".:\n",
      "draft.py  \u001b[0m\u001b[01;34m__pycache__\u001b[0m  quotes.jsonl  quotes_spider.py  \u001b[01;34mtutorial\u001b[0m\n",
      "\n",
      "./__pycache__:\n",
      "draft.cpython-311.pyc\n",
      "\n",
      "./tutorial:\n",
      "scrapy.cfg  \u001b[01;34mtutorial\u001b[0m\n",
      "\n",
      "./tutorial/tutorial:\n",
      "__init__.py  items.py  middlewares.py  pipelines.py  settings.py  \u001b[01;34mspiders\u001b[0m\n",
      "\n",
      "./tutorial/tutorial/spiders:\n",
      "__init__.py\n"
     ]
    }
   ],
   "source": [
    "ls -R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fd6194-8510-423d-b568-2aedbd634900",
   "metadata": {},
   "source": [
    "```python\n",
    "from pathlib import Path\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            \"https://quotes.toscrape.com/page/1/\",\n",
    "            \"https://quotes.toscrape.com/page/2/\",\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = f\"quotes-{page}.html\"\n",
    "        Path(filename).write_bytes(response.body)\n",
    "        self.log(f\"Saved file {filename}\")\n",
    "```\n",
    "\n",
    "As you can see, our Spider subclasses `scrapy.Spider` and defines some attributes and methods:\n",
    "\n",
    "- `name`: identifies the Spider. It **must be unique within a project**, that is, you can’t set the same name for different Spiders.\n",
    "\n",
    "- `start_requests()`: must return an iterable of `Request`s (you can return a **list** of requests or write a **generator** function) which the Spider will begin to crawl from. Subsequent requests will be generated successively from these initial requests.\n",
    "\n",
    "- `parse()`: a method that will be called to handle the response downloaded for each of the requests made. The `response` parameter is an instance of `TextResponse` that holds the page content and has further helpful methods to handle it.\n",
    "\n",
    "The `parse()` method usually parses the `response`, extracting the scraped data as dicts and also finding new URLs to follow and creating new requests (`Request`) from them.\n",
    "\n",
    "_ChatGPT:_  \n",
    "In Scrapy, when you use the `yield` statement within a Spider callback method like `parse`, the yielded items are not stored directly. Instead, they are processed by the Scrapy framework, typically passed to [Item Pipeline](#2.7-Item-Pipeline) components.\n",
    "\n",
    "Here's what happens when you yield items in Scrapy:\n",
    "- When you `yield` an item from a Spider callback method like `parse`, Scrapy will send that item to the Item Pipeline.\n",
    "- The Item Pipeline is a mechanism for processing the items scraped by the Spider. It allows you to perform various tasks on the scraped items, such as cleaning, validation, and persistence.\n",
    "- Each item that is `yield`ed is processed through the Item Pipeline sequentially, allowing you to define various stages of processing.\n",
    "- The Item Pipeline can perform operations like validation and transformation on the items before they are saved to a storage backend such as a database, JSON file, or CSV file.\n",
    "- You can define your own custom Item Pipeline to process the scraped items according to your requirements.\n",
    "\n",
    "So, in summary, the yielded data from a Scrapy Spider is not stored directly within the Spider itself; instead, it's passed through the Item Pipeline for further processing and eventual storage.\n",
    "\n",
    "In Scrapy, you should use the `yield` statement instead of creating a list to output items because Scrapy operates asynchronously and uses generators for item processing. There are several reasons why `yield` is preferred over creating lists:\n",
    "\n",
    "- **Memory Efficiency**: Using `yield` allows Scrapy to process items one at a time as they are generated, which is more memory-efficient compared to storing all items in a list. This is particularly beneficial when dealing with a large number of items or when scraping large websites.\n",
    "\n",
    "- **Asynchronous Processing**: Scrapy is built on top of Twisted, an asynchronous networking framework. By using generators and yield, Scrapy can process items asynchronously, allowing it to handle multiple requests and responses concurrently. This improves the overall performance and scalability of your spider.\n",
    "\n",
    "- **Streaming Response**: When you `yield` items in Scrapy, they are immediately sent to downstream components such as pipelines or middleware for further processing. This enables a streaming-like behavior where items can be processed in real-time as they are generated, rather than waiting for the entire list to be created.\n",
    "\n",
    "- **Pipeline Integration**: Scrapy pipelines expect items to be yielded one by one. By using yield, you can seamlessly integrate your spider with pipelines to perform various tasks such as data validation, cleaning, and storing without having to manage lists of items manually.\n",
    "\n",
    "- **Reduced Complexity**: Using `yield` simplifies the code and makes it more readable compared to managing lists of items. It follows the principle of generator-based programming, where you produce items as needed, resulting in cleaner and more maintainable code.\n",
    "\n",
    "Overall, using yield in Scrapy spiders aligns with the framework's design principles and asynchronous nature, leading to better performance, scalability, and code clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93f33ea-00e0-464f-a5c2-35378c70e7af",
   "metadata": {},
   "source": [
    "## How to run our spider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b388f81-c947-4fea-95f8-4a94758c5fd1",
   "metadata": {},
   "source": [
    "To put our spider to work, go to the project’s top level directory and run:\n",
    "\n",
    "```sh\n",
    "scrapy crawl quotes\n",
    "```\n",
    "This command runs the spider with name `quotes` that we’ve just added, that will send some requests for the `quotes.toscrape.com` domain. You will get an output similar to this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e7e96b-75f6-4259-9664-5ddc3d3c8bc6",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "2024-02-05 01:57:09 [scrapy.utils.log] INFO: Scrapy 2.11.0 started (bot: tutorial)\n",
    "2024-02-05 01:57:09 [scrapy.utils.log] INFO: Versions: lxml 5.1.0.0, libxml2 2.12.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.11.2 (main, Mar 13 2023, 12:18:29) [GCC 12.2.0], pyOpenSSL 24.0.0 (OpenSSL 3.2.1 30 Jan 2024), cryptography 42.0.2, Platform Linux-6.1.0-17-amd64-x86_64-with-glibc2.36\n",
    "2024-02-05 01:57:09 [scrapy.addons] INFO: Enabled addons:\n",
    "[]\n",
    "2024-02-05 01:57:09 [asyncio] DEBUG: Using selector: EpollSelector\n",
    "2024-02-05 01:57:09 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
    "2024-02-05 01:57:09 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
    "2024-02-05 01:57:09 [scrapy.extensions.telnet] INFO: Telnet Password: b9a7a03404bf04d7\n",
    "2024-02-05 01:57:09 [scrapy.middleware] INFO: Enabled extensions:\n",
    "['scrapy.extensions.corestats.CoreStats',\n",
    " 'scrapy.extensions.telnet.TelnetConsole',\n",
    " 'scrapy.extensions.memusage.MemoryUsage',\n",
    " 'scrapy.extensions.logstats.LogStats']\n",
    "2024-02-05 01:57:09 [scrapy.crawler] INFO: Overridden settings:\n",
    "{'BOT_NAME': 'tutorial',\n",
    " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
    " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
    " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
    " 'ROBOTSTXT_OBEY': True,\n",
    " 'SPIDER_MODULES': ['tutorial.spiders'],\n",
    " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
    "2024-02-05 01:57:09 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
    "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
    " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
    " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
    " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
    " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
    " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
    " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
    " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
    " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
    " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
    " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
    " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
    "2024-02-05 01:57:09 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
    "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
    " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
    " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
    " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
    " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
    "2024-02-05 01:57:09 [scrapy.middleware] INFO: Enabled item pipelines:\n",
    "[]\n",
    "2024-02-05 01:57:09 [scrapy.core.engine] INFO: Spider opened\n",
    "2024-02-05 01:57:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
    "2024-02-05 01:57:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
    "2024-02-05 01:57:09 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://quotes.toscrape.com/robots.txt> (referer: None)\n",
    "2024-02-05 01:57:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/1/> (referer: None)\n",
    "2024-02-05 01:57:10 [quotes] DEBUG: Saved file quotes-1.html\n",
    "2024-02-05 01:57:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/2/> (referer: None)\n",
    "2024-02-05 01:57:10 [quotes] DEBUG: Saved file quotes-2.html\n",
    "2024-02-05 01:57:10 [scrapy.core.engine] INFO: Closing spider (finished)\n",
    "2024-02-05 01:57:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
    "{'downloader/request_bytes': 684,\n",
    " 'downloader/request_count': 3,\n",
    " 'downloader/request_method_count/GET': 3,\n",
    " 'downloader/response_bytes': 25556,\n",
    " 'downloader/response_count': 3,\n",
    " 'downloader/response_status_count/200': 2,\n",
    " 'downloader/response_status_count/404': 1,\n",
    " 'elapsed_time_seconds': 1.235283,\n",
    " 'finish_reason': 'finished',\n",
    " 'finish_time': datetime.datetime(2024, 2, 4, 20, 57, 10, 482312, tzinfo=datetime.timezone.utc),\n",
    " 'log_count/DEBUG': 8,\n",
    " 'log_count/INFO': 10,\n",
    " 'memusage/max': 65585152,\n",
    " 'memusage/startup': 65585152,\n",
    " 'response_received_count': 3,\n",
    " 'robotstxt/request_count': 1,\n",
    " 'robotstxt/response_count': 1,\n",
    " 'robotstxt/response_status_count/404': 1,\n",
    " 'scheduler/dequeued': 2,\n",
    " 'scheduler/dequeued/memory': 2,\n",
    " 'scheduler/enqueued': 2,\n",
    " 'scheduler/enqueued/memory': 2,\n",
    " 'start_time': datetime.datetime(2024, 2, 4, 20, 57, 9, 247029, tzinfo=datetime.timezone.utc)}\n",
    "2024-02-05 01:57:10 [scrapy.core.engine] INFO: Spider closed (finished)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9498b6dc-d419-43a6-975f-c2fe9809be7a",
   "metadata": {},
   "source": [
    "Now, check the files in the current directory. You should notice that two new files have been created: \n",
    "- quotes-1.html and \n",
    "- quotes-2.html, \n",
    "\n",
    "with the content for the respective URLs, as our parse method instructs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "898572ae-7c3c-4152-9ec0-c516d00087ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-04T22:18:03.312774Z",
     "iopub.status.busy": "2024-02-04T22:18:03.312022Z",
     "iopub.status.idle": "2024-02-04T22:18:03.422712Z",
     "shell.execute_reply": "2024-02-04T22:18:03.421645Z",
     "shell.execute_reply.started": "2024-02-04T22:18:03.312747Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotes-1.html  quotes-2.html  scrapy.cfg  \u001b[0m\u001b[01;34mtutorial\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ls ./tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a184ab66-4896-47d7-9c7e-1643e6ca04b5",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-02-04T22:18:41.703321Z",
     "iopub.status.busy": "2024-02-04T22:18:41.702513Z",
     "iopub.status.idle": "2024-02-04T22:18:41.914813Z",
     "shell.execute_reply": "2024-02-04T22:18:41.913439Z",
     "shell.execute_reply.started": "2024-02-04T22:18:41.703251Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "<head>\n",
      "\t<meta charset=\"UTF-8\">\n",
      "\t<title>Quotes to Scrape</title>\n",
      "    <link rel=\"stylesheet\" href=\"/static/bootstrap.min.css\">\n",
      "    <link rel=\"stylesheet\" href=\"/static/main.css\">\n",
      "</head>\n",
      "<body>\n",
      "    <div class=\"container\">\n",
      "        <div class=\"row header-box\">\n",
      "            <div class=\"col-md-8\">\n",
      "                <h1>\n",
      "                    <a href=\"/\" style=\"text-decoration: none\">Quotes to Scrape</a>\n",
      "                </h1>\n",
      "            </div>\n",
      "            <div class=\"col-md-4\">\n",
      "                <p>\n",
      "                \n",
      "                    <a href=\"/login\">Login</a>\n",
      "                \n",
      "                </p>\n",
      "            </div>\n",
      "        </div>\n",
      "    \n",
      "\n",
      "<div class=\"row\">\n",
      "    <div class=\"col-md-8\">\n",
      "\n",
      "    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n",
      "        <span class=\"text\" itemprop=\"text\">“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”</span>\n",
      "        <span>by <small class=\"author\" itemprop=\"author\">Albert Einstein</small>\n",
      "        <a href=\"/author/Albert-Einstein\">(about)</a>\n",
      "        </span>\n",
      "        <div class=\"tags\">\n",
      "            Tags:\n",
      "            <meta class=\"keywords\" itemprop=\"keywords\" content=\"change,deep-thoughts,thinking,world\" /    > \n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/change/page/1/\">change</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/deep-thoughts/page/1/\">deep-thoughts</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/thinking/page/1/\">thinking</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/world/page/1/\">world</a>\n",
      "            \n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n",
      "        <span class=\"text\" itemprop=\"text\">“It is our choices, Harry, that show what we truly are, far more than our abilities.”</span>\n",
      "        <span>by <small class=\"author\" itemprop=\"author\">J.K. Rowling</small>\n",
      "        <a href=\"/author/J-K-Rowling\">(about)</a>\n",
      "        </span>\n",
      "        <div class=\"tags\">\n",
      "            Tags:\n",
      "            <meta class=\"keywords\" itemprop=\"keywords\" content=\"abilities,choices\" /    > \n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/abilities/page/1/\">abilities</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/choices/page/1/\">choices</a>\n",
      "            \n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n",
      "        <span class=\"text\" itemprop=\"text\">“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”</span>\n",
      "        <span>by <small class=\"author\" itemprop=\"author\">Albert Einstein</small>\n",
      "        <a href=\"/author/Albert-Einstein\">(about)</a>\n",
      "        </span>\n",
      "        <div class=\"tags\">\n",
      "            Tags:\n",
      "            <meta class=\"keywords\" itemprop=\"keywords\" content=\"inspirational,life,live,miracle,miracles\" /    > \n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/inspirational/page/1/\">inspirational</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/life/page/1/\">life</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/live/page/1/\">live</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/miracle/page/1/\">miracle</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/miracles/page/1/\">miracles</a>\n",
      "            \n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n",
      "        <span class=\"text\" itemprop=\"text\">“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”</span>\n",
      "        <span>by <small class=\"author\" itemprop=\"author\">Jane Austen</small>\n",
      "        <a href=\"/author/Jane-Austen\">(about)</a>\n",
      "        </span>\n",
      "        <div class=\"tags\">\n",
      "            Tags:\n",
      "            <meta class=\"keywords\" itemprop=\"keywords\" content=\"aliteracy,books,classic,humor\" /    > \n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/aliteracy/page/1/\">aliteracy</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/books/page/1/\">books</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/classic/page/1/\">classic</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/humor/page/1/\">humor</a>\n",
      "            \n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n",
      "        <span class=\"text\" itemprop=\"text\">“Imperfection is beauty, madness is genius and it&#39;s better to be absolutely ridiculous than absolutely boring.”</span>\n",
      "        <span>by <small class=\"author\" itemprop=\"author\">Marilyn Monroe</small>\n",
      "        <a href=\"/author/Marilyn-Monroe\">(about)</a>\n",
      "        </span>\n",
      "        <div class=\"tags\">\n",
      "            Tags:\n",
      "            <meta class=\"keywords\" itemprop=\"keywords\" content=\"be-yourself,inspirational\" /    > \n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/be-yourself/page/1/\">be-yourself</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/inspirational/page/1/\">inspirational</a>\n",
      "            \n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n",
      "        <span class=\"text\" itemprop=\"text\">“Try not to become a man of success. Rather become a man of value.”</span>\n",
      "        <span>by <small class=\"author\" itemprop=\"author\">Albert Einstein</small>\n",
      "        <a href=\"/author/Albert-Einstein\">(about)</a>\n",
      "        </span>\n",
      "        <div class=\"tags\">\n",
      "            Tags:\n",
      "            <meta class=\"keywords\" itemprop=\"keywords\" content=\"adulthood,success,value\" /    > \n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/adulthood/page/1/\">adulthood</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/success/page/1/\">success</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/value/page/1/\">value</a>\n",
      "            \n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n",
      "        <span class=\"text\" itemprop=\"text\">“It is better to be hated for what you are than to be loved for what you are not.”</span>\n",
      "        <span>by <small class=\"author\" itemprop=\"author\">André Gide</small>\n",
      "        <a href=\"/author/Andre-Gide\">(about)</a>\n",
      "        </span>\n",
      "        <div class=\"tags\">\n",
      "            Tags:\n",
      "            <meta class=\"keywords\" itemprop=\"keywords\" content=\"life,love\" /    > \n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/life/page/1/\">life</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/love/page/1/\">love</a>\n",
      "            \n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n",
      "        <span class=\"text\" itemprop=\"text\">“I have not failed. I&#39;ve just found 10,000 ways that won&#39;t work.”</span>\n",
      "        <span>by <small class=\"author\" itemprop=\"author\">Thomas A. Edison</small>\n",
      "        <a href=\"/author/Thomas-A-Edison\">(about)</a>\n",
      "        </span>\n",
      "        <div class=\"tags\">\n",
      "            Tags:\n",
      "            <meta class=\"keywords\" itemprop=\"keywords\" content=\"edison,failure,inspirational,paraphrased\" /    > \n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/edison/page/1/\">edison</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/failure/page/1/\">failure</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/inspirational/page/1/\">inspirational</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/paraphrased/page/1/\">paraphrased</a>\n",
      "            \n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n",
      "        <span class=\"text\" itemprop=\"text\">“A woman is like a tea bag; you never know how strong it is until it&#39;s in hot water.”</span>\n",
      "        <span>by <small class=\"author\" itemprop=\"author\">Eleanor Roosevelt</small>\n",
      "        <a href=\"/author/Eleanor-Roosevelt\">(about)</a>\n",
      "        </span>\n",
      "        <div class=\"tags\">\n",
      "            Tags:\n",
      "            <meta class=\"keywords\" itemprop=\"keywords\" content=\"misattributed-eleanor-roosevelt\" /    > \n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/misattributed-eleanor-roosevelt/page/1/\">misattributed-eleanor-roosevelt</a>\n",
      "            \n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n",
      "        <span class=\"text\" itemprop=\"text\">“A day without sunshine is like, you know, night.”</span>\n",
      "        <span>by <small class=\"author\" itemprop=\"author\">Steve Martin</small>\n",
      "        <a href=\"/author/Steve-Martin\">(about)</a>\n",
      "        </span>\n",
      "        <div class=\"tags\">\n",
      "            Tags:\n",
      "            <meta class=\"keywords\" itemprop=\"keywords\" content=\"humor,obvious,simile\" /    > \n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/humor/page/1/\">humor</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/obvious/page/1/\">obvious</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/simile/page/1/\">simile</a>\n",
      "            \n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <nav>\n",
      "        <ul class=\"pager\">\n",
      "            \n",
      "            \n",
      "            <li class=\"next\">\n",
      "                <a href=\"/page/2/\">Next <span aria-hidden=\"true\">&rarr;</span></a>\n",
      "            </li>\n",
      "            \n",
      "        </ul>\n",
      "    </nav>\n",
      "    </div>\n",
      "    <div class=\"col-md-4 tags-box\">\n",
      "        \n",
      "            <h2>Top Ten tags</h2>\n",
      "            \n",
      "            <span class=\"tag-item\">\n",
      "            <a class=\"tag\" style=\"font-size: 28px\" href=\"/tag/love/\">love</a>\n",
      "            </span>\n",
      "            \n",
      "            <span class=\"tag-item\">\n",
      "            <a class=\"tag\" style=\"font-size: 26px\" href=\"/tag/inspirational/\">inspirational</a>\n",
      "            </span>\n",
      "            \n",
      "            <span class=\"tag-item\">\n",
      "            <a class=\"tag\" style=\"font-size: 26px\" href=\"/tag/life/\">life</a>\n",
      "            </span>\n",
      "            \n",
      "            <span class=\"tag-item\">\n",
      "            <a class=\"tag\" style=\"font-size: 24px\" href=\"/tag/humor/\">humor</a>\n",
      "            </span>\n",
      "            \n",
      "            <span class=\"tag-item\">\n",
      "            <a class=\"tag\" style=\"font-size: 22px\" href=\"/tag/books/\">books</a>\n",
      "            </span>\n",
      "            \n",
      "            <span class=\"tag-item\">\n",
      "            <a class=\"tag\" style=\"font-size: 14px\" href=\"/tag/reading/\">reading</a>\n",
      "            </span>\n",
      "            \n",
      "            <span class=\"tag-item\">\n",
      "            <a class=\"tag\" style=\"font-size: 10px\" href=\"/tag/friendship/\">friendship</a>\n",
      "            </span>\n",
      "            \n",
      "            <span class=\"tag-item\">\n",
      "            <a class=\"tag\" style=\"font-size: 8px\" href=\"/tag/friends/\">friends</a>\n",
      "            </span>\n",
      "            \n",
      "            <span class=\"tag-item\">\n",
      "            <a class=\"tag\" style=\"font-size: 8px\" href=\"/tag/truth/\">truth</a>\n",
      "            </span>\n",
      "            \n",
      "            <span class=\"tag-item\">\n",
      "            <a class=\"tag\" style=\"font-size: 6px\" href=\"/tag/simile/\">simile</a>\n",
      "            </span>\n",
      "            \n",
      "        \n",
      "    </div>\n",
      "</div>\n",
      "\n",
      "    </div>\n",
      "    <footer class=\"footer\">\n",
      "        <div class=\"container\">\n",
      "            <p class=\"text-muted\">\n",
      "                Quotes by: <a href=\"https://www.goodreads.com/quotes\">GoodReads.com</a>\n",
      "            </p>\n",
      "            <p class=\"copyright\">\n",
      "                Made with <span class='zyte'>❤</span> by <a class='zyte' href=\"https://www.zyte.com\">Zyte</a>\n",
      "            </p>\n",
      "        </div>\n",
      "    </footer>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "cat ./tutorial/quotes-1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc12bb0-3ad8-475c-8c62-f34a35fd7775",
   "metadata": {},
   "source": [
    "## What has just happened under the hood?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbf9bea-4482-499c-b60a-8e16f745f855",
   "metadata": {},
   "source": [
    "Scrapy schedules the `scrapy.Request` objects returned by the `start_requests` method of the Spider. Upon receiving a `response` for each one, it instantiates `Response` objects and calls the callback method associated with the `request` (in this case, the `parse` method) passing the response as argument."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1817e5-2ae7-4871-84e6-bdceceb6cad1",
   "metadata": {},
   "source": [
    "## A shortcut to the `start_requests` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d87e87-96ac-4dda-8585-daf4d42c0839",
   "metadata": {},
   "source": [
    "Instead of implementing a `start_requests()` method that generates `scrapy.Request` objects from URLs, you can just define a `start_urls` class attribute with a list of URLs. This list will then be used by the default implementation of `start_requests()` to create the initial requests for your spider.\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    urls = [\n",
    "        \"https://quotes.toscrape.com/page/1/\",\n",
    "        \"https://quotes.toscrape.com/page/2/\",\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = f\"quotes-{page}.html\"\n",
    "        Path(filename).write_bytes(response.body)\n",
    "        self.log(f\"Saved file {filename}\")\n",
    "```\n",
    "\n",
    "The `parse()` method will be called to handle each of the requests for those URLs, even though we haven’t explicitly told Scrapy to do so. This happens because `parse()` is Scrapy’s default callback method, which is called for requests without an explicitly assigned callback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7cb801-1c53-4875-8d28-06c6864a880d",
   "metadata": {},
   "source": [
    "## Extracting data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5074321-f17a-4daf-bdb7-9d4e84341021",
   "metadata": {},
   "source": [
    "The best way to learn how to extract data with Scrapy is trying **selectors** using the Scrapy shell. Run:\n",
    "\n",
    "> Note: Remember to always enclose urls in quotes when running Scrapy shell from command-line, otherwise urls containing arguments (i.e. & character) will not work.<br>\n",
    "</br>\n",
    "On Windows, use double quotes instead:<br>\n",
    "</br>\n",
    "`scrapy shell \"https://quotes.toscrape.com/page/1/\"`\n",
    "\n",
    "```sh\n",
    "scrapy shell 'https://quotes.toscrape.com/page/1/'\n",
    "```\n",
    "```\n",
    "2024-02-06 14:47:14 [scrapy.utils.log] INFO: Scrapy 2.11.0 started (bot: tutorial)\n",
    "2024-02-06 14:47:14 [scrapy.utils.log] INFO: Versions: lxml 5.1.0.0, libxml2 2.12.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.11.2 (main, Mar 13 2023, 12:18:29) [GCC 12.2.0], pyOpenSSL 24.0.0 (OpenSSL 3.2.1 30 Jan 2024), cryptography 42.0.2, Platform Linux-6.1.0-17-amd64-x86_64-with-glibc2.36\n",
    "2024-02-06 14:47:14 [scrapy.addons] INFO: Enabled addons:\n",
    "[]\n",
    "2024-02-06 14:47:14 [asyncio] DEBUG: Using selector: EpollSelector\n",
    "2024-02-06 14:47:14 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
    "2024-02-06 14:47:14 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
    "2024-02-06 14:47:14 [scrapy.extensions.telnet] INFO: Telnet Password: 53d4e3939b5fb7e7\n",
    "2024-02-06 14:47:14 [scrapy.middleware] INFO: Enabled extensions:\n",
    "['scrapy.extensions.corestats.CoreStats',\n",
    " 'scrapy.extensions.telnet.TelnetConsole',\n",
    " 'scrapy.extensions.memusage.MemoryUsage']\n",
    "2024-02-06 14:47:14 [scrapy.crawler] INFO: Overridden settings:\n",
    "{'BOT_NAME': 'tutorial',\n",
    " 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',\n",
    " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
    " 'LOGSTATS_INTERVAL': 0,\n",
    " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
    " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
    " 'ROBOTSTXT_OBEY': True,\n",
    " 'SPIDER_MODULES': ['tutorial.spiders'],\n",
    " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
    "2024-02-06 14:47:14 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
    "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
    " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
    " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
    " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
    " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
    " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
    " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
    " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
    " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
    " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
    " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
    " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
    "2024-02-06 14:47:14 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
    "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
    " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
    " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
    " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
    " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
    "2024-02-06 14:47:14 [scrapy.middleware] INFO: Enabled item pipelines:\n",
    "[]\n",
    "2024-02-06 14:47:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
    "2024-02-06 14:47:14 [scrapy.core.engine] INFO: Spider opened\n",
    "2024-02-06 14:47:15 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://quotes.toscrape.com/robots.txt> (referer: None)\n",
    "2024-02-06 14:47:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/1/> (referer: None)\n",
    "[s] Available Scrapy objects:\n",
    "[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\n",
    "[s]   crawler    <scrapy.crawler.Crawler object at 0x7f5b3ece0cd0>\n",
    "[s]   item       {}\n",
    "[s]   request    <GET https://quotes.toscrape.com/page/1/>\n",
    "[s]   response   <200 https://quotes.toscrape.com/page/1/>\n",
    "[s]   settings   <scrapy.settings.Settings object at 0x7f5b3ffddc10>\n",
    "[s]   spider     <DefaultSpider 'default' at 0x7f5b3e7fa950>\n",
    "[s] Useful shortcuts:\n",
    "[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)\n",
    "[s]   fetch(req)                  Fetch a scrapy.Request and update local objects \n",
    "[s]   shelp()           Shell help (print this help)\n",
    "[s]   view(response)    View response in a browser\n",
    "2024-02-06 14:47:16 [asyncio] DEBUG: Using selector: EpollSelector\n",
    "```\n",
    "```ipython\n",
    "In [1]: \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724096bd-8857-43c4-914f-5f51b35222d9",
   "metadata": {},
   "source": [
    "Using the shell, you can try selecting elements using [CSS](https://www.w3.org/TR/selectors) with the `response` object:\n",
    "\n",
    "```ipython\n",
    "In [1]: response.css(\"title\")\n",
    "Out[1]: [<Selector query='descendant-or-self::title' data='<title>Quotes to Scrape</title>'>]\n",
    "\n",
    "In [2]: response.status\n",
    "Out[2]: 200\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b34527-4301-4b70-be21-f50099b57f90",
   "metadata": {},
   "source": [
    "### `get_all()` and `get()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86093451-8b67-4636-98ed-91283d3236a6",
   "metadata": {},
   "source": [
    "The result of running `response.css('title')` is a list-like object called **SelectorList**, which represents a list of Selector objects that wrap around XML/HTML elements and allow you to run further queries to fine-grain the selection or extract the data.\n",
    "\n",
    "To extract the text from the title above, you can do:\n",
    "\n",
    "```ipython\n",
    "In [9]: response.css(\"title::text\").getall()\n",
    "Out[9]: ['Quotes to Scrape']\n",
    "```\n",
    "\n",
    "There are two things to note here: one is that we’ve added `::text` to the CSS query, to mean we want to select only the text elements directly inside `<title>` element. If we don’t specify `::text`, we’d get the full title element, including its tags:\n",
    "\n",
    "```ipython\n",
    "In [11]: response.css(\"title\").getall()\n",
    "Out[11]: ['<title>Quotes to Scrape</title>']\n",
    "```\n",
    "\n",
    "The other thing is that the result of calling `.getall()` is a _list_: it is possible that a selector returns more than one result, so we extract them all. When you know you just want the first result, as in this case, you can do:\n",
    "\n",
    "```ipython\n",
    "In [12]: response.css(\"title::text\").get()\n",
    "Out[12]: 'Quotes to Scrape'\n",
    "```\n",
    "\n",
    "As an alternative, you could’ve written:\n",
    "\n",
    "```ipython\n",
    "In [16]: response.css(\"title::text\")[0].get()\n",
    "Out[16]: 'Quotes to Scrape'\n",
    "```\n",
    "\n",
    "Accessing an index on a SelectorList instance will raise an `IndexError` exception if there are no results. You might want to use `.get()` directly on the SelectorList instance instead, which returns `None` if there are no results:\n",
    "\n",
    "```ipython\n",
    "In [17]: response.css(\"noelement\").get()\n",
    "In [18]: response.css(\"noelement\")[0].get()\n",
    "---------------------------------------------------------------------------\n",
    "IndexError                                Traceback (most recent call last)\n",
    "...\n",
    "IndexError: list index out of range\n",
    "```\n",
    "\n",
    "There’s a lesson here: for most scraping code, you want it to be resilient to errors due to things not being found on a page, so that even if some parts fail to be scraped, you can at least get some data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55e7840-8d39-445e-8504-060085130009",
   "metadata": {},
   "source": [
    "### `re()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7055b91-e2f2-431e-9d30-c3b36a2750f8",
   "metadata": {},
   "source": [
    "Besides the `getall()` and `get()` methods, you can also use the `re()` method to extract using [regular expressions](https://docs.python.org/3/library/re.html):\n",
    "\n",
    "```ipython\n",
    "In [20]: response.css(\"title::text\").re(r\".*uot.*\")\n",
    "Out[20]: ['Quotes to Scrape']\n",
    "\n",
    "In [21]: response.css(\"title::text\").re(r\"Q\\w+\")\n",
    "Out[21]: ['Quotes']\n",
    "\n",
    "In [22]: response.css(\"title::text\").re(r\"(\\w+) to (\\w+)\")\n",
    "Out[22]: ['Quotes', 'Scrape']\n",
    "```\n",
    "\n",
    "- `\\w` represents any alphanumeric character (equivalent to `[a-zA-Z0-9_]`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dadabc-6323-4054-9a69-20753f17d10a",
   "metadata": {},
   "source": [
    "### `view(response)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de88dc81-81b3-4be4-b122-93cb530599a3",
   "metadata": {},
   "source": [
    "In order to find the proper CSS selectors to use, you might find it useful to open the `response` page from the shell in your web browser using `view(response)`. You can use your browser’s developer tools to inspect the HTML and come up with a selector (see [Using your browser’s Developer Tools for scraping](https://docs.scrapy.org/en/latest/topics/developer-tools.html#topics-developer-tools)).\n",
    "\n",
    "[Selector Gadget](https://selectorgadget.com/) is also a nice tool to quickly find CSS selector for visually selected elements, which works in many browsers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3677014c-d779-438a-b57d-dee723631675",
   "metadata": {},
   "source": [
    "### `XPath`: a brief intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74464531-cb38-4b0f-9444-3c2533fbf37f",
   "metadata": {},
   "source": [
    "See [XPath](../XPath_tutorial.ipynb#XPath).\n",
    "\n",
    "Besides CSS, Scrapy selectors also support using XPath expressions:\n",
    "\n",
    "```ipython\n",
    "In [24]: response.xpath(\"//title\")\n",
    "Out[24]: [<Selector query='//title' data='<title>Quotes to Scrape</title>'>]\n",
    "\n",
    "In [25]: response.xpath(\"//title/text()\").get()\n",
    "Out[25]: 'Quotes to Scrape'\n",
    "```\n",
    "\n",
    "XPath expressions are very powerful, and are the foundation of Scrapy Selectors. In fact, CSS selectors are converted to XPath under-the-hood. [You can see that](#Extracting-data) if you read closely the text representation of the selector objects in the shell.\n",
    "\n",
    "While perhaps not as popular as CSS selectors, XPath expressions offer more power because besides navigating the structure, it can also look at the content. Using XPath, you’re able to select things like: _select the link that contains the text “Next Page”_. This makes XPath very fitting to the task of scraping, and we encourage you to learn XPath even if you already know how to construct CSS selectors, it will make scraping much easier.\n",
    "\n",
    "We won’t cover much of XPath here, but you can read more about using [XPath with Scrapy Selectors](https://docs.scrapy.org/en/latest/topics/selectors.html#topics-selectors). To learn more about XPath, we recommend this [tutorial to learn XPath through examples](http://zvon.org/comp/r/tut-XPath_1.html), and this tutorial to learn [“how to think in XPath”](http://plasmasturm.org/log/xpath101/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fe8ac6-9ebe-4464-b0a9-7b0dea6e4569",
   "metadata": {},
   "source": [
    "### Extracting quotes and authors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035b167e-98de-4068-9c34-61ecdb081034",
   "metadata": {},
   "source": [
    "Now that you know a bit about selection and extraction, let’s complete our spider by writing the code to extract the quotes from the web page.\n",
    "\n",
    "Each quote in `https://quotes.toscrape.com` is represented by HTML elements that look like this:\n",
    "\n",
    "```html\n",
    "<div class=\"quote\">\n",
    "    <span class=\"text\">“The world as we have created it is a process of our\n",
    "    thinking. It cannot be changed without changing our thinking.”</span>\n",
    "    <span>\n",
    "        by <small class=\"author\">Albert Einstein</small>\n",
    "        <a href=\"/author/Albert-Einstein\">(about)</a>\n",
    "    </span>\n",
    "    <div class=\"tags\">\n",
    "        Tags:\n",
    "        <a class=\"tag\" href=\"/tag/change/page/1/\">change</a>\n",
    "        <a class=\"tag\" href=\"/tag/deep-thoughts/page/1/\">deep-thoughts</a>\n",
    "        <a class=\"tag\" href=\"/tag/thinking/page/1/\">thinking</a>\n",
    "        <a class=\"tag\" href=\"/tag/world/page/1/\">world</a>\n",
    "    </div>\n",
    "</div>\n",
    "```\n",
    "\n",
    "Let’s open up `scrapy shell` and play a bit to find out how to extract the data we want. We get a list of selectors for the quote HTML elements with:\n",
    "\n",
    "```sh\n",
    "scrapy shell 'https://quotes.toscrape.com'\n",
    "```\n",
    "```ipython\n",
    "In [1]: response.css(\"div.quote\")\n",
    "Out[1]: \n",
    "[<Selector query=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>,\n",
    " <Selector query=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>,\n",
    " <Selector query=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>,\n",
    " <Selector query=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>,\n",
    " <Selector query=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>,\n",
    " <Selector query=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>,\n",
    " <Selector query=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>,\n",
    " <Selector query=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>,\n",
    " <Selector query=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>,\n",
    " <Selector query=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>]\n",
    "```\n",
    "\n",
    "Each of the selectors returned by the query above allows us to run further queries over their sub-elements. Let’s assign the first selector to a variable, so that we can run our CSS selectors directly on a particular quote:\n",
    "\n",
    "```ipython\n",
    "In [3]: quote = response.css(\"div.quote\")[0]\n",
    "```\n",
    "\n",
    "Now, let’s extract `text`, `author` and the `tags` from that quote using the `quote` object we just created:\n",
    "\n",
    "```ipython\n",
    "In [4]: text = quote.css(\"span.text::text\").get()\n",
    "In [5]: text\n",
    "Out[5]: '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”'\n",
    "```\n",
    "\n",
    "Given that the `tags` are a list of strings, we can use the `.getall()` method to get all of them:\n",
    "\n",
    "```ipython\n",
    "In [6]: tags = quote.css(\"div.tags a.tag::text\").getall()\n",
    "In [7]: tags\n",
    "Out[7]: ['change', 'deep-thoughts', 'thinking', 'world']\n",
    "```\n",
    "\n",
    "Having figured out how to extract each bit, we can now iterate over all the quotes elements and put them together into a Python dictionary:\n",
    "\n",
    "```ipython\n",
    "In [8]: for quote in response.css(\"div.quote\"):\n",
    "   ...:     text = quote.css(\"span.text::text\").get()\n",
    "   ...:     author = quote.css(\"small.author::text\").get()\n",
    "   ...:     tags = quote.css(\"div.tags a.tag::text\").getall()\n",
    "   ...:     print(dict(text=text, author=author, tags=tags))\n",
    "   ...: \n",
    "{'text': '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”', 'author': 'Albert Einstein', 'tags': ['change', 'deep-thoughts', 'thinking', 'world']}\n",
    "{'text': '“It is our choices, Harry, that show what we truly are, far more than our abilities.”', 'author': 'J.K. Rowling', 'tags': ['abilities', 'choices']}\n",
    "{'text': '“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”', 'author': 'Albert Einstein', 'tags': ['inspirational', 'life', 'live', 'miracle', 'miracles']}\n",
    "{'text': '“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”', 'author': 'Jane Austen', 'tags': ['aliteracy', 'books', 'classic', 'humor']}\n",
    "{'text': \"“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\", 'author': 'Marilyn Monroe', 'tags': ['be-yourself', 'inspirational']}\n",
    "{'text': '“Try not to become a man of success. Rather become a man of value.”', 'author': 'Albert Einstein', 'tags': ['adulthood', 'success', 'value']}\n",
    "{'text': '“It is better to be hated for what you are than to be loved for what you are not.”', 'author': 'André Gide', 'tags': ['life', 'love']}\n",
    "{'text': \"“I have not failed. I've just found 10,000 ways that won't work.”\", 'author': 'Thomas A. Edison', 'tags': ['edison', 'failure', 'inspirational', 'paraphrased']}\n",
    "{'text': \"“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\", 'author': 'Eleanor Roosevelt', 'tags': ['misattributed-eleanor-roosevelt']}\n",
    "{'text': '“A day without sunshine is like, you know, night.”', 'author': 'Steve Martin', 'tags': ['humor', 'obvious', 'simile']}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc322623-939f-49b5-940d-ab90693866c1",
   "metadata": {},
   "source": [
    "### Extracting data in our spider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eaf8d7-9f0e-4a5e-b4a0-5491acdf0a4e",
   "metadata": {},
   "source": [
    "Let’s get back to our spider. Until now, it doesn’t extract any data in particular, just saves the whole HTML page to a local file. Let’s integrate the extraction logic above into our spider.\n",
    "\n",
    "A Scrapy spider typically generates many dictionaries containing the data extracted from the page. To do that, we use the `yield` Python keyword in the `callback`, as you can see below:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    urls = [\n",
    "        \"https://quotes.toscrape.com/page/1/\",\n",
    "        \"https://quotes.toscrape.com/page/2/\",\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css(\"div.quote\"):\n",
    "            yield {\n",
    "                \"text\": quote.css(\"span.text::text\").get(),\n",
    "                \"author\": quote.css(\"small.author::text\").get(),\n",
    "                \"tags\": quote.css(\"div.tags a.tag::text\").getall(),\n",
    "            }\n",
    "```\n",
    "\n",
    "To run this spider, exit the `scrapy shell` and run the crawler:\n",
    "\n",
    "```sh\n",
    "quit()\n",
    "scrapy crawl quotes\n",
    "```\n",
    "```\n",
    "2024-02-06 23:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
    "{'text': '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”', 'author': 'Albert Einstein', 'tags': ['change', 'deep-thoughts', 'thinking', 'world']}\n",
    "2024-02-06 23:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
    "{'text': '“It is our choices, Harry, that show what we truly are, far more than our abilities.”', 'author': 'J.K. Rowling', 'tags': ['abilities', 'choices']}\n",
    "2024-02-06 23:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
    "{'text': '“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”', 'author': 'Albert Einstein', 'tags': ['inspirational', 'life', 'live', 'miracle', 'miracles']}\n",
    "2024-02-06 23:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
    "{'text': '“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”', 'author': 'Jane Austen', 'tags': ['aliteracy', 'books', 'classic', 'humor']}\n",
    "2024-02-06 23:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
    "{'text': \"“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\", 'author': 'Marilyn Monroe', 'tags': ['be-yourself', 'inspirational']}\n",
    "2024-02-06 23:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
    "{'text': '“Try not to become a man of success. Rather become a man of value.”', 'author': 'Albert Einstein', 'tags': ['adulthood', 'success', 'value']}\n",
    "2024-02-06 23:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
    "{'text': '“It is better to be hated for what you are than to be loved for what you are not.”', 'author': 'André Gide', 'tags': ['life', 'love']}\n",
    "2024-02-06 23:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
    "{'text': \"“I have not failed. I've just found 10,000 ways that won't work.”\", 'author': 'Thomas A. Edison', 'tags': ['edison', 'failure', 'inspirational', 'paraphrased']}\n",
    "2024-02-06 23:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
    "{'text': \"“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\", 'author': 'Eleanor Roosevelt', 'tags': ['misattributed-eleanor-roosevelt']}\n",
    "2024-02-06 23:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
    "{'text': '“A day without sunshine is like, you know, night.”', 'author': 'Steve Martin', 'tags': ['humor', 'obvious', 'simile']}\n",
    "2024-02-06 23:53:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/2/> (referer: None)\n",
    "2024-02-06 23:53:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
    "{'text': \"“This life is what you make it. No matter what, you're going to mess up sometimes, it's a universal truth. But the good part is you get to decide how you're going to mess it up. Girls will be your friends - they'll act like it anyway. But just remember, some come, some go. The ones that stay with you through everything - they're your true best friends. Don't let go of them. Also remember, sisters make the best friends in the world. As for lovers, well, they'll come and go too. And baby, I hate to say it, most of them - actually pretty much all of them are going to break your heart, but you can't give up because if you give up, you'll never find your soulmate. You'll never find that half who makes you whole and that goes for everything. Just because you fail once, doesn't mean you're gonna fail at everything. Keep trying, hold on, and always, always, always believe in yourself, because if you don't, then who will, sweetie? So keep your head high, keep your chin up, and most importantly, keep smiling, because life's a beautiful thing and there's so much to smile about.”\", 'author': 'Marilyn Monroe', 'tags': ['friends', 'heartbreak', 'inspirational', 'life', 'love', 'sisters']}\n",
    "2024-02-06 23:53:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
    "{'text': '“It takes a great deal of bravery to stand up to our enemies, but just as much to stand up to our friends.”', 'author': 'J.K. Rowling', 'tags': ['courage', 'friends']}\n",
    "2024-02-06 23:53:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
    "{'text': \"“If you can't explain it to a six year old, you don't understand it yourself.”\", 'author': 'Albert Einstein', 'tags': ['simplicity', 'understand']}\n",
    "2024-02-06 23:53:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
    "{'text': \"“You may not be her first, her last, or her only. She loved before she may love again. But if she loves you now, what else matters? She's not perfect—you aren't either, and the two of you may never be perfect together but if she can make you laugh, cause you to think twice, and admit to being human and making mistakes, hold onto her and give her the most you can. She may not be thinking about you every second of the day, but she will give you a part of her that she knows you can break—her heart. So don't hurt her, don't change her, don't analyze and don't expect more than she can give. Smile when she makes you happy, let her know when she makes you mad, and miss her when she's not there.”\", 'author': 'Bob Marley', 'tags': ['love']}\n",
    "2024-02-06 23:53:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
    "{'text': '“I like nonsense, it wakes up the brain cells. Fantasy is a necessary ingredient in living.”', 'author': 'Dr. Seuss', 'tags': ['fantasy']}\n",
    "2024-02-06 23:53:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
    "{'text': '“I may not have gone where I intended to go, but I think I have ended up where I needed to be.”', 'author': 'Douglas Adams', 'tags': ['life', 'navigation']}\n",
    "2024-02-06 23:53:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
    "{'text': \"“The opposite of love is not hate, it's indifference. The opposite of art is not ugliness, it's indifference. The opposite of faith is not heresy, it's indifference. And the opposite of life is not death, it's indifference.”\", 'author': 'Elie Wiesel', 'tags': ['activism', 'apathy', 'hate', 'indifference', 'inspirational', 'love', 'opposite', 'philosophy']}\n",
    "2024-02-06 23:53:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
    "{'text': '“It is not a lack of love, but a lack of friendship that makes unhappy marriages.”', 'author': 'Friedrich Nietzsche', 'tags': ['friendship', 'lack-of-friendship', 'lack-of-love', 'love', 'marriage', 'unhappy-marriage']}\n",
    "2024-02-06 23:53:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
    "{'text': '“Good friends, good books, and a sleepy conscience: this is the ideal life.”', 'author': 'Mark Twain', 'tags': ['books', 'contentment', 'friends', 'friendship', 'life']}\n",
    "2024-02-06 23:53:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
    "{'text': '“Life is what happens to us while we are making other plans.”', 'author': 'Allen Saunders', 'tags': ['fate', 'life', 'misattributed-john-lennon', 'planning', 'plans']}\n",
    "2024-02-06 23:53:53 [scrapy.core.engine] INFO: Closing spider (finished)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0468328-06de-44fa-a1ed-8cff4be03ee4",
   "metadata": {},
   "source": [
    "## Storing the scraped data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c31e6f5-1a84-4e62-bbed-e2a0e860f768",
   "metadata": {},
   "source": [
    "The simplest way to store the scraped data is by using [Feed exports](#2.8-Feed-exports), with the following command:\n",
    "\n",
    "```sh\n",
    "scrapy crawl quotes -O quotes.json\n",
    "```\n",
    "\n",
    "That will generate a `quotes.json` file containing all scraped items, serialized in JSON.\n",
    "\n",
    "The `-O` command-line switch overwrites any existing file; use `-o` instead to append new content to any existing file. However, appending to a JSON file makes the file contents invalid JSON. When appending to a file, consider using a different serialization format, such as `JSON Lines`:\n",
    "\n",
    "```sh\n",
    "scrapy crawl quotes -o quotes.jsonl\n",
    "```\n",
    "\n",
    "The [JSON Lines format](http://jsonlines.org/) is useful because it’s stream-like, you can easily append new records to it. It doesn’t have the same problem of JSON when you run twice. Also, as each record is a separate line, you can process big files without having to fit everything in memory, there are tools like `JQ` to help do that at the command-line.\n",
    "\n",
    "In small projects (like the one in this tutorial), that should be enough. However, if you want to perform more complex things with the scraped items, you can write an [Item Pipeline](#2.7-Item-Pipeline). A placeholder file for Item Pipelines has been set up for you when the project is created, in `tutorial/pipelines.py`. Though you don’t need to implement any item pipelines if you just want to store the scraped items.\n",
    "\n",
    "```sh\n",
    "$ scrapy crawl -h\n",
    "```\n",
    "```\n",
    "Usage\n",
    "=====\n",
    "  scrapy crawl [options] <spider>\n",
    "\n",
    "Run a spider\n",
    "\n",
    "Options\n",
    "=======\n",
    "  -h, --help            show this help message and exit\n",
    "  -a NAME=VALUE         set spider argument (may be repeated)\n",
    "  -o FILE, --output FILE\n",
    "                        append scraped items to the end of FILE (use - for stdout), to define format set a colon at the end of the output\n",
    "                        URI (i.e. -o FILE:FORMAT)\n",
    "  -O FILE, --overwrite-output FILE\n",
    "                        dump scraped items into FILE, overwriting any existing file, to define format set a colon at the end of the\n",
    "                        output URI (i.e. -O FILE:FORMAT)\n",
    "  -t FORMAT, --output-format FORMAT\n",
    "                        format to use for dumping items\n",
    "\n",
    "Global Options\n",
    "--------------\n",
    "  --logfile FILE        log file. if omitted stderr will be used\n",
    "  -L LEVEL, --loglevel LEVEL\n",
    "                        log level (default: DEBUG)\n",
    "  --nolog               disable logging completely\n",
    "  --profile FILE        write python cProfile stats to FILE\n",
    "  --pidfile FILE        write process ID to FILE\n",
    "  -s NAME=VALUE, --set NAME=VALUE\n",
    "                        set/override setting (may be repeated)\n",
    "  --pdb                 enable pdb on failure\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1edd287b-96a0-4af3-a5f5-95f69dc6e739",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T19:19:42.039144Z",
     "iopub.status.busy": "2024-02-06T19:19:42.037500Z",
     "iopub.status.idle": "2024-02-06T19:19:42.052495Z",
     "shell.execute_reply": "2024-02-06T19:19:42.048680Z",
     "shell.execute_reply.started": "2024-02-06T19:19:42.039015Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# man jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d7db66-c191-448e-9218-a196f8ff5fbe",
   "metadata": {},
   "source": [
    "## Following links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f35f7cb-a8f6-4334-9b0f-9051deb9ed3f",
   "metadata": {},
   "source": [
    "Let’s say, instead of just scraping the stuff from the first two pages from https://quotes.toscrape.com, you want quotes from all the pages in the website.\n",
    "\n",
    "Now that you know how to extract data from pages, let’s see how to follow links from them.\n",
    "\n",
    "First thing is to extract the link to the page we want to follow. Examining our page, we can see there is a link to the next page with the following markup:\n",
    "\n",
    "```html\n",
    "<ul class=\"pager\">\n",
    "    <li class=\"next\">\n",
    "        <a href=\"/page/2/\">Next <span aria-hidden=\"true\">&rarr;</span></a>\n",
    "    </li>\n",
    "</ul>\n",
    "```\n",
    "\n",
    "We can try extracting it in the shell:\n",
    "\n",
    "```ipython\n",
    "In [1]: response.css(\"li.next a\")\n",
    "Out[1]: [<Selector query=\"descendant-or-self::li[@class and contains(concat(' ', normalize-space(@class), ' '), ' next ')]/descendant-or-self::*/a\" data='<a href=\"/page/2/\">Next <span aria-hi...'>]\n",
    "\n",
    "In [2]: response.css(\"li.next a\").get()\n",
    "Out[2]: '<a href=\"/page/2/\">Next <span aria-hidden=\"true\">→</span></a>'\n",
    "```\n",
    "\n",
    "This gets the anchor element, but we want the attribute `href`. For that, Scrapy supports a CSS extension that lets you select the attribute contents, like this:\n",
    "\n",
    "```ipython\n",
    "In [3]: response.css(\"li.next a::attr(href)\").get()\n",
    "Out[3]: '/page/2/'\n",
    "```\n",
    "There is also an `attrib` property available (see [Selecting element attributes](#Selecting-element-attributes) for more):\n",
    "\n",
    "```ipython\n",
    "In [4]: response.css(\"li.next a\").attrib[\"href\"]\n",
    "Out[4]: '/page/2/'\n",
    "```\n",
    "\n",
    "Let’s see now our spider modified to recursively follow the link to the next page, extracting data from it:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        \"https://quotes.toscrape.com/page/1/\",\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css(\"div.quote\"):\n",
    "            yield {\n",
    "                \"text\": quote.css(\"span.text::text\").get(),\n",
    "                \"author\": quote.css(\"small.author::text\").get(),\n",
    "                \"tags\": quote.css(\"div.tags a.tag::text\").getall(),\n",
    "            }\n",
    "\n",
    "        next_page = response.css(\"li.next a::attr(href)\").get()\n",
    "        if next_page is not None:\n",
    "            next_page = response.urljoin(next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "```\n",
    "\n",
    "Now, after extracting the data, the `parse()` method looks for the link to the next page, builds a full absolute URL using the `urljoin()` method (since the links can be relative) and `yield`s a new request to the next page, registering itself as callback to handle the data extraction for the next page and to keep the crawling going through all the pages.\n",
    "\n",
    "What you see here is Scrapy’s mechanism of following links: when you `yield` a `Request` in a callback method, Scrapy will schedule that request to be sent and register a callback method to be executed when that request finishes.\n",
    "\n",
    "Using this, you can build complex crawlers that follow links according to rules you define, and extract different kinds of data depending on the page it’s visiting.\n",
    "\n",
    "In our example, it creates a sort of loop, following all the links to the next page until it doesn’t find one – handy for crawling blogs, forums and other sites with **pagination**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcfc124-ed0e-4829-aee8-0adb585de4b3",
   "metadata": {},
   "source": [
    "### A shortcut for creating Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d1ec72-a2f2-48e9-a8e0-2a32a371c98d",
   "metadata": {},
   "source": [
    "As a shortcut for creating `Request` objects you can use `response.follow`:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        \"https://quotes.toscrape.com/page/1/\",\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css(\"div.quote\"):\n",
    "            yield {\n",
    "                \"text\": quote.css(\"span.text::text\").get(),\n",
    "                \"author\": quote.css(\"span small::text\").get(),\n",
    "                \"tags\": quote.css(\"div.tags a.tag::text\").getall(),\n",
    "            }\n",
    "\n",
    "        next_page = response.css(\"li.next a::attr(href)\").get()\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, callback=self.parse)\n",
    "```\n",
    "\n",
    "Unlike `scrapy.Request`, `response.follow` supports relative URLs directly - no need to call `urljoin`. Note that `response.follow` just returns a `Request` instance; you still have to `yield` this `Request`.\n",
    "\n",
    "You can also pass a selector to `response.follow` instead of a string; this selector should extract necessary attributes:\n",
    "\n",
    "```python\n",
    "for href in response.css(\"ul.pager a::attr(href)\"):\n",
    "    yield response.follow(href, callback=self.parse)\n",
    "```\n",
    "\n",
    "For `<a>` elements there is a shortcut: `response.follow` uses their `href` attribute automatically. So the code can be shortened further:\n",
    "\n",
    "```python\n",
    "for a in response.css(\"ul.pager a\"):\n",
    "    yield response.follow(a, callback=self.parse)\n",
    "```\n",
    "\n",
    "To create multiple requests from an iterable, you can use `response.follow_all` instead:\n",
    "\n",
    "```python\n",
    "anchors = response.css(\"ul.pager a\")\n",
    "yield from response.follow_all(anchors, callback=self.parse)\n",
    "```\n",
    "\n",
    "or, shortening it further:\n",
    "\n",
    "```python\n",
    "yield from response.follow_all(css=\"ul.pager a\", callback=self.parse)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3462a3bf-fff6-4f92-9fde-ee15248b9743",
   "metadata": {},
   "source": [
    "## More examples and patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523e5ad1-05c5-4636-98c9-8a016fae4ead",
   "metadata": {},
   "source": [
    "Here is another spider that illustrates callbacks and following links, this time for scraping author information:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class AuthorSpider(scrapy.Spider):\n",
    "    name = \"author\"\n",
    "\n",
    "    start_urls = [\"https://quotes.toscrape.com/\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        author_page_links = response.css(\".author + a\")\n",
    "        yield from response.follow_all(author_page_links, self.parse_author)\n",
    "\n",
    "        pagination_links = response.css(\"li.next a\")\n",
    "        yield from response.follow_all(pagination_links, self.parse)\n",
    "\n",
    "    def parse_author(self, response):\n",
    "        def extract_with_css(query):\n",
    "            return response.css(query).get(default=\"\").strip()\n",
    "\n",
    "        yield {\n",
    "            \"name\": extract_with_css(\"h3.author-title::text\"),\n",
    "            \"birthdate\": extract_with_css(\".author-born-date::text\"),\n",
    "            \"bio\": extract_with_css(\".author-description::text\"),\n",
    "        }\n",
    "```\n",
    "\n",
    "This spider will start from the main page, it will follow all the links to the authors pages calling the `parse_author` callback for each of them, and also the pagination links with the `parse` callback as we saw before.\n",
    "\n",
    "Here we’re passing callbacks to `response.follow_all` as positional arguments to make the code shorter; it also works for `Request`.\n",
    "\n",
    "The `parse_author` callback defines a helper function to extract and cleanup the data from a CSS query and `yield`s the Python `dict` with the author data.\n",
    "\n",
    "Another interesting thing this spider demonstrates is that, even if there are many quotes from the same author, we don’t need to worry about visiting the same author page multiple times. By default, Scrapy filters out duplicated requests to URLs already visited, avoiding the problem of hitting servers too much because of a programming mistake. This can be configured by the setting `DUPEFILTER_CLASS`.\n",
    "\n",
    "Hopefully by now you have a good understanding of how to use the mechanism of following links and callbacks with Scrapy.\n",
    "\n",
    "As yet another example spider that leverages the mechanism of following links, check out the `CrawlSpider` class for a generic spider that implements a small rules engine that you can use to write your crawlers on top of it.\n",
    "\n",
    "Also, a common pattern is to build an item with data from more than one page, using a trick to [pass additional data to the callbacks](#Passing-additional-data-to-callback-functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d457fd-147e-4115-bbc2-bb2b52127752",
   "metadata": {},
   "source": [
    "# 1.3 Using spider arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1381d791-26f4-408d-a798-caa6c5727581",
   "metadata": {},
   "source": [
    "You can provide command line arguments to your spiders by using the `-a` option when running them:\n",
    "\n",
    "```sh\n",
    "scrapy crawl quotes -O quotes-humor.json -a tag=humor\n",
    "```\n",
    "\n",
    "These arguments are passed to the Spider’s `__init__` method and become spider attributes by default.\n",
    "\n",
    "In this example, the value provided for the `tag` argument will be available via `self.tag`. You can use this to make your spider fetch only quotes with a specific tag, building the URL based on the argument:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        url = \"https://quotes.toscrape.com/\"\n",
    "        tag = getattr(self, \"tag\", None)\n",
    "        if tag is not None:\n",
    "            url += \"tag/\" + tag\n",
    "\n",
    "        yield scrapy.Request(url, self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css(\"div.quote\"):\n",
    "            yield {\n",
    "                \"text\": quote.css(\"span,text::text\").get(),\n",
    "                \"author\": qutote.css(\"small.author::text\").get(),\n",
    "            }\n",
    "\n",
    "        next_page = response.css(\"li.next a::attr(href)\").get()\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, self.parse)\n",
    "```\n",
    "\n",
    "If you pass the `tag=humor` argument to this spider, you’ll notice that it will only visit URLs from the humor tag, such as `https://quotes.toscrape.com/tag/humor`.\n",
    "\n",
    "You can learn more about [handling spider arguments here](#Spider-arguments)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f185efe-b616-43a1-a5e6-b353428a8bb9",
   "metadata": {},
   "source": [
    "# <b>2. Basic concepts</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2831e705-9968-45b4-be4c-3be857078eb9",
   "metadata": {},
   "source": [
    "# Spider vs Crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3086fd6a-3069-4b37-a725-b99c4aca2d05",
   "metadata": {},
   "source": [
    "_ChatGPT_:  \n",
    "In Scrapy, \"Crawler\" and \"Spider\" are closely related but distinct concepts.\n",
    "\n",
    "- **Spider**: A spider is a class in Scrapy that defines how a particular website (or a group of websites) will be scraped. It contains the logic to extract data from web pages, including how to follow links within those pages. Each spider is responsible for a specific website or set of websites and defines the scraping behavior, such as what pages to scrape, how to extract data, and how to follow links.\n",
    "\n",
    "- **Crawler**: The term \"crawler\" is often used interchangeably with \"spider\" in the context of web scraping. However, in Scrapy, the term \"crawler\" refers to the entire scraping process, which includes the spiders as well as other components such as the request scheduler, the item pipeline, and settings. The crawler manages the execution of spiders, handles requests and responses, and orchestrates the entire scraping process.\n",
    "\n",
    "In summary, a spider is a specific component within a Scrapy project responsible for defining how data will be extracted from a website, while a crawler is the broader term encompassing the entire scraping process, including the spiders, request management, and other related functionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcf98ed-4b96-4d47-bf9a-5541f63e6e1d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2.1 Command line tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af389cd8-1346-4b66-a4f7-493eb9e002b0",
   "metadata": {},
   "source": [
    "Scrapy is controlled through the scrapy command-line tool, to be referred here as the **“Scrapy tool”** to differentiate it from the sub-commands, which we just call **“commands”** or **“Scrapy commands”**.\n",
    "\n",
    "The Scrapy tool provides several commands, for multiple purposes, and each one accepts a different set of arguments and options.\n",
    "\n",
    "(The scrapy deploy command has been removed in 1.0 in favor of the standalone `scrapyd-deploy`. See [Deploying your project](#https://scrapyd.readthedocs.io/en/latest/deploy.html).)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ba4f14-0e35-4937-a359-047d7f31b446",
   "metadata": {},
   "source": [
    "## Configuration settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47412c3-d7dd-42fb-a252-01a02c749cbe",
   "metadata": {},
   "source": [
    "Scrapy will look for configuration parameters in ini-style `scrapy.cfg` files in standard locations:\n",
    "- `/etc/scrapy.cfg` or `c:\\scrapy\\scrapy.cfg` (system-wide),\n",
    "- `~/.config/scrapy.cfg` (`$XDG_CONFIG_HOME`) and `~/.scrapy.cfg` (`$HOME`) for global (user-wide) settings, and\n",
    "- `scrapy.cfg` inside a Scrapy project’s root (see next section).\n",
    "\n",
    "Settings from these files are merged in the listed order of preference: \n",
    "- user-defined values have higher priority than system-wide defaults and \n",
    "- project-wide settings will override all others, when defined.\n",
    "\n",
    "Scrapy also understands, and can be configured through, a number of environment variables. Currently these are:\n",
    "- `SCRAPY_SETTINGS_MODULE` (see [Designating the settings](#Designating-the-settings))\n",
    "- `SCRAPY_PROJECT` (see [Sharing the root directory between projects](#Sharing-the-root-directory-between-projects))\n",
    "- `SCRAPY_PYTHON_SHELL` (see [Scrapy shell](#2.3-Scrapy-shell))\n",
    "\n",
    "_ChatGPT:_  \n",
    "Ini-style, short for \"Initialization style,\" refers to a simple text-based file format used for configuration or initialization files in computing. It is named after the \".ini\" file extension commonly associated with these types of files in Windows environments.\n",
    "\n",
    "Ini-style files consist of sections, each containing key-value pairs, typically used to represent configuration settings for applications or systems. The structure is straightforward, with sections enclosed in square brackets \"`[ ]`\" and key-value pairs separated by an equals sign \"`=`\" or a colon \"`:`\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6e9a81-6f5f-43bd-92b0-ed446b46eb08",
   "metadata": {},
   "source": [
    "### Default structure of Scrapy projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68e269f-b48c-48d8-8762-f77af389761a",
   "metadata": {},
   "source": [
    "Before delving into the command-line tool and its sub-commands, let’s first understand the directory structure of a Scrapy project.\n",
    "\n",
    "Though it can be modified, all Scrapy projects have the same file structure by default, similar to this:\n",
    "\n",
    "```python\n",
    "scrapy.cfg\n",
    "myproject/\n",
    "    __init__.py\n",
    "    items.py\n",
    "    middlewares.py\n",
    "    pipelines.py\n",
    "    settings.py\n",
    "    spiders/\n",
    "        __init__.py\n",
    "        spider1.py\n",
    "        spider2.py\n",
    "        ...\n",
    "```\n",
    "\n",
    "The directory where the `scrapy.cfg` file resides is known as the **project root directory**. That file contains the name of the python module that defines the project settings. Here is an example:\n",
    "\n",
    "```python\n",
    "[settings]\n",
    "default = myproject.settings\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa09ec7b-fb2b-4281-9797-6881b097d5a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T13:25:15.012909Z",
     "iopub.status.busy": "2024-02-08T13:25:15.012624Z",
     "iopub.status.idle": "2024-02-08T13:25:15.126321Z",
     "shell.execute_reply": "2024-02-08T13:25:15.125041Z",
     "shell.execute_reply.started": "2024-02-08T13:25:15.012886Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mdata/tutorial\u001b[0m\n",
      "├── scrapy.cfg\n",
      "└── \u001b[01;34mtutorial\u001b[0m\n",
      "    ├── __init__.py\n",
      "    ├── items.py\n",
      "    ├── middlewares.py\n",
      "    ├── pipelines.py\n",
      "    ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "    │   ├── __init__.cpython-311.pyc\n",
      "    │   └── settings.cpython-311.pyc\n",
      "    ├── settings.py\n",
      "    └── \u001b[01;34mspiders\u001b[0m\n",
      "        ├── __init__.py\n",
      "        ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "        │   ├── __init__.cpython-311.pyc\n",
      "        │   ├── quotes_spider.cpython-311.pyc\n",
      "        │   └── tmp.cpython-311.pyc\n",
      "        ├── quotes_spider.py\n",
      "        └── tmp.py\n",
      "\n",
      "5 directories, 14 files\n"
     ]
    }
   ],
   "source": [
    "tree data/tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d9cc80-c93d-4b37-a4ee-43a2eb1a91a9",
   "metadata": {},
   "source": [
    "### Sharing the root directory between projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa40661-12a0-46a1-8f32-ae53547311ad",
   "metadata": {},
   "source": [
    "A project root directory, the one that contains the `scrapy.cfg`, may be shared by multiple Scrapy projects, each with its own settings module.\n",
    "\n",
    "In that case, you must define one or more aliases for those settings modules under `[settings]` in your `scrapy.cfg` file:\n",
    "\n",
    "```\n",
    "[settings]\n",
    "default = myproject1.settings\n",
    "project1 = myproject1.settings\n",
    "project2 = myproject2.settings\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64896358-a49f-4c6f-a891-356dc17f0562",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T13:26:09.487762Z",
     "iopub.status.busy": "2024-02-08T13:26:09.487365Z",
     "iopub.status.idle": "2024-02-08T13:26:09.596102Z",
     "shell.execute_reply": "2024-02-08T13:26:09.595094Z",
     "shell.execute_reply.started": "2024-02-08T13:26:09.487731Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Automatically created by: scrapy startproject\n",
      "#\n",
      "# For more information about the [deploy] section see:\n",
      "# https://scrapyd.readthedocs.io/en/latest/deploy.html\n",
      "\n",
      "[settings]\n",
      "default = tutorial.settings\n",
      "\n",
      "[deploy]\n",
      "#url = http://localhost:6800/\n",
      "project = tutorial\n"
     ]
    }
   ],
   "source": [
    "cat data/tutorial/scrapy.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e23f15e-2618-4b56-ada8-8228802be2ed",
   "metadata": {},
   "source": [
    "By default, the scrapy command-line tool will use the default settings. Use the `SCRAPY_PROJECT` environment variable to specify a different project for scrapy to use:\n",
    "\n",
    "```sh\n",
    "$ scrapy settings --get BOT_NAME\n",
    "Project 1 Bot\n",
    "$ export SCRAPY_PROJECT=project2\n",
    "$ scrapy settings --get BOT_NAME\n",
    "Project 2 Bot\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e59a853-45ee-45ec-9582-dd76c9ab0315",
   "metadata": {},
   "source": [
    "## Using the `scrapy` tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e3ddb8-0d24-433a-9710-c2fb35857f20",
   "metadata": {},
   "source": [
    "### General info and help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2d48da-5088-4f18-bf4d-770b5a58ecd0",
   "metadata": {},
   "source": [
    "You can start by running the Scrapy tool with no arguments and it will print some usage help and the available commands:\n",
    "\n",
    "```sh\n",
    "$ scrapy\n",
    "Scrapy X.Y - no active project\n",
    "\n",
    "Usage:\n",
    "  scrapy <command> [options] [args]\n",
    "\n",
    "Available commands:\n",
    "  crawl         Run a spider\n",
    "  fetch         Fetch a URL using the Scrapy downloader\n",
    "[...]\n",
    "```\n",
    "\n",
    "The first line will print the currently active project if you’re inside a Scrapy project. In this example it was run from outside a project. If run from inside a project it would have printed something like this:\n",
    "\n",
    "```sh\n",
    "Scrapy X.Y - project: myproject\n",
    "\n",
    "Usage:\n",
    "  scrapy <command> [options] [args]\n",
    "\n",
    "[...]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4357dd-ef05-48e8-8b4e-40c9c6346c26",
   "metadata": {},
   "source": [
    "### Creating projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b4069e-43d8-4c03-b075-b06666e5cd58",
   "metadata": {},
   "source": [
    "The first thing you typically do with the `scrapy` tool is create your Scrapy project:\n",
    "\n",
    "```sh\n",
    "scrapy startproject myproject [project_dir]\n",
    "```\n",
    "\n",
    "That will create a Scrapy project under the `project_dir` directory. If `project_dir` wasn’t specified, `project_dir` will be the same as `myproject`.\n",
    "\n",
    "Next, you go inside the new project directory:\n",
    "\n",
    "``` sh\n",
    "cd project_dir\n",
    "```\n",
    "\n",
    "And you’re ready to use the `scrapy` command to manage and control your project from there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a05beb3-9d7c-454e-b9b8-bbcf0df0a5ac",
   "metadata": {},
   "source": [
    "### Controlling projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864a1ee2-9024-4945-a01a-aa388e45a24c",
   "metadata": {},
   "source": [
    "You use the `scrapy` tool _from inside_ your projects to control and manage them.\n",
    "\n",
    "For example, to create a new spider:\n",
    "\n",
    "```\n",
    "scrapy genspider mydomain mydomain.com\n",
    "```\n",
    "\n",
    "Some Scrapy commands (like `crawl`) must be run from inside a Scrapy project. See the [commands reference](#Available-tool-commands) below for more information on which commands must be run from inside projects, and which not.\n",
    "\n",
    "Also keep in mind that some commands may have slightly different behaviours when running them from inside projects. For example, the `fetch` command will use spider-overridden behaviours (such as the `user_agent` attribute to override the user-agent) if the url being fetched is associated with some specific spider. This is intentional, as the `fetch` command is meant to be used to check how spiders are downloading pages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55e7475-de82-47d1-8944-63d700c1fa72",
   "metadata": {},
   "source": [
    "## Available tool commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75c6413-6b2a-40d6-98d8-7385c7346512",
   "metadata": {},
   "source": [
    "This section contains a list of the available built-in commands with a description and some usage examples. Remember, you can always get more info about each command by running:\n",
    "\n",
    "```sh\n",
    "scrapy <command> -h\n",
    "```\n",
    "\n",
    "And you can see all available commands with:\n",
    "\n",
    "```sh\n",
    "scrapy -h\n",
    "```\n",
    "\n",
    "There are two kinds of commands, \n",
    "- those that only work from inside a `Scrapy` project (**Project-specific commands**) and \n",
    "- those that also work without an active Scrapy project (**Global commands**), \n",
    "\n",
    "though they may behave slightly different when running from inside a project (as they would use the project overridden settings)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2465e2e4-2e8a-482a-b9ac-21225ab2a8c3",
   "metadata": {},
   "source": [
    "### Global commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce85567c-e09f-4a3b-a0a4-a69e9cf8904b",
   "metadata": {},
   "source": [
    "- `startproject`\n",
    "- `genspider`\n",
    "- `settings`\n",
    "- `runspider`\n",
    "- `shell`\n",
    "- `fetch`\n",
    "- `view`\n",
    "- `version`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1242a3-0a5c-4f4d-b89d-e77c163c76f5",
   "metadata": {},
   "source": [
    "#### `startproject`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5556c4-dd0f-4266-9b94-b91888db76f9",
   "metadata": {},
   "source": [
    "Syntax: `scrapy startproject <project_name> [project_dir]`\n",
    "\n",
    "Requires project: no\n",
    "\n",
    "Creates a new Scrapy project named `project_name`, under the `project_dir` directory. If `project_dir` wasn’t specified, `project_dir` will be the same as `project_name`.\n",
    "\n",
    "Usage example:\n",
    "```sh\n",
    "$ scrapy startproject myproject\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb24cc9-46a7-4f54-abf9-44bdf6ed5629",
   "metadata": {},
   "source": [
    "#### `genspider`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea80d55-9768-4625-83a8-d9c4f49efe3b",
   "metadata": {},
   "source": [
    "Syntax: `scrapy genspider [-t template] <name> <domain or URL>`\n",
    "\n",
    "Requires project: no\n",
    "\n",
    "New in version 2.6.0: The ability to pass a URL instead of a domain.\n",
    "\n",
    "Create a new spider in the current folder or in the current project’s spiders folder, if called from inside a project. The `<name>` parameter is set as the spider’s name, while `<domain or URL>` is used to generate the `allowed_domains` and `start_urls` spider’s attributes.\n",
    "\n",
    "Usage example:\n",
    "\n",
    "```sh\n",
    "$ scrapy genspider -l\n",
    "```\n",
    "```\n",
    "Available templates:\n",
    "  basic\n",
    "  crawl\n",
    "  csvfeed\n",
    "  xmlfeed\n",
    "```\n",
    "```sh\n",
    "$ scrapy genspider example example.com\n",
    "```\n",
    "```\n",
    "Created spider 'example' using template 'basic'\n",
    "```\n",
    "```sh\n",
    "$ scrapy genspider -t crawl scrapyorg scrapy.org\n",
    "```\n",
    "```\n",
    "Created spider 'scrapyorg' using template 'crawl'\n",
    "```\n",
    "\n",
    "This is just a convenience shortcut command for creating spiders based on pre-defined templates, but certainly not the only way to create spiders. \n",
    "\n",
    "> You can just create the spider source code files yourself, instead of using this command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68c17c5-9995-4470-97d8-d20102db32de",
   "metadata": {},
   "source": [
    "#### `fetch`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee76f657-e31d-418a-b954-8098a043f17a",
   "metadata": {},
   "source": [
    "Syntax: `scrapy fetch <url>`\n",
    "\n",
    "Requires project: no\n",
    "\n",
    "Downloads the given URL using the Scrapy downloader and writes the contents to standard output.\n",
    "\n",
    "The interesting thing about this command is that it fetches the page how the spider would download it. For example, if the spider has a `USER_AGENT` attribute which overrides the User Agent, it will use that one.\n",
    "\n",
    "So this command can be used to “see” how your spider would fetch a certain page.\n",
    "\n",
    "If used outside a project, no particular per-spider behaviour would be applied and it will just use the default Scrapy downloader settings.\n",
    "\n",
    "Supported options:\n",
    "- `--spider SPIDER`: bypass spider autodetection and force use of specific spider\n",
    "- `--headers`: print the response’s HTTP headers instead of the response’s body\n",
    "- `--no-redirect`: do not follow HTTP 3xx redirects (default is to follow them)\n",
    "\n",
    "Usage examples:\n",
    "\n",
    "```sh\n",
    "$ scrapy fetch --nolog http://www.example.com/some/page.html\n",
    "[ ... html content here ... ]\n",
    "\n",
    "$ scrapy fetch --nolog --headers http://www.example.com/\n",
    "{'Accept-Ranges': ['bytes'],\n",
    " 'Age': ['1263   '],\n",
    " 'Connection': ['close     '],\n",
    " 'Content-Length': ['596'],\n",
    " 'Content-Type': ['text/html; charset=UTF-8'],\n",
    " 'Date': ['Wed, 18 Aug 2010 23:59:46 GMT'],\n",
    " 'Etag': ['\"573c1-254-48c9c87349680\"'],\n",
    " 'Last-Modified': ['Fri, 30 Jul 2010 15:30:18 GMT'],\n",
    " 'Server': ['Apache/2.2.3 (CentOS)']}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1db59a4-1160-4dae-ba13-29399aa2701f",
   "metadata": {},
   "source": [
    "#### `shell`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3c60d7-2e40-4cf7-b7b1-71abbb6e788a",
   "metadata": {},
   "source": [
    "Syntax: `scrapy shell [url]`\n",
    "\n",
    "Requires project: no\n",
    "\n",
    "Starts the Scrapy shell for the given URL (if given) or empty if no URL is given. Also supports UNIX-style local file paths, either relative with `./` or `../` prefixes or absolute file paths. See [Scrapy shell](#2.6-Scrapy-shell) for more info.\n",
    "\n",
    "Supported options:\n",
    "- `--spider=SPIDER`: bypass spider autodetection and force use of specific spider\n",
    "- `-c code`: evaluate the code in the shell, print the result and exit\n",
    "- `--no-redirect`: do not follow HTTP 3xx redirects (default is to follow them); this only affects the URL you may pass as argument on the command line; once you are inside the shell, fetch(url) will still follow HTTP redirects by default.\n",
    "\n",
    "Usage example:\n",
    "\n",
    "```sh\n",
    "$ scrapy shell http://www.example.com/some/page.html\n",
    "```\n",
    "```\n",
    "[ ... scrapy shell starts ... ]\n",
    "```\n",
    "```sh\n",
    "$ scrapy shell --nolog http://www.example.com/ -c '(response.status, response.url)'\n",
    "```\n",
    "```\n",
    "(200, 'http://www.example.com/')\n",
    "```\n",
    "```sh\n",
    "# shell follows HTTP redirects by default\n",
    "$ scrapy shell --nolog http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F -c '(response.status, response.url)'\n",
    "```\n",
    "```\n",
    "(200, 'http://example.com/')\n",
    "```\n",
    "```sh\n",
    "# you can disable this with --no-redirect\n",
    "# (only for the URL passed as command line argument)\n",
    "$ scrapy shell --no-redirect --nolog http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F -c '(response.status, response.url)'\n",
    "```\n",
    "```\n",
    "(302, 'http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9e15f1-7f4f-4c7a-aaab-2c348b3ff58b",
   "metadata": {},
   "source": [
    "#### `view`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5319b6b-d66c-407b-b94b-a5fdf178e9fa",
   "metadata": {},
   "source": [
    "Syntax: `scrapy view <url>`\n",
    "\n",
    "Requires project: no\n",
    "\n",
    "Opens the given URL in a browser, as your Scrapy spider would “see” it. Sometimes spiders see pages differently from regular users, so this can be used to check what the spider “sees” and confirm it’s what you expect.\n",
    "\n",
    "Supported options:\n",
    "- `--spider SPIDER`: bypass spider autodetection and force use of specific spider\n",
    "- `--no-redirect`: do not follow HTTP 3xx redirects (default is to follow them)\n",
    "\n",
    "Usage example:\n",
    "```sh\n",
    "$ scrapy view http://www.example.com/some/page.html\n",
    "[ ... browser starts ... ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3455b1d4-9dfe-4ae4-b064-04bdfa0310bd",
   "metadata": {},
   "source": [
    "#### `settings`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d11ea3e-69f0-4ec0-b129-2662e703a239",
   "metadata": {},
   "source": [
    "Syntax: `scrapy settings [options]`\n",
    "\n",
    "Requires project: no\n",
    "\n",
    "Get the value of a Scrapy setting.\n",
    "\n",
    "If used inside a project it’ll show the project setting value, otherwise it’ll show the default Scrapy value for that setting.\n",
    "\n",
    "Example usage:\n",
    "```sh\n",
    "$ scrapy settings --get BOT_NAME\n",
    "scrapybot\n",
    "$ scrapy settings --get DOWNLOAD_DELAY\n",
    "0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d7a7e9-4f69-4246-9f5a-870c36bd8bb7",
   "metadata": {},
   "source": [
    "#### `runspider`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69893570-95cc-4db5-9590-e75f44c9b285",
   "metadata": {},
   "source": [
    "Syntax: `scrapy runspider <spider_file.py>`\n",
    "\n",
    "Requires project: no\n",
    "\n",
    "Run a spider self-contained in a Python file, without having to create a project.\n",
    "\n",
    "Example usage:\n",
    "```sh\n",
    "$ scrapy runspider myspider.py\n",
    "[ ... spider starts crawling ... ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad821fe-a7a6-414e-bd73-1360c6a7ee1a",
   "metadata": {},
   "source": [
    "#### `version`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cccbee-4ff7-484d-8476-946fbb25f811",
   "metadata": {},
   "source": [
    "Syntax: `scrapy version [-v]`\n",
    "\n",
    "Requires project: no\n",
    "\n",
    "Prints the Scrapy version. If used with `-v` it also prints Python, Twisted and Platform info, which is useful for bug reports."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83cbd90-a31e-4cef-8844-78df1a206d20",
   "metadata": {},
   "source": [
    "#### `bench`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c804180-568c-4db6-8512-1a7e41a1d7a2",
   "metadata": {},
   "source": [
    "Syntax: `scrapy bench`\n",
    "\n",
    "Requires project: no\n",
    "\n",
    "Run a quick benchmark test. [Benchmarking](#4.12-Benchmarking)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187bbb2b-c1e0-4d18-87a2-8b48c2c060cc",
   "metadata": {},
   "source": [
    "### Project-only commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3815f3-e657-42fb-8ea8-4a37491682d3",
   "metadata": {},
   "source": [
    "- `crawl`\n",
    "- `check`\n",
    "- `list`\n",
    "- `edit`\n",
    "- `parse`\n",
    "- `bench`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e1e36d-3ac3-416a-9d2b-a7d674c3ec92",
   "metadata": {},
   "source": [
    "#### `crawl`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f72c64f-4d54-4cd2-80c4-a9c4f0b7c90a",
   "metadata": {},
   "source": [
    "Syntax: `scrapy crawl <spider>`\n",
    "\n",
    "Requires project: yes\n",
    "\n",
    "Start crawling using a spider.\n",
    "\n",
    "Supported options:\n",
    "- `-h`, `--help`: show a help message and exit\n",
    "- `-a NAME=VALUE`: set a spider argument (may be repeated)\n",
    "- `--output FILE` or `-o FILE`: append scraped items to the end of `FILE` (use - for stdout), to define format set a colon at the end of the output URI (i.e. `-o FILE:FORMAT`)\n",
    "- `--overwrite-output FILE` or `-O FILE`: dump scraped items into `FILE`, overwriting any existing file, to define format set a colon at the end of the output URI (i.e. `-O FILE:FORMAT`)\n",
    "- `--output-format FORMAT` or `-t FORMAT`: deprecated way to define format to use for dumping items, does not work in combination with `-O`\n",
    "\n",
    "Usage examples:\n",
    "```sh\n",
    "$ scrapy crawl myspider\n",
    "```\n",
    "```\n",
    "[ ... myspider starts crawling ... ]\n",
    "```\n",
    "```sh\n",
    "$ scrapy crawl -o myfile:csv myspider\n",
    "```\n",
    "```\n",
    "[ ... myspider starts crawling and appends the result to the file myfile in csv format ... ]\n",
    "```\n",
    "```sh\n",
    "$ scrapy crawl -O myfile:json myspider\n",
    "```\n",
    "```\n",
    "[ ... myspider starts crawling and saves the result in myfile in json format overwriting the original content... ]\n",
    "```\n",
    "```sh\n",
    "$ scrapy crawl -o myfile -t csv myspider\n",
    "```\n",
    "```\n",
    "[ ... myspider starts crawling and appends the result to the file myfile in csv format ... ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af19fab-8920-4671-8a4d-603ba7540dc7",
   "metadata": {},
   "source": [
    "#### `check`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6813288-ea3d-40a7-bbc3-00b5b7c61002",
   "metadata": {},
   "source": [
    "Syntax: `scrapy check [options] <spider>`\n",
    "\n",
    "Requires project: yes\n",
    "\n",
    "Check spider contracts.\n",
    "\n",
    "Options:\n",
    "\n",
    "```\n",
    "  -h, --help            show this help message and exit\n",
    "  -l, --list            only list contracts, without checking them\n",
    "  -v, --verbose         print contract tests for all spiders\n",
    "\n",
    "Global Options\n",
    "--------------\n",
    "  --logfile FILE        log file. if omitted stderr will be used\n",
    "  -L LEVEL, --loglevel LEVEL\n",
    "                        log level (default: DEBUG)\n",
    "  --nolog               disable logging completely\n",
    "  --profile FILE        write python cProfile stats to FILE\n",
    "  --pidfile FILE        write process ID to FILE\n",
    "  -s NAME=VALUE, --set NAME=VALUE\n",
    "                        set/override setting (may be repeated)\n",
    "  --pdb                 enable pdb on failure\n",
    "```\n",
    "\n",
    "Usage examples:\n",
    "\n",
    "```sh\n",
    "$ scrapy check -l\n",
    "first_spider\n",
    "  * parse\n",
    "  * parse_item\n",
    "second_spider\n",
    "  * parse\n",
    "  * parse_item\n",
    "\n",
    "$ scrapy check\n",
    "[FAILED] first_spider:parse_item\n",
    ">>> 'RetailPricex' field is missing\n",
    "\n",
    "[FAILED] first_spider:parse\n",
    ">>> Returned 92 requests, expected 0..4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c34bdad-5244-4dcf-ba70-ca37ff894e6c",
   "metadata": {},
   "source": [
    "##### Spider contracts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f80a975-dada-4ba0-9d33-888de08573d2",
   "metadata": {},
   "source": [
    "_ChatGPT:_  \n",
    "In Scrapy, `\"spider contracts\"` refer to a feature designed to enforce certain rules or constraints on the output of your spiders (the crawlers you create using Scrapy). These contracts are defined using the `scrapy.contracts` module and are meant to ensure that your spiders are behaving as expected and producing the desired output.\n",
    "\n",
    "Spider contracts can be particularly useful for ensuring data quality and consistency, especially when you're dealing with large-scale web scraping projects where data structure or content might vary across different pages or domains.\n",
    "\n",
    "Here's a brief overview of how spider contracts work in Scrapy:\n",
    "\n",
    "- **Defining Contracts:** You define contracts using Python classes that subclass `scrapy.contracts.Contract`. Within these classes, you define methods to verify specific aspects of the spider output, such as the presence of certain fields or the structure of items.\n",
    "\n",
    "- **Implementing Verification Logic:** Within the contract class methods, you write the logic to verify whether the spider output meets the defined criteria. This logic typically involves inspecting the scraped data and raising exceptions if the criteria are not met.\n",
    "\n",
    "- **Enforcing Contracts:** Once you've defined your contracts, you can enable them for specific spiders by adding the contracts attribute to your spider classes and specifying which contracts to apply.\n",
    "\n",
    "- **Running Contract Checks:** When you run your spiders, Scrapy will automatically apply the specified contracts to the spider output and perform the verification checks. If any of the checks fail, Scrapy will raise an exception, indicating that the spider output does not conform to the defined contracts.\n",
    "\n",
    "By using spider contracts, you can ensure that your spiders are producing reliable and consistent output, which can be essential for downstream processing and analysis of the scraped data.\n",
    "\n",
    "Here's a simple example of a spider contract class that verifies the presence of certain fields in the spider output:\n",
    "\n",
    "```python\n",
    "from scrapy.contracts import Contract\n",
    "\n",
    "\n",
    "class RequiredFieldsContract(Contract):\n",
    "    \"\"\"Contract to verify the presence of required fields in spider output\"\"\"\n",
    "\n",
    "    name = 'required_fields'\n",
    "\n",
    "    def verify(self, output):\n",
    "        required_fields = ['title', 'url', 'content']\n",
    "        for item in output:\n",
    "            for field in required_fields:\n",
    "                if field not in item:\n",
    "                    raise ContractFail(f\"Required field '{field}' missing in item: {item}\")\n",
    "```\n",
    "\n",
    "And here's how you would enable this contract for a specific spider:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = 'myspider'\n",
    "    start_urls = ['http://example.com']\n",
    "\n",
    "    contracts = [RequiredFieldsContract]\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Your parsing logic here\n",
    "        pass\n",
    "```\n",
    "\n",
    "With this setup, Scrapy will verify that each item produced by MySpider contains the required fields specified in the `RequiredFieldsContract`. If any item fails this verification, Scrapy will raise a `ContractFail` exception."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b61150-9b0b-407d-b9d8-94fd1bda2456",
   "metadata": {},
   "source": [
    "#### `list`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc7c1f6-1f6a-4f00-b8d2-e1c04f180554",
   "metadata": {},
   "source": [
    "Syntax: `scrapy list`\n",
    "\n",
    "Requires project: yes\n",
    "\n",
    "List all available spiders in the current project. The output is one spider per line.\n",
    "\n",
    "Usage example:\n",
    "\n",
    "```sh\n",
    "$ scrapy list\n",
    "spider1\n",
    "spider2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddac57f-c471-4df5-9099-e6c17c1d074c",
   "metadata": {},
   "source": [
    "#### `edit`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3a49f7-c3b0-4be7-9cc2-f2fae06ad004",
   "metadata": {},
   "source": [
    "Syntax: `scrapy edit <spider>`\n",
    "\n",
    "Requires project: yes\n",
    "\n",
    "Edit the given spider using the editor defined in the `EDITOR` environment variable or (if unset) the `EDITOR` setting.\n",
    "\n",
    "This command is provided only as a convenience shortcut for the most common case, the developer is of course free to choose any tool or IDE to write and debug spiders.\n",
    "\n",
    "Usage example:\n",
    "```sh\n",
    "$ scrapy edit spider1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafd92c4-d913-4326-b241-eb4b7d1e46df",
   "metadata": {},
   "source": [
    "#### `parse`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0685754-6672-46e3-98bb-9768aaec1fbe",
   "metadata": {},
   "source": [
    "Syntax: `scrapy parse <url> [options]`\n",
    "\n",
    "Requires project: yes\n",
    "\n",
    "Fetches the given URL and parses it with the spider that handles it, using the method passed with the `--callback` option, or `parse` if not given.\n",
    "\n",
    "Supported options:\n",
    "- `--spider=SPIDER`: bypass spider autodetection and force use of specific spider\n",
    "- `--a NAME=VALUE`: set spider argument (may be repeated)\n",
    "- `--callback` or `-c`: spider method to use as callback for parsing the response\n",
    "- `--meta` or `-m`: additional request meta that will be passed to the callback request. This must be a valid json string. Example: `–meta=’{“foo” : “bar”}`’\n",
    "- `--cbkwargs`: additional keyword arguments that will be passed to the callback. This must be a valid json string. Example: `–cbkwargs=’{“foo” : “bar”}`’\n",
    "- `--pipelines`: process items through pipelines\n",
    "- `--rules` or `-r`: use `CrawlSpider` rules to discover the callback (i.e. spider method) to use for parsing the response\n",
    "- `--noitems`: don’t show scraped items\n",
    "- `--nolinks`: don’t show extracted links\n",
    "- `--nocolour`: avoid using pygments to colorize the output\n",
    "- `--depth` or `-d`: depth level for which the requests should be followed recursively (default: 1)\n",
    "- `--verbose` or `-v`: display information for each depth level\n",
    "- `--output` or `-o`: dump scraped items to a file\n",
    "\n",
    "New in version 2.3.\n",
    "\n",
    "Usage example:\n",
    "```sh\n",
    "$ scrapy parse http://www.example.com/ -c parse_item\n",
    "[ ... scrapy log lines crawling example.com spider ... ]\n",
    "\n",
    ">>> STATUS DEPTH LEVEL 1 <<<\n",
    "# Scraped Items  ------------------------------------------------------------\n",
    "[{'name': 'Example item',\n",
    " 'category': 'Furniture',\n",
    " 'length': '12 cm'}]\n",
    "\n",
    "# Requests  -----------------------------------------------------------------\n",
    "[]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39354f2-ed1f-4197-8cc1-2110a94b17da",
   "metadata": {},
   "source": [
    "## Custom project commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de89e9d9-4476-4739-bc20-a9d2fa44f66b",
   "metadata": {},
   "source": [
    "You can also add your custom project commands by using the `COMMANDS_MODULE` setting. See the Scrapy commands in [scrapy/commands](https://github.com/scrapy/scrapy/tree/master/scrapy/commands) for examples on how to implement your commands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc05d3e8-effa-46d8-aa1a-e70a56b092a2",
   "metadata": {},
   "source": [
    "### `COMMANDS_MODULE`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8376e57c-a9fb-4455-9aeb-92a2e8f0bd80",
   "metadata": {},
   "source": [
    "Default: `''` (empty string)\n",
    "\n",
    "A module to use for looking up custom Scrapy commands. This is used to add custom commands for your Scrapy project.\n",
    "\n",
    "Example:\n",
    "```sh\n",
    "COMMANDS_MODULE = \"mybot.commands\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc21c0e-28e7-486e-97b0-1ffdd34fa5e7",
   "metadata": {},
   "source": [
    "### Register commands via `setup.py` entry points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4e606d-945a-4fed-8f16-87198095e466",
   "metadata": {},
   "source": [
    "You can also add Scrapy commands from an external library by adding a `scrapy.commands` section in the entry points of the library `setup.py` file.\n",
    "\n",
    "The following example adds `my_command` command:\n",
    "\n",
    "```python\n",
    "from setuptools import setup, find_packages\n",
    "\n",
    "setup(\n",
    "    name=\"scrapy-mymodule\",\n",
    "    entry_points={\n",
    "        \"scrapy.commands\": [\n",
    "            \"my_command=my_scrapy_module.commands:MyCommand\",\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eb0cc9-17fc-4dce-bd26-e5e0521a3e29",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2.2 Spiders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23da3b48-32dc-4d2d-a293-97a6c9ed9a6b",
   "metadata": {},
   "source": [
    "**Spiders** are classes which define how a certain site (or a group of sites) will be scraped, including \n",
    "- how to perform the `crawl` (i.e. follow links) and \n",
    "- how to extract structured data from their pages (i.e. scraping `items`). \n",
    "\n",
    "In other words, Spiders are the place where you define the custom behaviour for crawling and parsing pages for a particular site (or, in some cases, a group of sites).\n",
    "\n",
    "For spiders, the scraping cycle goes through something like this:\n",
    "\n",
    "1. You start by generating the initial **Requests** to crawl the first URLs, and specify a **callback function** to be called with the `response` downloaded from those requests.\n",
    "\n",
    "    The first requests to perform are obtained by calling the `start_requests()` method which (by default) generates `Request` for the URLs specified in the `start_urls` and the `parse` method as callback function for the Requests.\n",
    "\n",
    "1. In the callback function, you parse the **response** (web page) and return \n",
    "- [item objects](#2.4-Items), \n",
    "- `Request` objects, or \n",
    "- an iterable of these objects. \n",
    "\n",
    "    Those Requests will also contain a callback (maybe the same) and will then be downloaded by Scrapy and then their response handled by the specified callback.\n",
    "\n",
    "3. In callback functions, you parse the page contents, typically using [Selectors](#2.3-Selectors) (but you can also use `BeautifulSoup`, `lxml` or whatever mechanism you prefer) and generate items with the parsed data.\n",
    "\n",
    "4. Finally, the items returned from the spider will be typically \n",
    "- persisted to a database (in some [Item Pipeline](#2.7-Item-Pipeline)) or \n",
    "- written to a file using [Feed exports](#2.8-Feed-exports).\n",
    "\n",
    "Even though this cycle applies (more or less) to any kind of spider, there are different kinds of default spiders bundled into Scrapy for different purposes. We will talk about those types here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc61fdf-4c86-470c-a0a1-1522cab0a38f",
   "metadata": {},
   "source": [
    "## `scrapy.Spider`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff575310-89e1-4257-addb-f5adf0784bdf",
   "metadata": {},
   "source": [
    "```python\n",
    "class scrapy.spiders.Spider\n",
    "class scrapy.Spider\n",
    "```\n",
    "\n",
    "This is the simplest spider, and the one from which every other spider must inherit (including spiders that come bundled with Scrapy, as well as spiders that you write yourself). It doesn’t provide any special functionality. It just provides a default `start_requests()` implementation which sends requests from the `start_urls` spider attribute and calls the spider’s method `parse` for each of the resulting responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4745f42a-c903-4e03-85e8-ba1d6c3bb485",
   "metadata": {},
   "source": [
    "### `name`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92720f61-00f0-4f41-a7fa-0a5920f436bf",
   "metadata": {},
   "source": [
    "A string which defines the name for this spider. \n",
    "\n",
    "The spider `name` is how the spider is located (and instantiated) by Scrapy, so **it must be unique**. However, nothing prevents you from instantiating more than one instance of the same spider. This is the most important spider attribute and it’s required.\n",
    "\n",
    "If the spider scrapes a single domain, a common practice is to name the spider after the domain, with or without the TLD. So, for example, a spider that crawls `mywebsite.com` would often be called `mywebsite`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa2ce8d-9341-464e-a9e8-8ccdede9469a",
   "metadata": {},
   "source": [
    "### `allowed_domains`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa92a733-1013-41f4-bf8c-5ddbdcde39c8",
   "metadata": {},
   "source": [
    "An optional list of strings containing domains that this spider is allowed to crawl. Requests for URLs not belonging to the domain names specified in this list (or their subdomains) won’t be followed if `OffsiteMiddleware` is enabled.\n",
    "\n",
    "Let’s say your target url is `https://www.example.com/1.html`, then add '`example.com`' to the list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96206b29-ff4d-42fd-b294-82cded02b6c9",
   "metadata": {},
   "source": [
    "### `start_urls`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7fc88b-0ab9-46b1-b443-81edf5fc1915",
   "metadata": {},
   "source": [
    "A list of URLs where the spider will begin to crawl from, when no particular URLs are specified. So, the first pages downloaded will be those listed here. The subsequent `Request` will be generated successively from data contained in the start URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ab07e2-1f6b-46bc-ab55-029dddd16834",
   "metadata": {},
   "source": [
    "### `custom_settings`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21e5c20-ed27-4bf7-a705-dc005aba57b5",
   "metadata": {},
   "source": [
    "A dictionary of settings that will be overridden from the project wide configuration when running this spider. It must be defined as a class attribute since the settings are updated before instantiation.\n",
    "\n",
    "For a list of available built-in settings see: [Built-in settings reference](#Built-in-settings-reference)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e79ecce-f740-4b21-bc50-9b9dfb16d004",
   "metadata": {},
   "source": [
    "### `crawler`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15b4cf3-296d-4810-b3a8-a4c4e9d2d9d1",
   "metadata": {},
   "source": [
    "This attribute is set by the `from_crawler()` class method after initializing the class, and links to the `Crawler` object to which this spider instance is bound.\n",
    "\n",
    "Crawlers encapsulate a lot of components in the project for their single entry access (such as extensions, middlewares, signals managers, etc). See [Crawler API](#Crawler-API) to know more about them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410ee02d-2cfb-488d-9713-30771f24b59d",
   "metadata": {},
   "source": [
    "### `settings`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7754e708-f842-45c0-8a4c-63b580516507",
   "metadata": {},
   "source": [
    "Configuration for running this spider. This is a `Settings` instance, see the [Settings](#2.11-Settings) topic for a detailed introduction on this subject."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d907670e-9820-4bd9-8bbb-f317ad16b07f",
   "metadata": {},
   "source": [
    "### `logger`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0212c630-32c0-40e2-96b9-922e4f3026f9",
   "metadata": {},
   "source": [
    "Python logger created with the Spider’s name. You can use it to send log messages through it as described on [Logging from Spiders](#Logging-from-Spiders)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91ffd3e-9476-4491-9ccc-b4978b497cd4",
   "metadata": {},
   "source": [
    "### `state`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1264b83c-fc26-489f-af6d-5dadcace5162",
   "metadata": {},
   "source": [
    "A `dict` you can use to persist some spider state between batches. See [Keeping persistent state between batches](#Keeping-persistent-state-between-batches) to know more about it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efab0609-e32f-4618-8720-bf80a27a8e0e",
   "metadata": {},
   "source": [
    "### `classmethod from_crawler(crawler, *args, **kwargs)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdc3f04-d689-459c-8119-1ad2b4be92fe",
   "metadata": {},
   "source": [
    "This is the class method used by Scrapy to create your spiders.\n",
    "\n",
    "You probably won’t need to override this directly because the default implementation acts as a proxy to the `__init__()` method, calling it with the given arguments `args` and named arguments `kwargs`.\n",
    "\n",
    "Nonetheless, this method sets the `crawler` and `settings` attributes in the new instance so they can be accessed later inside the spider’s code.\n",
    "\n",
    "_Changed in version 2.11: The settings in `crawler.settings` can now be modified in this method, which is handy if you want to modify them based on arguments. As a consequence, these settings aren’t the final values as they can be modified later by e.g. [add-ons](#5.2-Add-ons). For the same reason, most of the `Crawler` attributes aren’t initialized at this point._\n",
    "\n",
    "The final `settings` and the initialized `Crawler` attributes are available in the `start_requests()` method, handlers of the `engine_started` signal and later.\n",
    "\n",
    "**Parameters**\n",
    "- `crawler (Crawler instance)` – crawler to which the spider will be bound\n",
    "- `args (list)` – arguments passed to the `__init__()` method\n",
    "- `kwargs (dict)` – keyword arguments passed to the `__init__()` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d480951-a736-4d04-856c-048b646378f8",
   "metadata": {},
   "source": [
    "### `classmethod update_settings(settings)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a8be40-9650-431d-8140-2911138a174f",
   "metadata": {},
   "source": [
    "The `update_settings()` method is used to modify the spider’s settings and is called during initialization of a spider instance.\n",
    "\n",
    "It takes a `Settings` object as a parameter and can add or update the spider’s configuration values. This method is a class method, meaning that it is called on the Spider class and allows all instances of the spider to share the same configuration.\n",
    "\n",
    "While per-spider settings can be set in `custom_settings`, using `update_settings()` allows you to dynamically \n",
    "- add, \n",
    "- remove or \n",
    "- change settings \n",
    "\n",
    "based on other \n",
    "- settings, \n",
    "- spider attributes or \n",
    "- other factors \n",
    "\n",
    "and use setting priorities other than 'spider'. Also, it’s easy to extend `update_settings()` in a subclass by overriding it, while doing the same with `custom_settings` can be hard.\n",
    "\n",
    "For example, suppose a spider needs to modify `FEEDS`:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = \"myspider\"\n",
    "    custom_feed = {\n",
    "        \"/home/user/documents/items.json\": {\n",
    "            \"format\": \"json\",\n",
    "            \"indent\": 4,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def update_settings(cls, settings):\n",
    "        super().update_settings(settings)\n",
    "        settings.setdefault(\"FEEDS\", {}).update(cls.custom_feed)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aea074-29d9-458e-90eb-a41ecccca030",
   "metadata": {},
   "source": [
    "### `parse(response)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca3b02b-a092-4354-b43b-bc6e598bdca6",
   "metadata": {},
   "source": [
    "This is the default callback used by Scrapy to process downloaded `response`s, when their requests don’t specify a callback.\n",
    "\n",
    "The `parse` method is in charge of processing the `response` and returning scraped data and/or more URLs to follow. Other `Request`s callbacks have the same requirements as the Spider class.\n",
    "\n",
    "This method, as well as any other `Request` callback, must return \n",
    "- a `Request` object, \n",
    "- an [`item` object](#2.4-Items), \n",
    "- an iterable of `Request` objects and/or \n",
    "- [item objects](#2.4-Items), or \n",
    "- `None`.\n",
    "\n",
    "**Parameters**\n",
    "- `response` (Response) – the response to parse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c626767c-3fba-4f9c-b42f-d89327d382da",
   "metadata": {},
   "source": [
    "### `log(message[, level, component])`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bf1671-20a7-4c54-acd0-cb1d8cb5177b",
   "metadata": {},
   "source": [
    "Wrapper that sends a log message through the Spider’s logger, kept for backward compatibility. For more information see [Logging from Spiders](#Logging-from-Spiders)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf3c034-f640-4c9b-a663-bab745d377a8",
   "metadata": {},
   "source": [
    "### `closed(reason)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d44160-f8ad-45ca-8b11-fe2ff42cc58e",
   "metadata": {},
   "source": [
    "Called when the spider closes. This method provides a shortcut to `signals.connect()` for the `spider_closed` signal.\n",
    "\n",
    "Let’s see an example:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = \"example.com\"\n",
    "    allowed_domains = [\"example.com\"]\n",
    "    start_urls = [\n",
    "        \"http://www.example.com/1.html\",\n",
    "        \"http://www.example.com/2.html\",\n",
    "        \"http://www.example.com/3.html\",\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        self.logger.info(\"A response from %s just arrived!\", response.url)\n",
    "\n",
    "    # chat_gpt\n",
    "    def closed(self, reason):\n",
    "        self.logger.info(\"Spider closed: %s\", reason)\n",
    "```\n",
    "\n",
    "_ChatGPT:_  \n",
    "In this example, the `closed(reason)` method is defined within the `MySpider` class. When the spider is about to close, Scrapy will automatically call this method, passing the reason for closure as an argument (`reason`).\n",
    "\n",
    "Inside the `closed(reason)` method, you can include any cleanup logic or additional processing that needs to be performed before the spider closes. In the provided example, it logs a message indicating that the spider has closed and includes the reason for closure (`reason`).\n",
    "\n",
    "This method provides a convenient way to perform actions when the spider is shutting down, such as closing database connections, saving final results, or logging summary information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc85e5fc-fe8d-421d-9318-7d7354a5a791",
   "metadata": {},
   "source": [
    "### More examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ad622a-1ae5-4e3d-80b9-283eadd7079c",
   "metadata": {},
   "source": [
    "Return multiple Requests and items from a single callback:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = \"example.com\"\n",
    "    allowed_domains = [\"example.com\"]\n",
    "    start_urls = [\n",
    "        \"http://www.example.com/1.html\",\n",
    "        \"http://www.example.com/2.html\",\n",
    "        \"http://www.example.com/3.html\",\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for h3 in response.xpath(\"//h3\").getall():\n",
    "            yield {\"title\": h3}\n",
    "\n",
    "        for href in response.xpath(\"//a/@href\").getall():\n",
    "            yield scrapy.Request(response.urljoin(href), self.parse)\n",
    "```\n",
    "\n",
    "Instead of `start_urls` you can use `start_requests()` directly; to give data more structure you can use `Item` objects:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "from myproject.items import MyItem\n",
    "\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = \"example.com\"\n",
    "    allowed_domains = [\"example.com\"]\n",
    "\n",
    "    def start_requests(self):\n",
    "        yield scrapy.Request(\"http://www.example.com/1.html\", self.parse)\n",
    "        yield scrapy.Request(\"http://www.example.com/2.html\", self.parse)\n",
    "        yield scrapy.Request(\"http://www.example.com/3.html\", self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        for h3 in response.xpath(\"//h3\").getall():\n",
    "            yield MyItem(title=h3)\n",
    "\n",
    "        for href in response.xpath(\"//a/@href\").getall():\n",
    "            yield scrapy.Request(response.urljoin(href), self.parse)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7491c1a-a1ff-45c5-8a35-67d9b42768f7",
   "metadata": {},
   "source": [
    "## Spider arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df023785-1f13-428f-9aef-933b0321fac0",
   "metadata": {},
   "source": [
    "Spiders can receive arguments that modify their behaviour. Some common uses for spider arguments are to define the start URLs or to restrict the crawl to certain sections of the site, but they can be used to configure any functionality of the spider.\n",
    "\n",
    "Spider arguments are passed through the `crawl` command using the `-a` option. For example:\n",
    "\n",
    "```sh\n",
    "scrapy crawl myspider -a category=electronics\n",
    "```\n",
    "\n",
    "Spiders can access arguments in their `__init__` methods:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = \"myspider\"\n",
    "\n",
    "    def __init__(self, category=None, *args, **kwargs):\n",
    "        super(MySpider, self).__init__(*args, **kwargs)\n",
    "        self.start_urls = [f\"http://www.example.com/categories/{category}\"]\n",
    "        # ...\n",
    "```\n",
    "\n",
    "The default `__init__` method will take any spider arguments and copy them to the spider as attributes. The above example can also be written as follows:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = \"myspider\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        yield scrapy.Request(f\"http://www.example.com/categories/{self.category}\")\n",
    "```\n",
    "\n",
    "If you are running [Scrapy from a script](#Run-Scrapy-from-a-script), you can specify spider arguments when calling `CrawlerProcess.crawl` or `CrawlerRunner.crawl`:\n",
    "\n",
    "```python\n",
    "process = CrawlerProcess()\n",
    "process.crawl(MySpider, category=\"electronics\")\n",
    "```\n",
    "\n",
    "Keep in mind that spider arguments are only strings. The spider will not do any parsing on its own. If you were to set the `start_urls` attribute from the command line, you would have to parse it on your own into a list using something like `ast.literal_eval()` or `json.loads()` and then set it as an attribute. Otherwise, you would cause iteration over a `start_urls` string (a very common python pitfall) resulting in each character being seen as a separate url.\n",
    "\n",
    "A valid use case is to set the `http` auth credentials used by `HttpAuthMiddleware` or the user agent used by `UserAgentMiddleware`:\n",
    "\n",
    "```sh\n",
    "scrapy crawl myspider -a http_user=myuser -a http_pass=mypassword -a user_agent=mybot\n",
    "```\n",
    "\n",
    "Spider arguments can also be passed through the `Scrapyd` `schedule.json` API. See [Scrapyd documentation](https://scrapyd.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad5eb7d-0502-471b-8aa5-35aa361aa5a3",
   "metadata": {},
   "source": [
    "## Generic Spiders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7c0409-6f64-46ea-ad99-3f6fc56ec0d8",
   "metadata": {},
   "source": [
    "Scrapy comes with some useful generic spiders that you can use to subclass your spiders from. Their aim is to provide convenient functionality for a few common scraping cases, like following all links on a site based on certain rules, crawling from [Sitemaps](https://www.sitemaps.org/index.html), or parsing an XML/CSV feed.\n",
    "\n",
    "For the examples used in the following spiders, we’ll assume you have a project with a `TestItem` declared in a `myproject.items` module:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class TestItem(scrapy.Item):\n",
    "    id = scrapy.Field()\n",
    "    name = scrapy.Field()\n",
    "    description = scrapy.Field()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c43396-6357-470a-badf-e51271c22a39",
   "metadata": {},
   "source": [
    "### CrawlSpider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7690fc-e0c9-4c08-917d-460991431ccd",
   "metadata": {},
   "source": [
    "`class scrapy.spiders.CrawlSpider` [source](https://docs.scrapy.org/en/latest/_modules/scrapy/spiders/crawl.html#CrawlSpider)\n",
    "\n",
    "This is the most commonly used spider for crawling regular websites, as it provides a convenient mechanism for following links by defining a set of rules. It may not be the best suited for your particular web sites or project, but it’s generic enough for several cases, so you can start from it and override it as needed for more custom functionality, or just implement your own spider.\n",
    "\n",
    "Apart from the attributes inherited from `Spider` (that you must specify), this class supports a new attribute:\n",
    "\n",
    "- `rules`\n",
    "\n",
    "Which is a list of one (or more) `Rule` objects. Each `Rule` defines a certain behaviour for crawling the site. Rules objects are described below. If multiple rules match the same link, the first one will be used, according to the order they’re defined in this attribute.\n",
    "\n",
    "This spider also exposes an overridable method:\n",
    "\n",
    "- `parse_start_url(response, **kwargs)` [source](https://docs.scrapy.org/en/latest/_modules/scrapy/spiders/crawl.html#CrawlSpider.parse_start_url)\n",
    "\n",
    "This method is called for each `response` produced for the URLs in the spider’s `start_urls` attribute. It allows to parse the initial responses and must return either \n",
    "- an `item` object, \n",
    "- a `Request` object, or \n",
    "- an `iterable` containing any of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dc0826-cdd8-48cb-a29f-216808516cc6",
   "metadata": {},
   "source": [
    "#### Crawling rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9c6c38-9176-494c-bf7a-f94f2b339c6e",
   "metadata": {},
   "source": [
    "```python\n",
    "class scrapy.spiders.Rule(link_extractor=None, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None, errback=None)` \n",
    "```\n",
    "[source](https://docs.scrapy.org/en/latest/_modules/scrapy/spiders/crawl.html#Rule)\n",
    "\n",
    "- `link_extractor` is a [Link Extractor](#2.10-Link-Extractors) object which defines how links will be extracted from each crawled page. Each produced link will be used to generate a `Request` object, which will contain the link’s text in its meta dictionary (under the `link_text` key). If omitted, a default link extractor created with no arguments will be used, resulting in all links being extracted.\n",
    "\n",
    "- `callback` is a callable or a string (in which case a method from the spider object with that name will be used) to be called for each link extracted with the specified link extractor. This callback receives a `Response` as its first argument and must return either a single instance or an iterable of [item objects](#2.4-Items) and/or `Request` objects (or any subclass of them). As mentioned above, the received `Response` object will contain the text of the link that produced the `Request` in its meta dictionary (under the `link_text` key)\n",
    "\n",
    "- `cb_kwargs` is a dict containing the keyword arguments to be passed to the callback function.\n",
    "\n",
    "- `follow` is a boolean which specifies if links should be followed from each `response` extracted with this rule. If callback is `None` `follow` defaults to `True`, otherwise it defaults to `False`.\n",
    "\n",
    "- `process_links` is a callable, or a string (in which case a method from the spider object with that name will be used) which will be called for each list of links extracted from each response using the specified `link_extractor`. This is mainly used for filtering purposes.\n",
    "\n",
    "- `process_request` is a callable (or a string, in which case a method from the spider object with that name will be used) which will be called for every `Request` extracted by this rule. This callable should take said request as first argument and the `Response` from which the request originated as second argument. It must return a `Request` object or `None` (to filter out the request).\n",
    "\n",
    "- `errback` is a callable or a string (in which case a method from the spider object with that name will be used) to be called if any exception is raised while processing a request generated by the rule. It receives a `Twisted Failure` instance as first parameter.\n",
    "\n",
    "> <b>Warning</b>: Because of its internal implementation, you must explicitly set callbacks for new requests when writing CrawlSpider-based spiders; unexpected behaviour can occur otherwise.\n",
    "\n",
    "_New in version 2.0: The `errback` parameter._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb12be9-3453-4fa9-9177-1253092329d2",
   "metadata": {},
   "source": [
    "#### CrawlSpider example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8b5c7a-be8e-4899-be00-40556d9a118d",
   "metadata": {},
   "source": [
    "Let’s now take a look at an example `CrawlSpider` with rules:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "\n",
    "\n",
    "class MySpider(CrawlSpider):\n",
    "    name = \"example.com\"\n",
    "    allowed_domains = [\"example.com\"]\n",
    "    start_urls = [\"http://www.example.com\"]\n",
    "\n",
    "    rules = (\n",
    "        # Extract links matching 'category.php' (but not matching 'subsection.php')\n",
    "        # and follow links from them (since no callback means follow=True by default).\n",
    "        Rule(LinkExtractor(allow=(r\"category\\.php\",), deny=(r\"subsection\\.php\",))),\n",
    "        # Extract links matching 'item.php' and parse them with the spider's method parse_item\n",
    "        Rule(LinkExtractor(allow=(r\"item\\.php\",)), callback=\"parse_item\"),\n",
    "    )\n",
    "\n",
    "    def parse_item(self, response):\n",
    "        self.logger.info(\"Hi, this is an item page! %s\", response.url)\n",
    "        item = scrapy.Item()\n",
    "        item[\"id\"] = response.xpath('//td[@id=\"item_id\"]/text()').re(r\"ID: (\\d+)\")\n",
    "        item[\"name\"] = response.xpath('//td[@id=\"item_name\"]/text()').get()\n",
    "        item[\"description\"] = response.xpath(\n",
    "            '//td[@id=\"item_description\"]/text()'\n",
    "        ).get()\n",
    "        item[\"link_text\"] = response.meta[\"link_text\"]\n",
    "        url = response.xpath('//td[@id=\"additional_data\"]/@href').get()\n",
    "        return response.follow(\n",
    "            url, self.parse_additional_page, cb_kwargs=dict(item=item)\n",
    "        )\n",
    "\n",
    "    def parse_additional_page(self, response, item):\n",
    "        item[\"additional_data\"] = response.xpath(\n",
    "            '//p[@id=\"additional_data\"]/text()'\n",
    "        ).get()\n",
    "        return item\n",
    "```\n",
    "\n",
    "This spider would start crawling `example.com`’s home page, collecting category links, and item links, parsing the latter with the `parse_item` method. For each item response, some data will be extracted from the HTML using XPath, and an `Item` will be filled with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba05a7a-7db4-4a02-a329-f3f49ad5cdb0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d977100-d354-4111-b0ba-3c47b01dc83b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e19e123-9f7c-410a-933a-5b801a56ff97",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c79cc364-bab4-4ec7-a406-9fb39123b321",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fdd19ae-e6c6-44e8-8635-114b42a6c59d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6e8b05b-8ef2-4551-b3af-525802c5efd6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a7c41ee-29d6-41dc-b4f6-e11ecc64c8c7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "205b828a-f19c-4691-8c62-fc677a0810c9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c846f7c-3076-4db2-a87d-e424a6cdbfce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2189794a-fbbd-4ded-ad53-6ed1013cb4ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49b0b42d-6347-4a43-aab8-20bb01772048",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18ceefc5-e0af-4ec1-84d5-9584c00f29a2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d6c292e-3009-44bb-90bb-0b5dc96d3a7c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92f7690b-00ad-442f-a3e8-15f6a30b6e77",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79114932-10e2-43ed-940b-caa414176c50",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "185810a8-9745-4192-a975-d2a2ca48c395",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37b4f647-24be-4523-b9bd-23c3c42338ea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4c3f633-4055-4804-ae79-f8a9444b6eef",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83170bb3-b1b3-4fa7-8d0e-fcd84eab63a7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7a60bd0-ad5d-46c6-944e-14d7695864ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f6dcf4a-f99e-49f2-a3ac-049295411656",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe3e123c-2d5e-4d8e-826c-eb7f443738ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a54236f-61d7-4c88-bd69-acb69d1ad4e7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34a2c405-8dc2-45ee-abe1-23ea61d59eb4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c83020f-fc3e-4427-a177-b2df7142013d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7031e3ad-d653-46c9-a687-ac0aad49f74c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6441ac21-65bf-4e7f-9485-bc165e86f97c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c1951ba-9574-437c-9e7c-e0bcd5c1fd0f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27348b79-4b71-4335-bd11-7c53540bf7c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31ee05b0-0c8f-4e6f-8976-49cd2c634a5e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2984623-9c75-4959-a1a1-d178a46e8613",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fc17133-e134-4f4d-b1bc-c8fa3a1cfef1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bf12c77-842b-44aa-800c-18883be943c9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f652e9d9-a6f6-4650-9dc4-c571f587cf6b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc836c1b-9bee-4650-b047-0d18f712b678",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2176a897-1c04-490b-841e-ecebc15eb3cb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ecaa713-653c-4cf7-a6c3-969752d7b1cb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44db82ca-3722-4043-a68a-88d4fa9d30b4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52231378-329c-43a4-8701-fa6137937fde",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60c13c34-91e1-4cd5-a1e9-8ae0aaaa5a54",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e5dc560-4c6d-4e4b-8ebc-1fc575c34659",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "663f94ff-d95c-4053-a8e9-79e867447bbc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bed6e9b-829a-450b-8a36-4aa84906c352",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a23d0878-9763-4b9e-a877-bfdf3bc27d37",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fab8a2b6-fa94-4b24-9c15-f79c06af98bc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42910fa5-47c9-472a-8e0f-dc10a41e4fbd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba37ea4d-a6d0-4698-a277-eeb5c1b012f7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7def67f-f742-431b-ac65-b3ea702e6954",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2bbe11d-9beb-4388-9680-a7c09ef0b2df",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "370fbc1d-37f7-4bf1-ae2b-a484d036a935",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "756c4442-aae3-4181-9445-92355a1282ce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "963fb61a-9fce-4afe-ab7c-c53fb634b853",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2174d96-3070-4a33-a0ea-20e754bf193f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e843363b-eb9c-4253-a47d-b7ad41d2e1ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f22b85b9-2501-4017-b488-d08eb25743f5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74c35179-0fdd-4341-a0a0-0ed82abfd706",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ef4ec3e-18d5-47d8-aea9-3d0610a46435",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db8cc4a6-b05f-44d4-8177-2e67c1aaa102",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99390406-04a2-40a8-90a7-9a3b4e1cd11e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffff4872-52a2-4860-bafd-a062478f3ffa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74a97104-6490-41c9-a0fe-fb8703ef9f71",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0df0a8e5-e6dd-4b6f-9b00-3e4ab13669e9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "077ba53a-e7fb-45b3-97e2-4beae36d7f9e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb848ec9-39d0-42ac-91c4-7b84f37bf252",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3181b307-2f04-496d-a677-c3b96eb5fb60",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eacc428c-31c2-4103-9c17-a04a6a0f1685",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64bd4f4e-438e-47dd-b64c-96e6d94bd196",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "813c96d3-162f-49e5-aecf-38ddd9103128",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8634c1aa-a276-44bd-a6c9-3f40363f9cd0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b6ce8f1-50a8-45e3-8212-0bbc585aaaf2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf199003-b181-4e04-84c3-7e3d7e982f78",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b63e285-5871-4b4c-88b0-839ef9e656bd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c84a1e2e-f440-4a2c-a4e6-0f5d8736ba16",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7bb0f0ba-8443-4793-99e3-0c0d63ac9b5d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aef02e1f-3dc3-41c2-803a-9932d82ee2f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3dbea2aa-6ae9-4ad6-bbe1-5eb0377842a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ef75585-9444-45a2-981b-ea6eb0ac1783",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ba3cefa-e189-4289-8ad2-b0f19bc04755",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f39c2953-e301-4263-9ff4-3853d1d5792d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8e3e31c-431b-4b98-86e2-098d40f3d688",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "700c5ae8-ffb1-4ad0-8291-2ea0c4121364",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3af5fbe0-ca65-4916-911e-c37447109de7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fccd541b-c0a2-42f1-ac25-706608d546d2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecb79f98-fb05-4d0e-9d2d-a6d99bb81814",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5207d4c0-f716-4c41-a63b-c00971935d59",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cb82df8-fe35-4c9b-b98d-4fb12873374e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1877b67a-6b89-4b79-b328-08c48b022de8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8256ddb6-dac5-4108-9894-181ae50d42c5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f3f2ae5-98e6-40dc-b601-e63dd6113382",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e84ac4e9-7f5f-4b33-b604-49aa3d621e28",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "472efb7b-8cbd-463e-bd08-ec33bee6eb1c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7cbeb0e-acf7-4910-bd45-4cc032f7ebd7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc1d34d0-b7e1-4574-828a-d223509a7bb2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a24a664-e8e1-4340-baaa-7d69793bf577",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6718497-b1df-4418-b908-2a19a454bd03",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d672216-6f4c-4d33-8f9d-54e982df129c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09585f29-6d09-4a7b-a1cb-950cb6cb8b40",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b21878e8-7559-4f86-bce2-1a52e07842c2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc9291a8-65ea-4f7b-a65e-5825ebdec945",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a80fa174-b7cc-4239-b555-798a2e16bd3c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd1689b0-2dac-4e39-a78b-2102a852513b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40edd53a-d1d9-4116-92c0-8ba9f3bc841f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ecc5dc1-43e2-4e2d-8740-7160a8163323",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58ac8e3b-9988-432f-ad7f-0977036b9d8e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "837750af-85a2-4ffa-a2c6-c62b1d1b64d7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6015d606-ce34-4e61-8356-3510e7c7d9b6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8640ef78-4b36-4cd2-908d-fcdd2d34662e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3a80c95-bd96-456e-9b43-03fa87873628",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "467cea9d-8aa7-4559-ab74-54bc6d00579f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddc93cf2-696d-4539-89a6-caea293d452c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07cc3a4c-1518-4065-adff-285f8d03c39d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd47149e-ac49-48c9-992b-56cbc1d9272f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "703e4ba1-651e-4f00-99a2-149e7d6f5cb8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2082f614-df62-49b2-9cc2-4579d7e53bf5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9285a32-3415-40b9-8bcc-d5cad5df0106",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8d52766-f930-4c94-8f76-9ff95d79316f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80bf1236-d949-4e58-aa67-1e64679d4d1f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11f5bb3b-e413-4c5b-96b4-6a3e358fe209",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "443d4275-6b2d-478f-a46f-4bb058e4d02a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "632916b5-e088-46d5-90f2-256683b10555",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f2e2bc8-6065-407f-84a3-737e485eec91",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72102130-3c38-498f-b557-1f9ddd237e4f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c396704-0a72-458d-a053-d250c1789065",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "282fedd4-15f6-4dd6-8981-219ddd7c7699",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "251fc71a-48e6-40a2-9e85-6919f3f8383f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f9884e0-bf7f-4b84-84e9-d7b770c4bcaa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1481a87f-402e-4a60-ab24-4c102fe73124",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d25b01c-5051-4075-8705-4fca893f8aa5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64a13653-8f21-4b50-8525-dfde3428d151",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8dea94e1-ccdf-4ddc-bca5-07ce0eef88aa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "822881be-d4be-4f2a-88b2-b108ac30c3e6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90028eae-181d-4549-913b-dec120bd491d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cc3c57a-887b-422c-bb55-8efc57ebe40f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92f23c25-862a-4414-94a1-5d2f37bdadcf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b49ddc06-1291-4361-94b4-fc5a8c0e7664",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1976517-6bcd-4e8c-ae9a-1eab4a2e944a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "588f33f1-5628-47bb-91f8-3a3193cc085b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac71d957-d161-4881-ac2e-ea9b06ab0b11",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb933203-edda-48ca-8ca5-7873c6f5c247",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf53ce88-8b56-4fba-9cfa-78aad57b9775",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "130fc6cf-34b2-4c0a-a297-67e17a9076ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83d30708-5834-46f4-bbe3-b2a5a6788833",
   "metadata": {},
   "source": [
    "# 2.3 Selectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad2efea-b857-4781-bf32-6605780aae57",
   "metadata": {},
   "source": [
    "## Selecting element attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf2385d-9394-4362-b6c2-7e0480e91e88",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2.4 Items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f1426d-a4ed-4c61-99e0-b370875b6d02",
   "metadata": {},
   "source": [
    "The main goal in scraping is to extract structured data from unstructured sources, typically, web pages. Spiders may return the extracted data as **items**, Python objects that define **key-value pairs**.\n",
    "\n",
    "Scrapy supports multiple types of items. When you create an item, you may use whichever type of item you want. When you write code that receives an item, your code should work for any item type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0721c01-10b3-47ea-afb2-c8a9989732ee",
   "metadata": {},
   "source": [
    "## Item Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a28f229-6272-41cc-9c85-458cdd775d9e",
   "metadata": {},
   "source": [
    "Scrapy supports the following types of items, via the [itemadapter library](https://github.com/scrapy/itemadapter): \n",
    "- dictionaries, \n",
    "- `Item` objects, \n",
    "- `dataclass` objects, and \n",
    "- `attrs` objects.\n",
    "\n",
    "_ChatGPT:_  \n",
    "Let's discuss the best practices for choosing among these options:\n",
    "\n",
    "- **Dictionaries**: They are the most basic and widely used option. They are suitable for simple scraping tasks where the data structure is straightforward and doesn't require much validation or additional functionality. They are also more lightweight compared to defining custom classes.\n",
    "\n",
    "- **Item objects**: These are recommended for more complex scraping tasks where you need to enforce a specific data structure, perform validation, or implement custom methods for your items. If you prefer a more object-oriented approach and want to leverage features like inheritance and encapsulation, defining Item classes is a good choice.\n",
    "\n",
    "- **Dataclass objects** and **Attrs objects**: These options provide similar benefits, offering a more concise syntax for defining classes with attributes compared to traditional classes. They are suitable when you want a more structured approach compared to dictionaries but prefer a lightweight alternative to defining full-fledged Item classes. Choosing between dataclasses and attrs largely depends on your familiarity with each approach and any specific features offered by either library that you find beneficial.\n",
    "\n",
    "In summary, the best practice for choosing the type of items in Scrapy depends on the complexity of your scraping task, your preference for syntax and features, and your familiarity with the available options. For simpler tasks, dictionaries may suffice, while for more complex tasks, defining custom Item classes, dataclasses, or attrs classes may be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1472f3-b6bc-4c56-83e9-e3f31805138a",
   "metadata": {},
   "source": [
    "### Dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011146ba-009c-4daf-9c10-3b5e0814476b",
   "metadata": {},
   "source": [
    "As an item type, `dict` is convenient and familiar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4e4da2-c98b-4bd9-91d2-236a1fa9877b",
   "metadata": {},
   "source": [
    "### `Item` objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69e308d-7225-4046-9c26-f56dfb51f76f",
   "metadata": {},
   "source": [
    "`Item` provides a `dict`-like API plus additional features that make it the most feature-complete item type:\n",
    "\n",
    "```python\n",
    "class scrapy.item.Item([arg])\n",
    "class scrapy.Item([arg])\n",
    "```\n",
    "\n",
    "- `Item` objects replicate the standard `dict` API, including its `__init__` method.\n",
    "- `Item` allows defining field names, so that:\n",
    "    - `KeyError` is raised when using undefined field names (i.e. prevents typos going unnoticed)\n",
    "    - [Item exporters](#5.8-Item-Exporters) can export all fields by default even if the first scraped object does not have values for all of them\n",
    "- `Item` also allows defining field metadata, which can be used to customize [serialization](#Serialization-of-item-fields).\n",
    "- `trackref` tracks `Item` objects to help find memory leaks (see [Debugging memory leaks](#Debugging-memory-leaks-with-trackref) with trackref).\n",
    "- Item objects also provide the following additional API members:\n",
    "    - `Item.copy()`\n",
    "    - `Item.deepcopy()`: Return a `deepcopy()` of this item.\n",
    "    - `fields`: A dictionary containing all declared fields for this `Item`, not only those populated. The keys are the field names and the values are the `Field` objects used in the Item declaration.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "from scrapy.item import Item, Field\n",
    "\n",
    "\n",
    "class CustomItem(Item):\n",
    "    one_field = Field()\n",
    "    another_field = Field()\n",
    "```\n",
    "\n",
    "_ChatGPT:_  \n",
    "In the context of Scrapy, `Field()` objects are used within the `scrapy.Item` subclass definitions. `Field()` is not a datatype in the traditional sense. It's a class provided by Scrapy for defining the structure of items in your spider. Internally, Scrapy uses `Field()` instances to manage and validate the data that is scraped and stored.\n",
    "\n",
    "When you define an `Item` subclass in Scrapy, you're essentially creating a structured container to hold the data you want to scrape from web pages. Each piece of data you want to extract from a web page corresponds to a field in your Item subclass.\n",
    "\n",
    "The flexibility of `Field()` objects allows you to handle a wide variety of data types encountered during web scraping. When defining your Scrapy Item subclasses, you should choose the most appropriate data type for each field based on the nature of the data you're scraping.\n",
    "\n",
    "Here's a basic example of how you might define a Scrapy Item subclass with fields of different data types:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class ProductItem(scrapy.Item):\n",
    "    name = scrapy.Field()             # String\n",
    "    price = scrapy.Field()            # Float\n",
    "    in_stock = scrapy.Field()         # Boolean\n",
    "    tags = scrapy.Field()             # List\n",
    "    attributes = scrapy.Field()       # Dictionary\n",
    "    release_date = scrapy.Field()     # Date\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a40729-7e3a-49e6-9813-0a14d0c7cda3",
   "metadata": {},
   "source": [
    "### `Dataclass` objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7c8e25-1ea4-4b30-a196-747c8fd3b939",
   "metadata": {},
   "source": [
    "_New in version 2.2._  \n",
    "[`dataclass()`](https://docs.python.org/3/library/dataclasses.html#dataclasses.dataclass) allows defining item classes with field names, so that [item exporters](#5.8-Item-Exporters) can export all fields by default even if the first scraped object does not have values for all of them.\n",
    "\n",
    "Additionally, `dataclass` items also allow to:\n",
    "- define the type and default value of each defined field.\n",
    "- define custom field metadata through `dataclasses.field()`, which can be used to [customize serialization](#Serialization-of-item-fields).\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CustomItem:\n",
    "    one_field: str\n",
    "    another_field: int\n",
    "```\n",
    "\n",
    "> _Note:_ Field types are not enforced at run time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5e5964-5755-48a9-a585-6d32b0721556",
   "metadata": {},
   "source": [
    "### `attr.s` objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c80299-6dd3-45f3-95f9-b5560f80a28d",
   "metadata": {},
   "source": [
    "_New in version 2.2._  \n",
    "[`attr.s()`](https://www.attrs.org/en/stable/api-attr.html#attr.s) allows defining item classes with field names, so that [item exporters](#5.8-Item-Exporters) can export all fields by default even if the first scraped object does not have values for all of them.\n",
    "\n",
    "Additionally, attr.s items also allow to:\n",
    "- define the type and default value of each defined field.\n",
    "- define custom field [metadata](https://www.attrs.org/en/stable/examples.html#metadata), which can be used to [customize serialization](#Serialization-of-item-fields).\n",
    "\n",
    "In order to use this type, the [attrs package](https://www.attrs.org/en/stable/index.html) needs to be installed.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "import attr\n",
    "\n",
    "\n",
    "@attr.s\n",
    "class CustomItem:\n",
    "    one_field = attr.ib()\n",
    "    another_field = attr.ib()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022b4fff-b5a1-43c7-9035-5bab9627118a",
   "metadata": {},
   "source": [
    "## Working with `Item` objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbe41bd-85db-4931-877b-bfc3302ecb0f",
   "metadata": {},
   "source": [
    "### Declaring `Item` subclasses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2675dc9a-9a82-4009-840a-bf35cea1d87c",
   "metadata": {},
   "source": [
    "`Item` subclasses are declared using a simple `class` definition syntax and `Field` objects. Here is an example:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class Product(scrapy.Item):\n",
    "    name = scrapy.Field()\n",
    "    price = scrapy.Field()\n",
    "    stock = scrapy.Field()\n",
    "    tags = scrapy.Field()\n",
    "    last_updated = scrapy.Field(serializer=str)\n",
    "```\n",
    "\n",
    "> _Note:_ Those familiar with Django will notice that Scrapy `Items` are declared similar to [Django Models](https://docs.djangoproject.com/en/dev/topics/db/models/), except that Scrapy `Items` are much simpler as there is no concept of different field types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a9c6bd-939b-4198-aa90-0429561e6117",
   "metadata": {},
   "source": [
    "### Declaring `fields`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c97ede-cdbb-47ad-8d90-89e8d67c5e38",
   "metadata": {},
   "source": [
    "`Field` objects are used to specify metadata for each field. For example, the `serializer` function for the `last_updated` field illustrated in the example above.\n",
    "\n",
    "You can specify any kind of metadata for each field. There is no restriction on the values accepted by `Field` objects. For this same reason, there is no reference list of all available metadata keys. Each key defined in `Field` objects could be used by a different component, and only those components know about it. You can also define and use any other `Field` key in your project too, for your own needs. \n",
    "\n",
    "> The main goal of `Field` objects is to provide a way to define all `field` metadata in one place. \n",
    "\n",
    "Typically, those components whose behaviour depends on each field use certain field keys to configure that behaviour. You must refer to their documentation to see which metadata keys are used by each component.\n",
    "\n",
    "It’s important to note that \n",
    "\n",
    "> the `Field` objects used to declare the `item` do not stay assigned as class attributes. Instead, they can be accessed through the `Item.field` attribute.\n",
    "\n",
    "```python\n",
    "class scrapy.item.Field([arg])\n",
    "class scrapy.Field([arg])\n",
    "```\n",
    "\n",
    "The `Field` class is just an alias to the built-in `dict` class and doesn’t provide any extra functionality or attributes. In other words, `Field` objects are plain-old Python dicts. A separate class is used to support the `item` declaration syntax based on class attributes.\n",
    "\n",
    "> _Note:_ `Field` metadata can also be declared for `dataclass` and `attrs` items. Please refer to the documentation for [`dataclasses.field`](https://docs.python.org/3/library/dataclasses.html#dataclasses.field) and [`attr.ib`](https://www.attrs.org/en/stable/api.html#attr.ib) for additional information.\n",
    "\n",
    "***\n",
    "_ChatGPT:_  \n",
    "In Scrapy, the components the authors are referring to are likely the various parts of the framework that utilize the metadata associated with `Field` objects.\n",
    "\n",
    "Here's a breakdown:\n",
    "\n",
    "- **`Field` objects**: In Scrapy, `Field` objects are used to define the fields (or attributes) of items that will be scraped from web pages. These fields can represent various types of data such as strings, integers, dates, etc. Each `field` can have associated metadata, which provides additional information or instructions about how the `field` should be processed or handled.\n",
    "\n",
    "- **Components**: Scrapy is composed of several components that work together to facilitate the scraping process. These components include spiders, pipelines, middlewares, and extensions, among others. Each of these components may utilize the metadata associated with `Field` objects in different ways to customize the scraping behavior or perform additional processing on the scraped data.\n",
    "\n",
    "- **Metadata keys**: Metadata keys are the specific attributes or properties that can be associated with `Field` objects to provide additional information. Examples of metadata keys include `xpath`, `css`, `serializer`, `output_processor`, etc. These keys can be used to specify how the `field` should be extracted from the web page, processed, or serialized.\n",
    "\n",
    "- **Customization**: The authors are emphasizing that Scrapy allows for flexibility and customization by allowing users to define and use any metadata keys they need for their specific project requirements. This means that users can define their own metadata keys and use them within their spiders, pipelines, or other components to customize the scraping process according to their needs.\n",
    "\n",
    "Overall, the authors are highlighting the flexibility and extensibility of Scrapy, allowing users to define and use metadata keys as needed across various components of their scraping projects.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148025fd-b4f8-44bb-861f-feb6f926d482",
   "metadata": {},
   "source": [
    "Here are some examples of common tasks performed with items, using the `Product` item declared above. You will notice the API is very similar to the dict API.\n",
    "\n",
    "```python\n",
    "class Product(scrapy.Item):\n",
    "    name = scrapy.Field()\n",
    "    price = scrapy.Field()\n",
    "    stock = scrapy.Field()\n",
    "    tags = scrapy.Field()\n",
    "    last_updated = scrapy.Field(serializer=str)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01ad9b2-283b-45e1-a735-d512441d4b7f",
   "metadata": {},
   "source": [
    "### Creating `item`s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9acbea1-51ea-4b0e-a2cf-20617cde027f",
   "metadata": {},
   "source": [
    "```python\n",
    "product = Product(name=\"Desktop PC\", price=1000)\n",
    "print(product)\n",
    "```\n",
    "```\n",
    "Product(name='Desktop PC', price=1000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bd56fd-50ee-4806-bd85-6d1fce32cebc",
   "metadata": {},
   "source": [
    "### Getting `field` values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20932cf-2660-4928-ac3a-443348c8f930",
   "metadata": {},
   "source": [
    "```python\n",
    "product[\"name\"]\n",
    "Desktop PC\n",
    "\n",
    "product.get(\"name\")\n",
    "Desktop PC\n",
    "\n",
    "product[\"price\"]\n",
    "1000\n",
    "\n",
    "product[\"last_updated\"]\n",
    "Traceback (most recent call last):\n",
    "    ...\n",
    "KeyError: 'last_updated'\n",
    "\n",
    "product.get(\"last_updated\", \"not set\")\n",
    "not set\n",
    "\n",
    "product[\"lala\"]  # getting unknown field\n",
    "Traceback (most recent call last):\n",
    "    ...\n",
    "KeyError: 'lala'\n",
    "\n",
    "product.get(\"lala\", \"unknown field\")\n",
    "'unknown field'\n",
    "\n",
    "\"name\" in product  # is name field populated?\n",
    "True\n",
    "\n",
    "\"last_updated\" in product  # is last_updated populated?\n",
    "False\n",
    "\n",
    "\"last_updated\" in product.fields  # is last_updated a declared field?\n",
    "True\n",
    "\n",
    "\"lala\" in product.fields  # is lala a declared field?\n",
    "False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a473abd-58e2-4dbd-b78d-9d832829a8ca",
   "metadata": {},
   "source": [
    "### Setting `field` values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ff7960-9c12-49d4-8e5d-30c990a40aab",
   "metadata": {},
   "source": [
    "```python\n",
    "product[\"last_updated\"] = \"today\"\n",
    "product[\"last_updated\"]\n",
    "today\n",
    "\n",
    "product[\"lala\"] = \"test\"  # setting unknown field\n",
    "Traceback (most recent call last):\n",
    "    ...\n",
    "KeyError: 'Product does not support field: lala'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e15adda-c29f-42bf-a44a-e0cdfc4204b6",
   "metadata": {},
   "source": [
    "### Accessing all populated values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975a840e-9efa-4678-a1e4-571773ae9ec5",
   "metadata": {},
   "source": [
    "To access all populated values, just use the typical dict API:\n",
    "\n",
    "```python\n",
    "product.keys()\n",
    "['price', 'name']\n",
    "\n",
    "product.items()\n",
    "[('price', 1000), ('name', 'Desktop PC')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c0b5f4-90a9-47d1-9324-91e820c43a64",
   "metadata": {},
   "source": [
    "### Copying `item`s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b8e4bb-b9dd-42c6-835a-c7044a309501",
   "metadata": {},
   "source": [
    "To copy an item, you must first decide whether you want a **shallow copy** (by reference) or a **deep copy** (by value).\n",
    "\n",
    "If your item contains mutable values like lists or dictionaries, a _shallow copy will keep references_ to the same mutable values across all different copies.\n",
    "\n",
    "For example, if you have an item with a list of tags, and you create a shallow copy of that item, both the original item and the copy have the same list of tags. Adding a tag to the list of one of the items will add the tag to the other item as well.\n",
    "\n",
    "If that is not the desired behavior, use a deep copy instead.\n",
    "\n",
    "See [`copy`](https://docs.python.org/3/library/copy.html#module-copy) for more information.\n",
    "\n",
    "To create a shallow copy of an item, you can either call `copy()` on an existing item (`product2 = product.copy()`) or instantiate your `item` class from an existing item (`product2 = Product(product)`).\n",
    "\n",
    "To create a deep copy, call `deepcopy()` instead (`product2 = product.deepcopy()`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d581c0f5-cf3b-4733-8935-2ac5d4eee3ae",
   "metadata": {},
   "source": [
    "### Other common tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75494e52-ce3f-4604-a501-3f63d4804805",
   "metadata": {},
   "source": [
    "Creating dicts from items:\n",
    "\n",
    "```python\n",
    "dict(product)  # create a dict from all populated values\n",
    "{'price': 1000, 'name': 'Desktop PC'}\n",
    "```\n",
    "\n",
    "Creating items from dicts:\n",
    "```python\n",
    "Product({\"name\": \"Laptop PC\", \"price\": 1500})\n",
    "Product(price=1500, name='Laptop PC')\n",
    "\n",
    "Product({\"name\": \"Laptop PC\", \"lala\": 1500})  # warning: unknown field in dict\n",
    "Traceback (most recent call last):\n",
    "    ...\n",
    "KeyError: 'Product does not support field: lala'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6fdb99-d011-4eec-b42c-340dbd6c8bb2",
   "metadata": {},
   "source": [
    "### Extending `Item` subclasses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6c1705-65fa-4fd4-8e1d-b6f94dd15d66",
   "metadata": {},
   "source": [
    "You can extend `Item`s (to add more fields or to change some metadata for some fields) by declaring a subclass of your original Item.\n",
    "\n",
    "For example:\n",
    "```python\n",
    "class DiscountedProduct(Product):\n",
    "    discount_percent = scrapy.Field(serializer=str)\n",
    "    discount_expiration_date = scrapy.Field()\n",
    "```\n",
    "\n",
    "You can also extend field metadata by using the previous field metadata and appending more values, or changing existing values, like this:\n",
    "```python\n",
    "class SpecificProduct(Product):\n",
    "    name = scrapy.Field(Product.fields[\"name\"], serializer=my_serializer)\n",
    "```\n",
    "\n",
    "That adds (or replaces) the serializer metadata key for the `name` field, keeping all the previously existing metadata values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72eb04c-c511-4310-ac10-3d099118e021",
   "metadata": {},
   "source": [
    "## Supporting All `Item` Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5202bf46-e09f-4255-8ced-38b0f6d52c2e",
   "metadata": {},
   "source": [
    "In code that receives an `item`, such as methods of [item pipelines](#2.7-Item-Pipeline) or [spider middlewares](#5.4-Spider-Middleware), it is a good practice to use the `ItemAdapter` class and the `is_item()` function to write code that works for any supported item type:\n",
    "\n",
    "- `class itemadapter.ItemAdapter(item: Any)` [source](https://docs.scrapy.org/en/latest/_modules/itemadapter/adapter.html#ItemAdapter): \n",
    "    - Wrapper class to interact with data container objects. It provides a common interface to extract and set data without having to take the object’s type into account.\n",
    "- `itemadapter.is_item(obj: Any)→ bool` [source](https://docs.scrapy.org/en/latest/_modules/itemadapter/utils.html#is_item): \n",
    "    - Return `True` if the given object belongs to one of the supported types, `False` otherwise. Alias for `ItemAdapter.is_item`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9fca74-c362-4813-b1e4-acc866013d13",
   "metadata": {},
   "source": [
    "- `class scrapy.item.ItemMeta(class_name, bases, attrs)` [source](https://docs.scrapy.org/en/latest/_modules/scrapy/item.html#ItemMeta): \n",
    "    - [Metaclass](https://realpython.com/python-metaclasses) of `Item` that handles field definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72b46f0-d92e-4954-b501-34d0e7f3d77a",
   "metadata": {},
   "source": [
    "# 2.5 Item Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63493c25-683c-40dd-a5b7-0709bb5550ab",
   "metadata": {},
   "source": [
    "**Item Loaders** provide a convenient mechanism for populating scraped items. Even though items can be populated directly, Item Loaders provide a much more convenient API for populating them from a scraping process, by automating some common tasks like parsing the raw extracted data before assigning it.\n",
    "\n",
    "In other words, \n",
    "\n",
    "> items provide the _container_ of scraped data, while Item Loaders provide the mechanism for _populating_ that container.\n",
    "\n",
    "Item Loaders are designed to provide a flexible, efficient and easy mechanism for extending and overriding different `field` parsing rules, either by spider, or by source format (`HTML`, `XML`, etc) without becoming a nightmare to maintain.\n",
    "\n",
    "> _Note:_ Item Loaders are an extension of the [itemloaders library](https://itemloaders.readthedocs.io/en/latest/) that make it easier to work with Scrapy by adding support for [responses](#2.9-Requests-and-Responses)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090e3c87-d58c-4b42-a775-e9c3d4ea270c",
   "metadata": {},
   "source": [
    "## Using Item Loaders to populate items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0adb11-5b3b-43b1-9007-e967fd0beb47",
   "metadata": {},
   "source": [
    "To use an Item Loader, you must first instantiate it. You can either instantiate it with an `item` object or without one, in which case an `item` object is automatically created in the Item Loader `__init__` method using the `item` class specified in the `ItemLoader.default_item_class` attribute.\n",
    "\n",
    "Then, you start collecting values into the Item Loader, typically using [Selectors](#2.3-Selectors). You can add more than one value to the same item `field`; the Item Loader will know how to “join” those values later using a proper processing function.\n",
    "\n",
    "> _Note:_ Collected data is internally stored as `list`s, allowing to add several values to the same `field`. If an item argument is passed when creating a loader, each of the item’s values will be stored as-is if it’s already an iterable, or wrapped with a list if it’s a single value.\n",
    "\n",
    "Here is a typical Item Loader usage in a Spider, using the `Product` item declared in the Items chapter:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class Product(scrapy.Item):\n",
    "    name = scrapy.Field()\n",
    "    price = scrapy.Field()\n",
    "    stock = scrapy.Field()\n",
    "    tags = scrapy.Field()\n",
    "    last_updated = scrapy.Field(serializer=str)\n",
    "```\n",
    "\n",
    "```python\n",
    "from scrapy.loader import ItemLoader\n",
    "from myproject.items import Product\n",
    "\n",
    "\n",
    "def parse(self, response):\n",
    "    l = ItemLoader(item=Product(), response=response)\n",
    "    l.add_xpath(\"name\", '//div[@class=\"product_name\"]')\n",
    "    l.add_xpath(\"name\", '//div[@class=\"product_title\"]')\n",
    "    l.add_xpath(\"price\", '//p[@id=\"price\"]')\n",
    "    l.add_css(\"stock\", \"p#stock\")\n",
    "    l.add_value(\"last_updated\", \"today\")  # you can also use literal values\n",
    "    return l.load_item()\n",
    "```\n",
    "\n",
    "By quickly looking at that code, we can see the `name` field is being extracted from two different XPath locations in the page:\n",
    "\n",
    "```html\n",
    "//div[@class=\"product_name\"]\n",
    "\n",
    "//div[@class=\"product_title\"]\n",
    "```\n",
    "\n",
    "In other words, data is being collected by extracting it from two XPath locations, using the `add_xpath()` method. This is the data that will be assigned to the `name` field later.\n",
    "\n",
    "Afterwards, similar calls are used for `price` and `stock` fields (the latter using a CSS selector with the `add_css()` method), and finally the `last_update` field is populated directly with a literal value (`today`) using a different method: `add_value()`.\n",
    "\n",
    "Finally, when all data is collected, the `ItemLoader.load_item()` method is called which actually returns the `item` populated with the data previously extracted and collected with the `add_xpath()`, `add_css()`, and `add_value()` calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2de0ab-b359-46c0-8db4-734538db2a46",
   "metadata": {},
   "source": [
    "## Working with `dataclass` items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e47f4d4-3bf3-4407-82f3-f209838386f7",
   "metadata": {},
   "source": [
    "By default, `dataclass` items require all fields to be passed when created. This could be an issue when using `dataclass` items with item loaders: \n",
    "- unless a pre-populated item is passed to the loader, fields will be populated incrementally using the loader’s `add_xpath()`, `add_css()` and `add_value()` methods.\n",
    "\n",
    "One approach to overcome this is to define items using the `field()` function, with a default argument:\n",
    "\n",
    "```python\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InventoryItem:\n",
    "    name: Optional[str] = field(default=None)\n",
    "    price: Optional[float] = field(default=None)\n",
    "    stock: Optional[int] = field(default=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2220ce24-0325-47b8-96cb-b8798d7348a1",
   "metadata": {},
   "source": [
    "## Input and Output processors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49470cb-b2bf-4057-a6e7-d4137709b106",
   "metadata": {},
   "source": [
    "An Item Loader contains one **input processor** and one **output processor** for each (`item`) `field`. \n",
    "\n",
    "The input processor processes the extracted data as soon as it’s received (through the `add_xpath()`, `add_css()` or `add_value()` methods) and the result of the input processor is collected and kept inside the `ItemLoader`. \n",
    "\n",
    "After collecting all data, the `ItemLoader.load_item()` method is called to populate and get the populated `item` object. That’s when the output processor is called with the data previously collected (and processed using the input processor). The result of the output processor is the final value that gets assigned to the `item`.\n",
    "\n",
    "Let’s see an example to illustrate how the input and output processors are called for a particular `field` (the same applies for any other field):\n",
    "\n",
    "```python\n",
    "l = ItemLoader(Product(), response)\n",
    "l.add_xpath(\"name\", xpath1)  # (1)\n",
    "l.add_xpath(\"name\", xpath2)  # (2)\n",
    "l.add_css(\"name\", css)  # (3)\n",
    "l.add_value(\"name\", \"test\")  # (4)\n",
    "return l.load_item()  # (5)\n",
    "```\n",
    "\n",
    "So what happens is:\n",
    "1. Data from `xpath1` is extracted, and passed through the input processor of the `name` field. The result of the input processor is collected and kept in the Item Loader (but not yet assigned to the item).\n",
    "1. Data from `xpath2` is extracted, and passed through the same input processor used in (`1`). The result of the input processor is appended to the data collected in (`1`) (if any).\n",
    "1. This case is similar to the previous ones, except that the data is extracted from the css `CSS` selector, and passed through the same input processor used in (`1`) and (`2`). The result of the input processor is appended to the data collected in (`1`) and (`2`) (if any).\n",
    "1. This case is also similar to the previous ones, except that the value to be collected is assigned directly, instead of being extracted from a `XPath` expression or a `CSS` selector. However, the value is still passed through the input processors. In this case, since the value is not iterable it is converted to an iterable of a single element before passing it to the input processor, because input processor always receive iterables.\n",
    "1. The data collected in steps (`1`), (`2`), (`3`) and (`4`) is passed through the output processor of the `name` field. The result of the output processor is the value assigned to the `name` field in the `item`.\n",
    "\n",
    "It’s worth noticing that processors are just callable objects, which are called with the data to be parsed, and return a parsed value. So you can use any function as input or output processor. The only requirement is that they must accept one (and only one) positional argument, which will be an iterable.\n",
    "\n",
    "_Changed in version 2.0: Processors no longer need to be methods._\n",
    "\n",
    "> _Note:_ Both input and output processors must receive an iterable as their first argument. The output of those functions can be anything. The result of input processors will be appended to an internal list (in the `Loader`) containing the collected values (for that `field`). The result of the output processors is the value that will be finally assigned to the `item`.\n",
    "\n",
    "The other thing you need to keep in mind is that the values returned by input processors are collected internally (in lists) and then passed to output processors to populate the fields.\n",
    "\n",
    "Last, but not least, itemloaders come with some [commonly used processors](https://itemloaders.readthedocs.io/en/latest/built-in-processors.html#built-in-processors) built-in for convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013f7594-f771-46a9-8ba3-1e1fd36a6115",
   "metadata": {},
   "source": [
    "### Available [built-in processors](https://itemloaders.readthedocs.io/en/latest/built-in-processors.html#built-in-processors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa887070-6599-4636-a45a-e059e5e128c4",
   "metadata": {},
   "source": [
    "Even though you can use any callable function as input and output processors, `itemloaders` provides some commonly used processors, which are described below.\n",
    "\n",
    "Some of them, like the `MapCompose` (which is typically used as input processor) compose the output of several functions executed in order, to produce the final parsed value.\n",
    "\n",
    "This module provides some commonly used processors for Item Loaders.\n",
    "\n",
    "See documentation in `docs/topics/loaders.rst`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1081d97a-13ab-4b67-b850-b7e3addff7d0",
   "metadata": {},
   "source": [
    "#### `Compose`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296548b8-56af-449c-a452-6c092f1adb30",
   "metadata": {},
   "source": [
    "`class itemloaders.processors.Compose(*functions, **default_loader_context)` [source](https://itemloaders.readthedocs.io/en/latest/_modules/itemloaders/processors.html#Compose)\n",
    "\n",
    "A processor which is constructed from the composition of the given functions. This means that each input value of this processor is passed to the first function, and the result of that function is passed to the second function, and so on, until the last function returns the output value of this processor.\n",
    "\n",
    "By default, stop process on `None` value. This behaviour can be changed by passing keyword argument `stop_on_none=False`.\n",
    "\n",
    "Example:\n",
    "\n",
    "```ipython\n",
    ">>> from itemloaders.processors import Compose\n",
    ">>> proc = Compose(lambda v: v[0], str.upper)\n",
    ">>> proc(['hello', 'world'])\n",
    "'HELLO'\n",
    "```\n",
    "\n",
    "Each function can optionally receive a [`loader_context`](https://itemloaders.readthedocs.io/en/latest/loaders-context.html#loaders-context) parameter. For those which do, this processor will pass the currently active `Loader` context through that parameter.\n",
    "\n",
    "The keyword arguments passed in the `__init__` method are used as the default `Loader` context values passed to each function call. However, the final `Loader` context values passed to functions are overridden with the currently active `Loader` context accessible through the `ItemLoader.context` attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07da42ee-ab34-4a1e-a82b-7b1ba491db7f",
   "metadata": {},
   "source": [
    "#### `Identity`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef978993-c814-415d-b080-c9d1e3a3bc83",
   "metadata": {},
   "source": [
    "`class itemloaders.processors.Identity` [source](https://itemloaders.readthedocs.io/en/latest/_modules/itemloaders/processors.html#Identity)\n",
    "\n",
    "The simplest processor, which doesn’t do anything. It returns the original values unchanged. It doesn’t receive any `__init__` method arguments, nor does it accept Loader contexts.\n",
    "\n",
    "Example:\n",
    "\n",
    "```ipython\n",
    ">>> from itemloaders.processors import Identity\n",
    ">>> proc = Identity()\n",
    ">>> proc(['one', 'two', 'three'])\n",
    "['one', 'two', 'three']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbf3557-8fc3-4bdf-9efe-44be3d331757",
   "metadata": {},
   "source": [
    "#### `Join`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0e1e93-0fb6-4daa-8e1a-63617420db65",
   "metadata": {},
   "source": [
    "`class itemloaders.processors.Join(separator=' ')` [source](https://itemloaders.readthedocs.io/en/latest/_modules/itemloaders/processors.html#Join)\n",
    "\n",
    "Returns the values joined with the separator given in the `__init__` method, which defaults to `' '`. It doesn’t accept **Loader contexts**.\n",
    "\n",
    "When using the default separator, this processor is equivalent to the function: `' '.join`.\n",
    "\n",
    "Examples:\n",
    "\n",
    "```ipython\n",
    ">>> from itemloaders.processors import Join\n",
    ">>> proc = Join()\n",
    ">>> proc(['one', 'two', 'three'])\n",
    "'one two three'\n",
    ">>> proc = Join('<br>')\n",
    ">>> proc(['one', 'two', 'three'])\n",
    "'one<br>two<br>three'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb782ce-6c50-43da-a9a6-d592a21f46fc",
   "metadata": {},
   "source": [
    "#### `MapCompose`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6662a3-3f28-4d0d-91ee-ff4e8a6b62e1",
   "metadata": {},
   "source": [
    "`class itemloaders.processors.MapCompose(*functions, **default_loader_context)` [source](https://itemloaders.readthedocs.io/en/latest/_modules/itemloaders/processors.html#MapCompose)\n",
    "\n",
    "A processor which is constructed from the composition of the given functions, similar to the `Compose` processor. The difference with this processor is the way internal results are passed among functions, which is as follows:\n",
    "\n",
    "The input value of this processor is iterated and the first function is applied to each element. The results of these function calls (one for each element) are concatenated to construct a new iterable, which is then used to apply the second function, and so on, until the last function is applied to each value of the list of values collected so far. The output values of the last function are concatenated together to produce the output of this processor.\n",
    "\n",
    "Each particular function can return a value or a list of values, which is flattened with the list of values returned by the same function applied to the other input values. The functions can also return `None` in which case the output of that function is ignored for further processing over the chain.\n",
    "\n",
    "This processor provides a convenient way to compose functions that only work with single values (instead of iterables). For this reason the `MapCompose` processor is typically used as input processor, since data is often extracted using the `extract()` method of [parsel selectors](https://parsel.readthedocs.io/en/latest/parsel.html#parsel.selector.Selector.extract), which returns a list of unicode strings.\n",
    "\n",
    "The example below should clarify how it works:\n",
    "\n",
    "```ipython\n",
    ">>> def filter_world(x):\n",
    "...     return None if x == 'world' else x\n",
    "...\n",
    ">>> from itemloaders.processors import MapCompose\n",
    ">>> proc = MapCompose(filter_world, str.upper)\n",
    ">>> proc(['hello', 'world', 'this', 'is', 'something'])\n",
    "['HELLO', 'THIS', 'IS', 'SOMETHING']\n",
    "```\n",
    "\n",
    "As with the `Compose` processor, functions can receive `Loader` contexts, and `__init__` method keyword arguments are used as default context values. See [`Compose`](#Compose) processor for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917a0f59-95c6-4e95-9dee-c4798199b217",
   "metadata": {},
   "source": [
    "#### `SelectJmes`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a1c928-20b9-495f-af4f-808f5b7af398",
   "metadata": {},
   "source": [
    "`class itemloaders.processors.SelectJmes(json_path)` [source](https://itemloaders.readthedocs.io/en/latest/_modules/itemloaders/processors.html#SelectJmes)\n",
    "\n",
    "Query the input string for the `jmespath` (given at instantiation), and return the answer. Requires : [jmespath](https://github.com/jmespath/jmespath) \n",
    "\n",
    "> _Note:_ `SelectJmes` accepts only one input element at a time.\n",
    "\n",
    "Example:\n",
    "\n",
    "```ipython\n",
    ">>> from itemloaders.processors import SelectJmes, Compose, MapCompose\n",
    ">>> proc = SelectJmes(\"foo\") #for direct use on lists and dictionaries\n",
    ">>> proc({'foo': 'bar'})\n",
    "'bar'\n",
    ">>> proc({'foo': {'bar': 'baz'}})\n",
    "{'bar': 'baz'}\n",
    "```\n",
    "\n",
    "Working with Json:\n",
    "\n",
    "```ipython\n",
    ">>> import json\n",
    ">>> proc_single_json_str = Compose(json.loads, SelectJmes(\"foo\"))\n",
    ">>> proc_single_json_str('{\"foo\": \"bar\"}')\n",
    "'bar'\n",
    ">>> proc_json_list = Compose(json.loads, MapCompose(SelectJmes('foo')))\n",
    ">>> proc_json_list('[{\"foo\":\"bar\"}, {\"baz\":\"tar\"}]')\n",
    "['bar']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a48e076-bb64-4b60-91e1-2e6fa79ee308",
   "metadata": {},
   "source": [
    "#### `TakeFirst`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570a8757-bb83-4d17-b9b3-87bc98b6fe20",
   "metadata": {},
   "source": [
    "`class itemloaders.processors.TakeFirst` [source](https://itemloaders.readthedocs.io/en/latest/_modules/itemloaders/processors.html#TakeFirst)\n",
    "\n",
    "Returns the first non-null/non-empty value from the values received, so it’s typically used as an output processor to single-valued fields. It doesn’t receive any `__init__` method arguments, nor does it accept Loader contexts.\n",
    "\n",
    "Example:\n",
    "\n",
    "```ipython\n",
    ">>> from itemloaders.processors import TakeFirst\n",
    ">>> proc = TakeFirst()\n",
    ">>> proc(['', 'one', 'two', 'three'])\n",
    "'one'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f97863-5f5f-438b-93eb-365cb2af873f",
   "metadata": {},
   "source": [
    "## Declaring Item Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79a107f-9b2d-4e34-bb54-58fc3fea810f",
   "metadata": {},
   "source": [
    "Item Loaders are declared using a class definition syntax. Here is an example:\n",
    "\n",
    "```python\n",
    "from itemloaders.processors import TakeFirst, MapCompose, Join\n",
    "from scrapy.loader import ItemLoader\n",
    "\n",
    "\n",
    "class ProductLoader(ItemLoader):\n",
    "    default_output_processor = TakeFirst()\n",
    "\n",
    "    name_in = MapCompose(str.title)\n",
    "    name_out = Join()\n",
    "\n",
    "    price_in = MapCompose(str.strip)\n",
    "\n",
    "    # ...\n",
    "```\n",
    "\n",
    "As you can see, input processors are declared using the `_in` suffix while output processors are declared using the `_out` suffix. And you can also declare a default input/output processors using the `ItemLoader.default_input_processor` and `ItemLoader.default_output_processor` attributes.\n",
    "\n",
    "_ChatGPT:_  \n",
    "`*_in` and `*_out` are special attributes recognized by Scrapy's ItemLoader class for defining input and output processors, respectively.\n",
    "\n",
    "- `*_in`: Specifies how input data should be processed before being assigned to the corresponding item field.\n",
    "- `*_out`: Specifies how the output data should be processed before being outputted from the item loader.\n",
    "\n",
    "These attributes allow you to define custom processing logic for input and output data, providing flexibility in how data is handled during the loading process.\n",
    "\n",
    "When you define input processors (`*_in`), they are applied to the input data (e.g., extracted values from XPath expressions) before assigning them to item fields. This allows you to clean or transform the input data as needed.\n",
    "\n",
    "Similarly, when you define output processors (`*_out`), they are applied to the output data (e.g., final values of item fields) before outputting them from the item loader. This allows you to further process or format the output data before it's returned from the loader.\n",
    "\n",
    "By using input and output processors, you can customize how data is processed during the loading process, ensuring that it meets your specific requirements and standards. This enhances the flexibility and power of Scrapy's item loading mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdffbccd-7c42-4892-a6c0-a77b9abbba84",
   "metadata": {},
   "source": [
    "## Declaring Input and Output Processors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fb02ec-3fda-40d9-9960-414a862e001d",
   "metadata": {},
   "source": [
    "As seen in the previous section, input and output processors can be declared in the Item Loader definition, and it’s very common to declare input processors this way. However, there is one more place where you can specify the input and output processors to use: in the `Item` `Field` metadata. Here is an example:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "from itemloaders.processors import Join, MapCompose, TakeFirst\n",
    "from w3lib.html import remove_tags\n",
    "\n",
    "\n",
    "def filter_price(value):\n",
    "    if value.isdigit():\n",
    "        return value\n",
    "\n",
    "\n",
    "class Product(scrapy.Item):\n",
    "    name = scrapy.Field(\n",
    "        input_processor=MapCompose(remove_tags),\n",
    "        output_processor=Join(),\n",
    "    )\n",
    "    price = scrapy.Field(\n",
    "        input_processor=MapCompose(remove_tags, filter_price),\n",
    "        output_processor=TakeFirst(),\n",
    "    )\n",
    "```\n",
    "\n",
    "```ipython\n",
    ">>> from scrapy.loader import ItemLoader\n",
    ">>> il = ItemLoader(item=Product())\n",
    ">>> il.add_value(\"name\", [\"Welcome to my\", \"<strong>website</strong>\"])\n",
    ">>> il.add_value(\"price\", [\"&euro;\", \"<span>1000</span>\"])\n",
    ">>> il.load_item()\n",
    "{'name': 'Welcome to my website', 'price': '1000'}\n",
    "```\n",
    "\n",
    "The precedence order, for both input and output processors, is as follows:\n",
    "1. Item Loader field-specific attributes: `field_in` and `field_out` (most precedence)\n",
    "1. Field metadata (`input_processor` and `output_processor` key)\n",
    "1. Item Loader defaults: `ItemLoader.default_input_processor()` and `ItemLoader.default_output_processor()` (least precedence)\n",
    "\n",
    "See also: [Reusing and extending Item Loaders](#Reusing-and-extending-Item-Loaders)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350a87b0-eefa-499a-a935-34cf5d816ad4",
   "metadata": {},
   "source": [
    "## Item Loader Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39688f3-ce59-4cbe-87e9-3ee3e4e05940",
   "metadata": {},
   "source": [
    "The **Item Loader Context** is a `dict` of arbitrary key/values which is shared among all input and output processors in the **Item Loader**. It can be passed when declaring, instantiating or using Item Loader. They are used to modify the behaviour of the input/output processors.\n",
    "\n",
    "For example, suppose you have a function `parse_length` which receives a text value and extracts a length from it:\n",
    "\n",
    "```python\n",
    "def parse_length(text, loader_context):\n",
    "    unit = loader_context.get(\"unit\", \"m\")\n",
    "    # ... length parsing code goes here ...\n",
    "    return parsed_length\n",
    "```\n",
    "\n",
    "By accepting a `loader_context` argument the function is explicitly telling the Item Loader that it’s able to receive an Item Loader context, so the Item Loader passes the currently active context when calling it, and the processor function (`parse_length` in this case) can thus use them.\n",
    "\n",
    "There are several ways to modify Item Loader context values:\n",
    "\n",
    "- By modifying the currently active Item Loader context (`context` attribute):\n",
    "\n",
    "```python\n",
    "loader = ItemLoader(product)\n",
    "loader.context[\"unit\"] = \"cm\"\n",
    "```\n",
    "\n",
    "- On Item Loader instantiation (the keyword arguments of Item Loader `__init__` method are stored in the Item Loader context):\n",
    "\n",
    "```python\n",
    "loader = ItemLoader(product, unit=\"cm\")\n",
    "```\n",
    "\n",
    "- On Item Loader declaration, for those input/output processors that support instantiating them with an Item Loader context. `MapCompose` is one of them:\n",
    "\n",
    "```python\n",
    "class ProductLoader(ItemLoader):\n",
    "    length_out = MapCompose(parse_length, unit=\"cm\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9486e40-1001-41e1-8acb-a8740cdc1f9d",
   "metadata": {},
   "source": [
    "## `ItemLoader` objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86848bc6-3e27-4968-8334-8b43793860cd",
   "metadata": {},
   "source": [
    "```python\n",
    "class scrapy.loader.ItemLoader(item=None, selector=None, response=None, parent=None, **context)\n",
    "``` \n",
    "[source](https://docs.scrapy.org/en/latest/_modules/scrapy/loader.html#ItemLoader)\n",
    "\n",
    "A user-friendly abstraction to populate an item with data by applying field processors to scraped data. When instantiated with a `selector` or a `response` it supports data extraction from web pages using selectors.\n",
    "\n",
    "**Parameters**\n",
    "- `item (scrapy.item.Item)` – The item instance to populate using subsequent calls to \n",
    "    - `add_xpath()`, \n",
    "    - `add_css()`, or \n",
    "    - `add_value()`.\n",
    "- `selector (Selector object)` – The selector to extract data from, when using the \n",
    "    - `add_xpath()`, \n",
    "    - `add_css()`, \n",
    "    - `replace_xpath()`, or \n",
    "    - `replace_css()` method.\n",
    "- `response (Response object)` – The response used to construct the selector using the `default_selector_class`, unless the selector argument is given, in which case this argument is ignored.\n",
    "\n",
    "If no `item` is given, one is instantiated automatically using the class in `default_item_class`.\n",
    "\n",
    "The `item`, `selector`, `response` and remaining keyword arguments are assigned to the Loader context (accessible through the `context` attribute)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d9fb4d-9b4d-4e50-9e6a-746dd8a6fa21",
   "metadata": {},
   "source": [
    "### `item`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ee38c1-c350-4f21-91a8-ea4aca77dc83",
   "metadata": {},
   "source": [
    "The item object being parsed by this Item Loader. This is mostly used as a property so, when attempting to override this value, you may want to check out `default_item_class` first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b94ffe-ab06-419d-95e0-bef157ad3206",
   "metadata": {},
   "source": [
    "### `context`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a03689-c072-4894-967b-0bb5748700a6",
   "metadata": {},
   "source": [
    "The currently active `Context` of this Item Loader."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07272676-f059-4335-8e25-43d5e79c5542",
   "metadata": {},
   "source": [
    "### `default_item_class`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03682e71-007b-4ece-812a-a047f4e8a761",
   "metadata": {},
   "source": [
    "An `item` class (or factory), used to instantiate items when not given in the `__init__` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f12210-4c0f-4234-81ed-36edb3a45d0a",
   "metadata": {},
   "source": [
    "### `default_input_processor`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac48c6f-f3e3-4258-b64a-4bdb3ffb63d9",
   "metadata": {},
   "source": [
    "The default input processor to use for those fields which don’t specify one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6066e7d0-fe82-4b98-98e9-4ddae186f34a",
   "metadata": {},
   "source": [
    "### `default_output_processor`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d3219e-fbc4-47f0-8413-5b7136e85268",
   "metadata": {},
   "source": [
    "The default output processor to use for those fields which don’t specify one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce0e9ef-12c7-4f26-a6e3-7978d6bad491",
   "metadata": {},
   "source": [
    "### `default_selector_class`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74252ca-4979-4bd5-ba00-08577396bc63",
   "metadata": {},
   "source": [
    "The class used to construct the selector of this `ItemLoader`, if only a `response` is given in the `__init__` method. If a `selector` is given in the `__init__` method this attribute is ignored. This attribute is sometimes overridden in subclasses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca64d92-0b3b-4ead-8c80-3ee58e7ea126",
   "metadata": {},
   "source": [
    "### `selector`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c52831-dca6-40cf-94b6-40a4bb8c80d6",
   "metadata": {},
   "source": [
    "The `Selector` object to extract data from. It’s either the `selector` given in the `__init__` method or one created from the `response` given in the `__init__` method using the `default_selector_class`. This attribute is meant to be read-only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc978eb-e61a-4037-86fb-1d254b19bdbb",
   "metadata": {},
   "source": [
    "### `add_css()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4294fda9-0394-436c-8e87-b45b5f6cba89",
   "metadata": {},
   "source": [
    "`add_css(field_name, css, *processors, re=None, **kw)` [source](https://docs.scrapy.org/en/latest/_modules/itemloaders.html#ItemLoader.add_css)\n",
    "\n",
    "Similar to `ItemLoader.add_value()` but receives a CSS `selector` instead of a value, which is used to extract a list of unicode strings from the `selector` associated with this `ItemLoader`.\n",
    "\n",
    "See `get_css()` for `kwargs`.\n",
    "\n",
    "**Parameters**:\n",
    "- `css (str)` – the CSS selector to extract data from\n",
    "\n",
    "Examples:\n",
    "\n",
    "```python\n",
    "# HTML snippet: <p class=\"product-name\">Color TV</p>\n",
    "loader.add_css('name', 'p.product-name')\n",
    "# HTML snippet: <p id=\"price\">the price is $1200</p>\n",
    "loader.add_css('price', 'p#price', re='the price is (.*)')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a369a98b-f9e3-4331-a5dc-fb870769f262",
   "metadata": {},
   "source": [
    "### `add_jmes()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83fffc7-51d5-45a2-b4e7-e48ca7f19121",
   "metadata": {},
   "source": [
    "`add_jmes(field_name, jmes, *processors, re=None, **kw)` [source](https://docs.scrapy.org/en/latest/_modules/itemloaders.html#ItemLoader.add_jmes)\n",
    "\n",
    "Similar to `ItemLoader.add_value()` but receives a JMESPath `selector` instead of a value, which is used to extract a list of unicode strings from the selector associated with this `ItemLoader`.\n",
    "\n",
    "See `get_jmes()` for `kwargs`.\n",
    "\n",
    "**Parameters**\n",
    "- `jmes (str)` – the JMESPath selector to extract data from\n",
    "\n",
    "Examples:\n",
    "\n",
    "```python\n",
    "# HTML snippet: {\"name\": \"Color TV\"}\n",
    "loader.add_jmes('name')\n",
    "# HTML snippet: {\"price\": the price is $1200\"}\n",
    "loader.add_jmes('price', TakeFirst(), re='the price is (.*)')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f428e6e5-9074-45bc-853d-97fb3c83d0a4",
   "metadata": {},
   "source": [
    "### `add_value()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2020a4e-f53f-4e4c-a9e2-10ebd4b80e35",
   "metadata": {},
   "source": [
    "`add_value(field_name, value, *processors, re=None, **kw)` [source](https://docs.scrapy.org/en/latest/_modules/itemloaders.html#ItemLoader.add_value)\n",
    "\n",
    "Process and then add the given value for the given field.\n",
    "\n",
    "The value is first passed through `get_value()` by giving the processors and kwargs, and then passed through the field input processor and its result appended to the data collected for that field. If the field already contains collected data, the new data is added.\n",
    "\n",
    "The given `field_name` can be `None`, in which case values for multiple fields may be added. And the processed value should be a `dict` with `field_name` mapped to values.\n",
    "\n",
    "Examples:\n",
    "\n",
    "```python\n",
    "loader.add_value('name', 'Color TV')\n",
    "loader.add_value('colours', ['white', 'blue'])\n",
    "loader.add_value('length', '100')\n",
    "loader.add_value('name', 'name: foo', TakeFirst(), re='name: (.+)')\n",
    "loader.add_value(None, {'name': 'foo', 'sex': 'male'})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f97d631-e754-4df7-b0f5-534f1fe331ad",
   "metadata": {},
   "source": [
    "### `add_xpath()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99843dd2-b72a-464a-b152-ef26cd37a501",
   "metadata": {},
   "source": [
    "`add_xpath(field_name, xpath, *processors, re=None, **kw)` [source](https://docs.scrapy.org/en/latest/_modules/itemloaders.html#ItemLoader.add_xpath)\n",
    "\n",
    "Similar to `ItemLoader.add_value()` but receives an XPath instead of a value, which is used to extract a list of strings from the selector associated with this `ItemLoader`.\n",
    "\n",
    "See `get_xpath()` for `kwargs`.\n",
    "\n",
    "**Parameters**\n",
    "- `xpath (str)` – the XPath to extract data from\n",
    "\n",
    "Examples:\n",
    "\n",
    "```python\n",
    "# HTML snippet: <p class=\"product-name\">Color TV</p>\n",
    "loader.add_xpath('name', '//p[@class=\"product-name\"]')\n",
    "# HTML snippet: <p id=\"price\">the price is $1200</p>\n",
    "loader.add_xpath('price', '//p[@id=\"price\"]', re='the price is (.*)')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed34cb54-f6a9-4732-bc3e-132470824f88",
   "metadata": {},
   "source": [
    "### `get_collected_values()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cdeaaf-0228-4d31-97a0-9641339e214f",
   "metadata": {},
   "source": [
    "`get_collected_values(field_name)` \n",
    "\n",
    "[source](https://docs.scrapy.org/en/latest/_modules/itemloaders.html#ItemLoader.get_collected_values)\n",
    "\n",
    "Return the collected values for the given field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3f468d-8c88-49f6-bc65-1b2b2b650116",
   "metadata": {},
   "source": [
    "### `get_css()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbc6263-3f0f-4c90-b7bd-6730a0bbe2e0",
   "metadata": {},
   "source": [
    "`get_css(css, *processors, re=None, **kw)` [source](https://docs.scrapy.org/en/latest/_modules/itemloaders.html#ItemLoader.get_collected_values)\n",
    "\n",
    "Similar to `ItemLoader.get_value()` but receives a CSS selector instead of a value, which is used to extract a list of unicode strings from the selector associated with this ItemLoader.\n",
    "\n",
    "**Parameters**\n",
    "- `css (str)` – the CSS selector to extract data from\n",
    "- `re (str or Pattern)` – a regular expression to use for extracting data from the selected CSS region\n",
    "\n",
    "Examples:\n",
    "\n",
    "```python\n",
    "# HTML snippet: <p class=\"product-name\">Color TV</p>\n",
    "loader.get_css('p.product-name')\n",
    "# HTML snippet: <p id=\"price\">the price is $1200</p>\n",
    "loader.get_css('p#price', TakeFirst(), re='the price is (.*)')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ac4c98-f1a7-4b1b-ae73-39f2bdb0944f",
   "metadata": {},
   "source": [
    "### `get_jmes()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb5468a-b97a-49b6-b937-031e01e072bf",
   "metadata": {},
   "source": [
    "`get_jmes(jmes, *processors, re=None, **kw)` [source](https://docs.scrapy.org/en/latest/_modules/itemloaders.html#ItemLoader.get_jmes)\n",
    "\n",
    "Similar to `ItemLoader.get_value()` but receives a JMESPath selector instead of a value, which is used to extract a list of unicode strings from the selector associated with this ItemLoader.\n",
    "\n",
    "**Parameters**\n",
    "- `jmes (str)` – the JMESPath selector to extract data from\n",
    "- `re (str or Pattern)` – a regular expression to use for extracting data from the selected JMESPath\n",
    "\n",
    "Examples:\n",
    "\n",
    "```python\n",
    "# HTML snippet: {\"name\": \"Color TV\"}\n",
    "loader.get_jmes('name')\n",
    "# HTML snippet: {\"price\": the price is $1200\"}\n",
    "loader.get_jmes('price', TakeFirst(), re='the price is (.*)')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e138f0c7-a130-444d-9484-e5accc7c6554",
   "metadata": {},
   "source": [
    "### `get_output_value()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1798dea7-986c-480f-bf35-dfdb4ce0a194",
   "metadata": {},
   "source": [
    "`get_output_value(field_name)` [source](https://docs.scrapy.org/en/latest/_modules/itemloaders.html#ItemLoader.get_output_value)\n",
    "\n",
    "Return the collected values parsed using the output processor, for the given `field`. This method doesn’t populate or modify the `item` at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469604a4-a23f-43f3-966a-e0cf97c865eb",
   "metadata": {},
   "source": [
    "### `get_value()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3138b71a-263a-461a-b6d6-aed9c3a63594",
   "metadata": {},
   "source": [
    "`get_value(value, *processors, re=None, **kw)` [source](https://docs.scrapy.org/en/latest/_modules/itemloaders.html#ItemLoader.get_value)\n",
    "\n",
    "Process the given value by the given processors and keyword arguments.\n",
    "\n",
    "Available keyword arguments:\n",
    "\n",
    "**Parameters**\n",
    "- `re (str or Pattern)` – a regular expression to use for extracting data from the given value using `extract_regex()` method, applied before processors\n",
    "\n",
    "Examples:\n",
    "\n",
    "```ipython\n",
    ">>> from itemloaders import ItemLoader\n",
    ">>> from itemloaders.processors import TakeFirst\n",
    ">>> loader = ItemLoader()\n",
    ">>> loader.get_value('name: foo', TakeFirst(), str.upper, re='name: (.+)')\n",
    "'FOO'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124cba71-8386-4796-a43b-f2e0ec6871ab",
   "metadata": {},
   "source": [
    "### `get_xpath()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a141eb-48bf-49f7-9f3c-73525acfad73",
   "metadata": {},
   "source": [
    "`get_xpath(xpath, *processors, re=None, **kw)` [source](https://docs.scrapy.org/en/latest/_modules/itemloaders.html#ItemLoader.get_xpath)\n",
    "\n",
    "Similar to `ItemLoader.get_value()` but receives an XPath instead of a value, which is used to extract a list of unicode strings from the selector associated with this ItemLoader.\n",
    "\n",
    "**Parameters**\n",
    "- `xpath (str)` – the XPath to extract data from\n",
    "- `re (str or Pattern)` – a regular expression to use for extracting data from the selected XPath region\n",
    "\n",
    "Examples:\n",
    "\n",
    "```python\n",
    "# HTML snippet: <p class=\"product-name\">Color TV</p>\n",
    "loader.get_xpath('//p[@class=\"product-name\"]')\n",
    "# HTML snippet: <p id=\"price\">the price is $1200</p>\n",
    "loader.get_xpath('//p[@id=\"price\"]', TakeFirst(), re='the price is (.*)')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08af38b2-6ccd-404c-87de-b9811f620bc0",
   "metadata": {},
   "source": [
    "### `load_item()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c92e020-0cc6-4e6e-a547-8e866ef36763",
   "metadata": {},
   "source": [
    "[source](https://docs.scrapy.org/en/latest/_modules/itemloaders.html#ItemLoader.load_item)\n",
    "\n",
    "Populate the item with the data collected so far, and return it. The data collected is first passed through the output processors to get the final value to assign to each item field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279845f4-304e-4d1f-8c90-433c8b0ffb12",
   "metadata": {},
   "source": [
    "### `nested_css()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aef7fc4-1675-45c3-8a13-e4d3fabdc64c",
   "metadata": {},
   "source": [
    "`nested_css(css, **context)` [source](https://docs.scrapy.org/en/latest/_modules/itemloaders.html#ItemLoader.nested_css)\n",
    "\n",
    "Create a nested loader with a css selector. The supplied selector is applied relative to selector associated with this `ItemLoader`. The nested loader shares the `item` with the parent `ItemLoader` so calls to `add_xpath()`, `add_value()`, `replace_value()`, etc. will behave as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98624a45-e8f6-40f0-85e4-aaa32aa834c9",
   "metadata": {},
   "source": [
    "### `nested_xpath()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd51ed-922a-4165-9759-1865e519ef46",
   "metadata": {},
   "source": [
    "`nested_xpath(xpath, **context)` [source](https://docs.scrapy.org/en/latest/_modules/itemloaders.html#ItemLoader.nested_xpath)\n",
    "\n",
    "Create a nested loader with an xpath selector. The supplied selector is applied relative to selector associated with this `ItemLoader`. The nested loader shares the item with the parent `ItemLoader` so calls to `add_xpath()`, `add_value()`, `replace_value()`, etc. will behave as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8603f8e-23f0-4784-b749-786fd0aef066",
   "metadata": {},
   "source": [
    "### `replace_css()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e0cd6a-2a2b-41a5-80fb-a254d52044c0",
   "metadata": {},
   "source": [
    "`replace_css(field_name, css, *processors, re=None, **kw)` [source](https://docs.scrapy.org/en/latest/_modules/itemloaders.html#ItemLoader.replace_css)\n",
    "\n",
    "Similar to `add_css()` but replaces collected data instead of adding it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27f6d4b-869e-465f-bee0-4ca58e091ebe",
   "metadata": {},
   "source": [
    "### `replace_jmes()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f4407c-7711-4869-a039-0e74438e3eab",
   "metadata": {},
   "source": [
    "`replace_jmes(field_name, jmes, *processors, re=None, **kw)` [source](https://docs.scrapy.org/en/latest/_modules/itemloaders.html#ItemLoader.replace_jmes)\n",
    "\n",
    "Similar to `add_jmes()` but replaces collected data instead of adding it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e35bd97-ea06-4fcb-b87b-3e7de0453156",
   "metadata": {},
   "source": [
    "### `replace_value()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284ada81-3be4-463e-8c3e-1ba291427795",
   "metadata": {},
   "source": [
    "`replace_value(field_name, value, *processors, re=None, **kw)` [source](https://docs.scrapy.org/en/latest/_modules/itemloaders.html#ItemLoader.replace_value)\n",
    "\n",
    "Similar to `add_value()` but replaces the collected data with the new value instead of adding it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b8888b-c0f5-4b2b-8bbe-1c522228445e",
   "metadata": {},
   "source": [
    "### `replace_xpath()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88be63a8-1744-4101-8e27-c1d49ea25eaf",
   "metadata": {},
   "source": [
    "`replace_xpath(field_name, xpath, *processors, re=None, **kw)` [source](https://docs.scrapy.org/en/latest/_modules/itemloaders.html#ItemLoader.replace_xpath)\n",
    "\n",
    "Similar to `add_xpath()` but replaces collected data instead of adding it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7312cf6-eb3a-46c0-929a-ea21e6ac5215",
   "metadata": {},
   "source": [
    "## Nested Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1c46ea-c721-4697-8868-464089e56ad2",
   "metadata": {},
   "source": [
    "When parsing related values from a subsection of a document, it can be useful to create nested loaders. Imagine you’re extracting details from a footer of a page that looks something like:\n",
    "\n",
    "Example:\n",
    "\n",
    "```html\n",
    "<footer>\n",
    "    <a class=\"social\" href=\"https://facebook.com/whatever\">Like Us</a>\n",
    "    <a class=\"social\" href=\"https://twitter.com/whatever\">Follow Us</a>\n",
    "    <a class=\"email\" href=\"mailto:whatever@example.com\">Email Us</a>\n",
    "</footer>\n",
    "```\n",
    "\n",
    "Without nested loaders, you need to specify the full `xpath` (or `css`) for each value that you wish to extract.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "loader = ItemLoader(item=Item())\n",
    "# load stuff not in the footer\n",
    "loader.add_xpath(\"social\", '//footer/a[@class = \"social\"]/@href')\n",
    "loader.add_xpath(\"email\", '//footer/a[@class = \"email\"]/@href')\n",
    "loader.load_item()\n",
    "```\n",
    "\n",
    "Instead, you can create a nested loader with the footer selector and add values relative to the footer. The functionality is the same but you avoid repeating the footer selector.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "loader = ItemLoader(item=Item())\n",
    "# load stuff not in the footer\n",
    "footer_loader = loader.nested_xpath(\"//footer\")\n",
    "footer_loader.add_xpath(\"social\", 'a[@class = \"social\"]/@href')\n",
    "footer_loader.add_xpath(\"email\", 'a[@class = \"email\"]/@href')\n",
    "# no need to call footer_loader.load_item()\n",
    "loader.load_item()\n",
    "```\n",
    "\n",
    "You can nest loaders arbitrarily and they work with either `xpath` or `css` selectors. As a general guideline, use nested loaders when they make your code simpler but do not go overboard with nesting or your parser can become difficult to read."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4338d61-4f9f-452a-9a34-35d321cf5709",
   "metadata": {},
   "source": [
    "## Reusing and extending Item Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bea4c2-8dad-4725-8d9e-2ae9ecb38a87",
   "metadata": {},
   "source": [
    "As your project grows bigger and acquires more and more spiders, maintenance becomes a fundamental problem, especially when you have to deal with many different parsing rules for each spider, having a lot of exceptions, but also wanting to reuse the common processors.\n",
    "\n",
    "Item Loaders are designed to ease the maintenance burden of parsing rules, without losing flexibility and, at the same time, providing a convenient mechanism for extending and overriding them. For this reason Item Loaders support traditional Python class inheritance for dealing with differences of specific spiders (or groups of spiders).\n",
    "\n",
    "Suppose, for example, that some particular site encloses their product names in three dashes (e.g. `---Plasma TV---`) and you don’t want to end up scraping those dashes in the final product names.\n",
    "\n",
    "Here’s how you can remove those dashes by reusing and extending the default Product Item Loader (`ProductLoader`):\n",
    "\n",
    "```python\n",
    "from itemloaders.processors import MapCompose\n",
    "from myproject.ItemLoaders import ProductLoader\n",
    "\n",
    "\n",
    "def strip_dashes(x):\n",
    "    return x.strip(\"-\")\n",
    "\n",
    "\n",
    "class SiteSpecificLoader(ProductLoader):\n",
    "    name_in = MapCompose(strip_dashes, ProductLoader.name_in)\n",
    "```\n",
    "\n",
    "Another case where extending Item Loaders can be very helpful is when you have multiple source formats, for example `XML` and `HTML`. In the `XML` version you may want to remove CDATA occurrences. Here’s an example of how to do it:\n",
    "\n",
    "```python\n",
    "from itemloaders.processors import MapCompose\n",
    "from myproject.ItemLoaders import ProductLoader\n",
    "from myproject.utils.xml import remove_cdata\n",
    "\n",
    "\n",
    "class XmlProductLoader(ProductLoader):\n",
    "    name_in = MapCompose(remove_cdata, ProductLoader.name_in)\n",
    "```\n",
    "\n",
    "And that’s how you typically extend input processors.\n",
    "\n",
    "As for output processors, \n",
    "\n",
    "> it is more common to declare them in the field metadata, as they usually depend only on the field and not on each specific site parsing rule (as input processors do). \n",
    "\n",
    "See also: [Declaring Input and Output Processors](#Declaring-Input-and-Output-Processors).\n",
    "\n",
    "There are many other possible ways to extend, inherit and override your Item Loaders, and different Item Loaders hierarchies may fit better for different projects. Scrapy only provides the mechanism; it doesn’t impose any specific organization of your Loaders collection - that’s up to you and your project’s needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94113415-1d52-4323-9507-301d4bf73340",
   "metadata": {},
   "source": [
    "# 2.6 Scrapy shell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bc7914-2d6d-4b07-b77c-108702d40e4b",
   "metadata": {},
   "source": [
    "The `Scrapy shell` is an interactive shell where you can try and debug your scraping code very quickly, without having to run the spider. It’s meant to be used for testing data extraction code, but you can actually use it for testing any kind of code as it is also a regular Python shell.\n",
    "\n",
    "The shell is used for testing `XPath` or `CSS` expressions and see how they work and what data they extract from the web pages you’re trying to scrape. It allows you to interactively test your expressions while you’re writing your spider, without having to run the spider to test every change.\n",
    "\n",
    "Once you get familiarized with the Scrapy shell, you’ll see that it’s an invaluable tool for developing and debugging your spiders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7814b9-f0bb-4778-9c18-7f9a014aefdf",
   "metadata": {},
   "source": [
    "## Configuring the shell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b7069e-252f-411f-803a-fb5270e21739",
   "metadata": {},
   "source": [
    "If you have [IPython](https://ipython.org/) installed, the Scrapy shell will use it (instead of the standard Python console). The IPython console is much more powerful and provides smart auto-completion and colorized output, among other things.\n",
    "\n",
    "We highly recommend you install IPython, specially if you’re working on Unix systems (where IPython excels). \n",
    "\n",
    "Scrapy also has support for [bpython](https://bpython-interpreter.org/), and will try to use it where IPython is unavailable.\n",
    "\n",
    "Through Scrapy’s settings you can configure it to use any one of ipython, bpython or the standard python shell, regardless of which are installed. This is done by setting the `SCRAPY_PYTHON_SHELL` environment variable; or by defining it in your `scrapy.cfg`:\n",
    "\n",
    "```sh\n",
    "[settings]\n",
    "shell = bpython\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4198f5a-9cfe-4645-89ff-401293f08738",
   "metadata": {},
   "source": [
    "## Launch the shell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6639249a-819c-4550-accf-dd8131683961",
   "metadata": {},
   "source": [
    "To launch the `Scrapy shell` you can use the shell command like this:\n",
    "\n",
    "```sh\n",
    "scrapy shell <url>\n",
    "```\n",
    "\n",
    "Where the `<url>` is the URL you want to scrape.\n",
    "\n",
    "`shell` also works for local files. This can be handy if you want to play around with a local copy of a web page. shell understands the following syntaxes for local files:\n",
    "\n",
    "```sh\n",
    "# UNIX-style\n",
    "scrapy shell ./path/to/file.html\n",
    "scrapy shell ../other/path/to/file.html\n",
    "scrapy shell /absolute/path/to/file.html\n",
    "\n",
    "# File URI\n",
    "scrapy shell file:///absolute/path/to/file.html\n",
    "```\n",
    "\n",
    "> _Note:_ When using relative file paths, be explicit and prepend them with `./` (or `../` when relevant). `scrapy shell index.html` will not work as one might expect (and this is by design, not a bug).<br>\n",
    "</br>\n",
    "Because shell favors HTTP URLs over File URIs, and `index.html` being syntactically similar to `example.com`, shell will treat `index.html` as a domain name and trigger a DNS lookup error:\n",
    "\n",
    "```sh\n",
    "$ scrapy shell index.html\n",
    "[ ... scrapy shell starts ... ]\n",
    "[ ... traceback ... ]\n",
    "twisted.internet.error.DNSLookupError: DNS lookup failed:\n",
    "address 'index.html' not found: [Errno -5] No address associated with hostname.\n",
    "```\n",
    "\n",
    "`shell` will not test beforehand if a file called `index.html` exists in the current directory. Again, _**be explicit**_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bb2105-3590-414a-b2bb-b5707dccf795",
   "metadata": {},
   "source": [
    "## Using the `shell`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06da515e-f74d-45eb-aa60-97f5585094f7",
   "metadata": {},
   "source": [
    "The Scrapy `shell` is just a regular Python console (or IPython console if you have it available) which provides some additional shortcut functions for convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30f9940-4733-4528-b6e8-98f326d746ad",
   "metadata": {},
   "source": [
    "### Available Shortcuts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbcb319-f3ad-4620-99c8-4866ea05c0cd",
   "metadata": {},
   "source": [
    "- `shelp()` - print a help with the list of available objects and shortcuts\n",
    "\n",
    "- `fetch(url[, redirect=True])` - fetch a new response from the given URL and update all related objects accordingly. You can optionally ask for HTTP `3xx` redirections to not be followed by passing `redirect=False`\n",
    "\n",
    "- `fetch(request)` - fetch a new response from the given `request` and update all related objects accordingly.\n",
    "\n",
    "- `view(response)` - open the given response in your local web browser, for inspection. This will add a [`<base>` tag](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/base) to the `response` body in order for external links (such as images and style sheets) to display properly. Note, however, that this will create a temporary file in your computer, which _**won’t be removed automatically**_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33071bdd-f1aa-4431-8b53-129f57e9abd1",
   "metadata": {},
   "source": [
    "### Available Scrapy objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11404fd9-6a23-4cd2-8575-b9fb7a467d39",
   "metadata": {},
   "source": [
    "The Scrapy `shell` automatically creates some convenient objects from the downloaded page, like the `Response` object and the `Selector` objects (for both `HTML` and `XML` content).\n",
    "\n",
    "Those objects are:\n",
    "\n",
    "- `crawler` - the current `Crawler` object.\n",
    "\n",
    "- `spider` - the `Spider` which is known to handle the URL, or a Spider object if there is no spider found for the current URL\n",
    "\n",
    "- `request` - a `Request` object of the last fetched page. You can modify this request using `replace()` or fetch a new request (without leaving the shell) using the `fetch` shortcut.\n",
    "\n",
    "- `response` - a `Response` object containing the last fetched page\n",
    "\n",
    "- `settings` - the current [Scrapy settings](#2.11-Settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efe8a88-073c-434c-8ffd-d218a4063377",
   "metadata": {},
   "source": [
    "### Example of shell session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daa97fb-e852-48ba-9843-4a6994a059f7",
   "metadata": {},
   "source": [
    "Here’s an example of a typical shell session where we start by scraping the [https://scrapy.org](https://scrapy.org) page, and then proceed to scrape the [https://old.reddit.com/](https://old.reddit.com/) page. Finally, we modify the (Reddit) `request` method to `POST` and re-fetch it getting an error. We end the session by typing `Ctrl-D` (in Unix systems) or `Ctrl-Z` in Windows.\n",
    "\n",
    "Keep in mind that the data extracted here may not be the same when you try it, as those pages are not static and could have changed by the time you test this. The only purpose of this example is to get you familiarized with how the Scrapy `shell` works.\n",
    "\n",
    "First, we launch the shell:\n",
    "\n",
    "```sh\n",
    "scrapy shell 'https://scrapy.org' --nolog\n",
    "```\n",
    "\n",
    "> _Note:_ Remember to always enclose URLs in quotes when running the Scrapy shell from the command line, otherwise URLs containing arguments (i.e. the `&` character) will not work.\n",
    "\n",
    "On Windows, use double quotes instead:\n",
    "\n",
    "```sh\n",
    "scrapy shell \"https://scrapy.org\" --nolog\n",
    "```\n",
    "\n",
    "Then, the shell fetches the URL (using the Scrapy downloader) and prints the list of available objects and useful shortcuts (you’ll notice that these lines all start with the [s] prefix):\n",
    "\n",
    "```\n",
    "[s] Available Scrapy objects:\n",
    "[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\n",
    "[s]   crawler    <scrapy.crawler.Crawler object at 0x7f6b866f6790>\n",
    "[s]   item       {}\n",
    "[s]   request    <GET https://scrapy.org>\n",
    "[s]   response   <200 https://scrapy.org>\n",
    "[s]   settings   <scrapy.settings.Settings object at 0x7f6b868cd5d0>\n",
    "[s]   spider     <DefaultSpider 'default' at 0x7f6b868992d0>\n",
    "[s] Useful shortcuts:\n",
    "[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)\n",
    "[s]   fetch(req)                  Fetch a scrapy.Request and update local objects \n",
    "[s]   shelp()           Shell help (print this help)\n",
    "[s]   view(response)    View response in a browser\n",
    "In [1]:\n",
    "```\n",
    "\n",
    "After that, we can start playing with the objects:\n",
    "\n",
    "```ipython\n",
    "In [1]: response.xpath(\"//title/text()\").get()\n",
    "Out[1]: 'Scrapy | A Fast and Powerful Scraping and Web Crawling Framework'\n",
    "\n",
    "In [2]: fetch('https://old.reddit.com/')\n",
    "\n",
    "In [3]: response.xpath(\"//title/text()\").get()\n",
    "Out[3]: 'reddit: the front page of the internet'\n",
    "\n",
    "In [4]: request = request.replace(method='POST')\n",
    "\n",
    "In [5]: fetch(request)\n",
    "\n",
    "In [6]: response.status\n",
    "Out[6]: 404\n",
    "\n",
    "In [7]: from pprint import pprint\n",
    "\n",
    "In [8]: pprint(response.headers)\n",
    "{b'Accept-Ranges': [b'bytes'],\n",
    " b'Cache-Control': [b'private, max-age=3600'],\n",
    " b'Content-Length': [b'8919'],\n",
    " b'Content-Type': [b'text/html; charset=UTF-8'],\n",
    " b'Date': [b'Sun, 11 Feb 2024 09:04:10 GMT'],\n",
    " b'Nel': [b'{\"report_to\": \"w3-reporting-nel\", \"max_age\": 14400, \"include_sub'\n",
    "          b'domains\": false, \"success_fraction\": 1.0, \"failure_fraction\": 1.'\n",
    "          b'0}'],\n",
    " b'Report-To': [b'{\"group\": \"w3-reporting-nel\", \"max_age\": 14400, \"include_sub'\n",
    "                b'domains\": true,  \"endpoints\": [{ \"url\": \"https://w3-reportin'\n",
    "                b'g-nel.reddit.com/reports\" }]}, {\"group\": \"w3-reporting\", \"ma'\n",
    "                b'x_age\": 14400, \"include_subdomains\": true, \"endpoints\": [{ \"'\n",
    "                b'url\": \"https://w3-reporting.reddit.com/reports\" }]}, {\"group'\n",
    "                b'\": \"w3-reporting-csp\", \"max_age\": 14400, \"include_subdomains'\n",
    "                b'\": true, \"endpoints\": [{ \"url\": \"https://w3-reporting-csp.re'\n",
    "                b'ddit.com/reports\" }]}'],\n",
    " b'Server': [b'snooserv'],\n",
    " b'Set-Cookie': [b'session_tracker=kijjhfblqpcpaadmer.0.1707642250866.Z0FBQUFBQ'\n",
    "                 b'mx5STJLd3pMTnZNMVRORllCWUZybnh2NFJ2S0RyOHhwOExLN0dsZi1xb1dnW'\n",
    "                 b'kZiYVQzenRaRmpCVUVjbFF6SWNRX2NpVGNMdUxmcWtCNlVIUVpHUl8xdEhZT'\n",
    "                 b'DBaMGQ3RW5QZjVVLWRCajBuUTlqcm52a0NkcXh4eWR2VERMVUhZdVdxdEY; '\n",
    "                 b'Domain=reddit.com; Max-Age=7199; Path=/; expires=Sun, 11-Feb'\n",
    "                 b'-2024 11:04:10 GMT; secure',\n",
    "                 b'session_tracker=kijjhfblqpcpaadmer.0.1707642250873.Z0FBQUFBQ'\n",
    "                 b'mx5STJLTzc0ZS04dmsyWTBYY1EzQmc1Y0lpYUpaQ0ViR2ExZVdfMXBGTENoY'\n",
    "                 b'y1MUmJmSWRPTDZGUFJ3cmQ3N3dfUU1rVlNWWlUydHg2eDFfZ2xlUml2VFNqZ'\n",
    "                 b'mprUUdCTmE1SlpHTm1zNm5nZ01SVGNQMjQzWm10a2VNekNRSVhRVEZBWlo; '\n",
    "                 b'Domain=reddit.com; Max-Age=7199; Path=/; expires=Sun, 11-Feb'\n",
    "                 b'-2024 11:04:10 GMT; secure'],\n",
    " b'Vary': [b'accept-encoding'],\n",
    " b'Via': [b'1.1 varnish'],\n",
    " b'X-Frame-Options': [b'SAMEORIGIN'],\n",
    " b'X-Reddit-Internal-Ratelimit-Rls-Type': [b'loid-standard'],\n",
    " b'X-Reddit-Pod-Ip': [b'10.102.228.135:80'],\n",
    " b'X-Ua-Compatible': [b'IE=edge']}\n",
    "```\n",
    "\n",
    "_ChatGPT:_  \n",
    "- **Accept-Ranges**: Indicates that the server accepts byte ranges.\n",
    "- **Cache-Control**: Specifies caching directives for the client, indicating that the response is private and can be cached for a maximum of 3600 seconds (1 hour).\n",
    "- **Content-Length**: Indicates the length of the content body in bytes, which is 8919 bytes in this case.\n",
    "- **Content-Type**: Specifies the type of content being returned, which is text/html with UTF-8 charset.\n",
    "- **Date**: Provides the date and time when the response was generated by the server.\n",
    "- **Nel**: This seems to be a JSON-encoded value providing reporting information.\n",
    "- **Report-To**: Another JSON-encoded value providing reporting information with different groups and endpoints.\n",
    "- **Server**: Specifies the server software being used, in this case, \"snooserv.\"\n",
    "- **Set-Cookie**: Sets one or more cookies on the client-side. There are multiple cookies being set, each with its own attributes like domain, expiration time, and secure flag.\n",
    "- **Vary**: Specifies what request header(s) were used to determine the response.\n",
    "- **Via**: Indicates the intermediate protocols and recipients between the client making the request and the server that ultimately responds to it.\n",
    "- **X-Frame-Options, X-Reddit-Internal-Ratelimit-Rls-Type, X-Reddit-Pod-Ip, X-Ua-Compatible**: These are additional headers providing various types of information, such as frame options, rate-limiting information, pod IP, and browser compatibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b76be1-c15f-42f2-bca7-f42d137cd60c",
   "metadata": {},
   "source": [
    "### Invoking the shell from spiders to inspect responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7db174-e525-4b13-9b9d-7294bcc8eed8",
   "metadata": {},
   "source": [
    "Sometimes you want to inspect the responses that are being processed in a certain point of your spider, if only to check that response you expect is getting there.\n",
    "\n",
    "This can be achieved by using the `scrapy.shell.inspect_response` function.\n",
    "\n",
    "Here’s an example of how you would call it from your spider:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = \"myspider\"\n",
    "    start_urls = [\n",
    "        \"http://example.com\",\n",
    "        \"http://example.org\",\n",
    "        \"http://example.net\",\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        # We want to inspect one specific response.\n",
    "        if \".org\" in response.url:\n",
    "            from scrapy.shell import inspect_response\n",
    "\n",
    "            inspect_response(response, self)\n",
    "\n",
    "        # Rest of parsing code.\n",
    "```\n",
    "\n",
    "When you run the spider, you will get something similar to this:\n",
    "\n",
    "```ipython\n",
    "2014-01-23 17:48:31-0400 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://example.com> (referer: None)\n",
    "2014-01-23 17:48:31-0400 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://example.org> (referer: None)\n",
    "[s] Available Scrapy objects:\n",
    "[s]   crawler    <scrapy.crawler.Crawler object at 0x1e16b50>\n",
    "...\n",
    "\n",
    ">>> response.url\n",
    "'http://example.org'\n",
    "```\n",
    "\n",
    "Then, you can check if the extraction code is working:\n",
    "\n",
    "```ipython\n",
    ">>> response.xpath('//h1[@class=\"fn\"]')\n",
    "[]\n",
    "```\n",
    "\n",
    "Nope, it doesn’t. So you can open the `response` in your web browser and see if it’s the response you were expecting:\n",
    "\n",
    "```ipython\n",
    "view(response)\n",
    "True\n",
    "```\n",
    "\n",
    "Finally you hit `Ctrl-D` (or `Ctrl-Z` in Windows) to exit the shell and resume the crawling:\n",
    "\n",
    "```ipython\n",
    "^D\n",
    "2014-01-23 17:50:03-0400 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://example.net> (referer: None)\n",
    "...\n",
    "```\n",
    "\n",
    "Note that you can’t use the `fetch` shortcut here since the Scrapy engine is blocked by the `shell`. However, after you leave the `shell`, the spider will continue crawling where it stopped, as shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3191754-4504-4d20-9889-58c438ef1544",
   "metadata": {},
   "source": [
    "# 2.7 Item Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8290a0c6-53d8-4953-9e82-495a37f96173",
   "metadata": {},
   "source": [
    "After an item has been scraped by a spider, it is sent to the Item Pipeline which processes it through several components that are executed sequentially.\n",
    "\n",
    "Each **item pipeline component** (sometimes referred as just **“Item Pipeline”**) is a Python class that implements a simple method. They receive an item and perform an action over it, also deciding if the item should continue through the pipeline or be dropped and no longer processed.\n",
    "\n",
    "Typical uses of item pipelines are:\n",
    "- cleansing HTML data\n",
    "- validating scraped data (checking that the items contain certain fields)\n",
    "- checking for duplicates (and dropping them)\n",
    "- storing the scraped item in a database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb346d7-0c0e-488a-b782-0d08bc7cd869",
   "metadata": {},
   "source": [
    "## Writing your own item pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6075c6-9de7-4c55-afe2-8a095b11aae2",
   "metadata": {},
   "source": [
    "Each item pipeline component is a Python class that must implement the following method:\n",
    "\n",
    "- `process_item(self, item, spider)`\n",
    "    - This method is called for every item pipeline component.\n",
    "    - `item` is an item object, see [Supporting All Item Types](#Supporting-All-Item-Types).\n",
    "    - `process_item()` must either: \n",
    "        - return an `item` object, \n",
    "        - return a `Deferred` or \n",
    "        - raise a `DropItem` exception.\n",
    "    - Dropped items are no longer processed by further pipeline components.\n",
    "    - **Parameters:**\n",
    "        - `item (item object)` – the scraped item\n",
    "        - `spider (Spider object)` – the spider which scraped the item\n",
    "\n",
    "Additionally, they may also implement the following methods:\n",
    "\n",
    "- `open_spider(self, spider)`\n",
    "    - This method is called when the spider is opened.\n",
    "    - **Parameters:**\n",
    "        - `spider (Spider object)` – the spider which was opened<br>\n",
    "</br>\n",
    "- `close_spider(self, spider)`\n",
    "    - This method is called when the spider is closed.\n",
    "    - **Parameters:**\n",
    "        - `spider (Spider object)` – the spider which was closed<br>\n",
    "</br>\n",
    "- `classmethodfrom_crawler(cls, crawler)`\n",
    "    - If present, this class method is called to create a pipeline instance from a `Crawler`. It must return a new instance of the pipeline. `Crawler` object provides access to all Scrapy core components like `settings` and `signals`; it is a way for pipeline to access them and hook its functionality into Scrapy.\n",
    "    - **Parameters**:\n",
    "        - `crawler (Crawler object)` – crawler that uses this pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a85b539-1e77-4af2-88ca-cd2c252d4831",
   "metadata": {},
   "source": [
    "## Item pipeline example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491c8fe6-bece-4f03-b429-ba9ca69bb748",
   "metadata": {},
   "source": [
    "### Price validation and dropping items with no prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642f0747-b3e2-467d-97c5-d2f9bb8062f8",
   "metadata": {},
   "source": [
    "Let’s take a look at the following hypothetical pipeline that adjusts the price attribute for those items that do not include VAT (\"Value Added Tax\", `price_excludes_vat` attribute), and drops those items which don’t contain a price:\n",
    "\n",
    "```python\n",
    "from itemadapter import ItemAdapter\n",
    "from scrapy.exceptions import DropItem\n",
    "\n",
    "\n",
    "class PricePipeline:\n",
    "    vat_factor = 1.15\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        adapter = ItemAdapter(item)\n",
    "        if adapter.get(\"price\"):\n",
    "            if adapter.get(\"price_excludes_vat\"):\n",
    "                adapter[\"price\"] = adapter[\"price\"] * self.vat_factor\n",
    "            return item\n",
    "        else:\n",
    "            raise DropItem(f\"Missing price in {item}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf365e2e-ee51-4a68-a170-9d16e1f5c38d",
   "metadata": {},
   "source": [
    "### Write items to a `JSON lines` file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5be129-a06a-4c2b-8563-daa34c884a17",
   "metadata": {},
   "source": [
    "The following pipeline stores all scraped items (from all spiders) into a single `items.jsonl` file, containing one item per line serialized in JSON format:\n",
    "\n",
    "```python\n",
    "import json\n",
    "\n",
    "from itemadapter import ItemAdapter\n",
    "\n",
    "\n",
    "class JsonWriterPipeline:\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open(\"items.jsonl\", \"w\")\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(ItemAdapter(item).asdict()) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item\n",
    "```\n",
    "\n",
    "> _Note:_ The purpose of `JsonWriterPipeline` is just to introduce how to write item pipelines. If you really want to store all scraped items into a JSON file you should use the [Feed exports](#2.8-Feed-exports)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5099ff57-c00d-4b38-a4dc-4ccfbb7fb2d3",
   "metadata": {},
   "source": [
    "### Write items to MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539f875e-5a02-4af8-a651-6fd368fc34e7",
   "metadata": {},
   "source": [
    "In this example we’ll write items to MongoDB using `pymongo`. MongoDB address and database name are specified in Scrapy `settings`; MongoDB collection is named after item class.\n",
    "\n",
    "The main point of this example is to show how to use `from_crawler()` method and how to clean up the resources properly.\n",
    "\n",
    "```python\n",
    "import pymongo\n",
    "from itemadapter import ItemAdapter\n",
    "\n",
    "\n",
    "class MongoPipeline:\n",
    "    collection_name = \"scrapy_items\"\n",
    "\n",
    "    def __init__(self, mongo_uri, mongo_db):\n",
    "        self.mongo_uri = mongo_uri\n",
    "        self.mongo_db = mongo_db\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        return cls(\n",
    "            mongo_uri=crawler.settings.get(\"MONGO_URI\"),\n",
    "            mongo_db=crawler.settings.get(\"MONGO_DATABASE\", \"items\"),\n",
    "        )\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        self.client = pymongo.MongoClient(self.mongo_uri)\n",
    "        self.db = self.client[self.mongo_db]\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.client.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        self.db[self.collection_name].insert_one(ItemAdapter(item).asdict())\n",
    "        return item\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5045c749-ec85-44ce-bdf0-f4e6820d9c33",
   "metadata": {},
   "source": [
    "### Take screenshot of `item`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a042cd6-689c-41f3-83a0-acb00146f722",
   "metadata": {},
   "source": [
    "This example demonstrates how to use coroutine syntax in the `process_item()` method.\n",
    "\n",
    "This item pipeline makes a request to a locally-running instance of Splash to render a screenshot of the item URL. After the request response is downloaded, the item pipeline saves the screenshot to a file and adds the filename to the item.\n",
    "\n",
    "```python\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from urllib.parse import quote\n",
    "\n",
    "import scrapy\n",
    "from itemadapter import ItemAdapter\n",
    "from scrapy.http.request import NO_CALLBACK\n",
    "from scrapy.utils.defer import maybe_deferred_to_future\n",
    "\n",
    "\n",
    "class ScreenshotPipeline:\n",
    "    \"\"\"Pipeline that uses Splash to render screenshot of\n",
    "    every Scrapy item.\"\"\"\n",
    "\n",
    "    SPLASH_URL = \"http://localhost:8050/render.png?url={}\"\n",
    "\n",
    "    async def process_item(self, item, spider):\n",
    "        adapter = ItemAdapter(item)\n",
    "        encoded_item_url = quote(adapter[\"url\"])\n",
    "        screenshot_url = self.SPLASH_URL.format(encoded_item_url)\n",
    "        request = scrapy.Request(screenshot_url, callback=NO_CALLBACK)\n",
    "        response = await maybe_deferred_to_future(\n",
    "            spider.crawler.engine.download(request)\n",
    "        )\n",
    "\n",
    "        if response.status != 200:\n",
    "            # Error happened, return item.\n",
    "            return item\n",
    "\n",
    "        # Save screenshot to file, filename will be hash of url.\n",
    "        url = adapter[\"url\"]\n",
    "        url_hash = hashlib.md5(url.encode(\"utf8\")).hexdigest()\n",
    "        filename = f\"{url_hash}.png\"\n",
    "        Path(filename).write_bytes(response.body)\n",
    "\n",
    "        # Store filename in item.\n",
    "        adapter[\"screenshot_filename\"] = filename\n",
    "        return item\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7fa087-2cda-46c6-85d7-5cca6d0e73a6",
   "metadata": {},
   "source": [
    "### Duplicates filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbb3994-8f84-448a-bfd9-3b122df0bc40",
   "metadata": {},
   "source": [
    "A filter that looks for duplicate items, and drops those items that were already processed. Let’s say that our items have a unique `id`, but our spider returns multiples items with the same `id`:\n",
    "\n",
    "```python\n",
    "from itemadapter import ItemAdapter\n",
    "from scrapy.exceptions import DropItem\n",
    "\n",
    "\n",
    "class DuplicatesPipeline:\n",
    "    def __init__(self):\n",
    "        self.ids_seen = set()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        adapter = ItemAdapter(item)\n",
    "        if adapter[\"id\"] in self.ids_seen:\n",
    "            raise DropItem(f\"Duplicate item found: {item!r}\")\n",
    "        else:\n",
    "            self.ids_seen.add(adapter[\"id\"])\n",
    "            return item\n",
    "```\n",
    "\n",
    "_ChatGPT:_  \n",
    "In Python, `{item!r}` is a syntax used within a string to represent the value of the variable item using its \"representation\" (`!r` is a conversion flag, `__repr__()`).\n",
    "\n",
    "The `!r` conversion flag is used to ensure that the value of item is represented in a way that is as close to its Python code representation as possible. This can be useful for debugging purposes, as it provides a string representation of the object that can be re-evaluated as valid Python code.\n",
    "\n",
    "In the provided code snippet, `{item!r}` is used within the `DropItem` exception to include the representation of the item that caused the duplication. This allows for a more informative error message that helps in identifying which item is a duplicate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866d725a-1120-4090-88f3-2098763dcd98",
   "metadata": {},
   "source": [
    "## Activating an Item Pipeline component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9730128-4af0-4fd2-a31e-9305ab0ae3fc",
   "metadata": {},
   "source": [
    "To activate an Item Pipeline component you must add its class to the `ITEM_PIPELINES` setting, like in the following example:\n",
    "\n",
    "```python\n",
    "ITEM_PIPELINES = {\n",
    "    \"myproject.pipelines.PricePipeline\": 300,\n",
    "    \"myproject.pipelines.JsonWriterPipeline\": 800,\n",
    "}\n",
    "```\n",
    "\n",
    "The integer values you assign to classes in this setting determine the order in which they run: items go through from lower valued to higher valued classes. It’s customary to define these numbers in the `0-1000` range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62b64c5-ed05-4b7e-8215-9e4f9c8a48f0",
   "metadata": {},
   "source": [
    "# 2.8 Feed exports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6816e2e6-8748-4156-b0d1-1e04a636e2da",
   "metadata": {},
   "source": [
    "# 2.9 Requests and Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8370633f-a7d9-4d49-9064-9c56e73b94ed",
   "metadata": {},
   "source": [
    "Scrapy uses `Request` and `Response` objects for crawling web sites.\n",
    "\n",
    "Typically, `Request` objects are generated in the spiders and pass across the system until they reach the `Downloader`, which executes the `request` and returns a `Response` object which travels back to the spider that issued the request.\n",
    "\n",
    "Both `Request` and `Response` classes have subclasses which add functionality not required in the base classes. These are described below in `Request` subclasses and `Response` subclasses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f953eba-2209-4b71-9082-6512e3898030",
   "metadata": {},
   "source": [
    "## `Request` objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfdd385-0320-4c54-9b83-f56315177fb7",
   "metadata": {},
   "source": [
    "### `scrapy.http.Request`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79165640-8b17-4ef2-904a-d5f34fc21d3a",
   "metadata": {},
   "source": [
    "```python\n",
    "class scrapy.http.Request(*args: Any, **kwargs: Any)\n",
    "```\n",
    "\n",
    "[source](https://docs.scrapy.org/en/latest/_modules/scrapy/http/request.html#Request)  \n",
    "Represents an HTTP request, which is usually generated in a Spider and executed by the Downloader, thus generating a Response.\n",
    "\n",
    "**Parameters**:\n",
    "\n",
    "- `url (str)` – the URL of this request. If the URL is invalid, a `ValueError` exception is raised.\n",
    "- `callback (collections.abc.Callable)` – the function that will be called with the response of this request (once it’s downloaded) as its first parameter. In addition to a function, the following values are supported: \n",
    "    - `None` (default), which indicates that the spider’s `parse()` method must be used.\n",
    "    - `NO_CALLBACK()`\n",
    "    For more information, see Passing additional data to callback functions.\n",
    "\n",
    "> _Note:_ If exceptions are raised during processing, errback is called instead.\n",
    "\n",
    "- `method (str)` – the HTTP method of this request. Defaults to '`GET`'.\n",
    "- `meta (dict)` – the initial values for the `Request.meta` attribute. If given, the `dict` passed in this parameter will be shallow copied.\n",
    "- `body (bytes or str)` – the request body. If a string is passed, then it’s encoded as bytes using the encoding passed (which defaults to `utf-8`). If body is not given, an empty bytes object is stored. Regardless of the type of this argument, the final value stored will be a bytes object (never a string or `None`).\n",
    "- `headers (dict)` – the headers of this request. The dict values can be strings (for single valued headers) or lists (for multi-valued headers). If `None` is passed as value, the HTTP header will not be sent at all. \n",
    "\n",
    "> **Caution**: Cookies set via the Cookie header are not considered by the CookiesMiddleware. If you need to set cookies for a request, use the Request.cookies parameter. This is a known current limitation that is being worked on.\n",
    "\n",
    "- `cookies (dict or list)` – the request cookies. These can be sent in two forms.\n",
    "    1. Using a `dict`:\n",
    "```python\n",
    "request_with_cookies = Request(\n",
    "    url=\"http://www.example.com\",\n",
    "    cookies={\"currency\": \"USD\", \"country\": \"UY\"},\n",
    ")\n",
    "```\n",
    "    2. Using a list of dicts:\n",
    "```python\n",
    "request_with_cookies = Request(\n",
    "    url=\"http://www.example.com\",\n",
    "    cookies=[\n",
    "        {\n",
    "            \"name\": \"currency\",\n",
    "            \"value\": \"USD\",\n",
    "            \"domain\": \"example.com\",\n",
    "            \"path\": \"/currency\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "```\n",
    "\n",
    "    The latter form allows for customizing the domain and path attributes of the cookie. This is only useful if the cookies are saved for later requests. When some site returns cookies (in a response) those are stored in the cookies for that domain and will be sent again in future requests. That’s the typical behaviour of any regular web browser. Note that setting the `dont_merge_cookies` key to `True` in `request.meta` causes custom cookies to be ignored. For more info see [CookiesMiddleware](#CookiesMiddleware).\n",
    "\n",
    "> **Caution**: Cookies set via the `Cookie` header are not considered by the `CookiesMiddleware`. If you need to set cookies for a request, use the `Request.cookies` parameter. This is a known current limitation that is being worked on.\n",
    "\n",
    "_New in version 2.6.0: Cookie values that are `bool`, `float` or `int` are casted to `str`._\n",
    "\n",
    "- `encoding (str)` – the encoding of this request (defaults to 'utf-8'). This encoding will be used to percent-encode the URL and to convert the body to bytes (if given as a string).\n",
    "- `priority (int)` – the priority of this request (defaults to `0`). The priority is used by the scheduler to define the order used to process requests. Requests with a higher priority value will execute earlier. Negative values are allowed in order to indicate relatively low-priority.\n",
    "- `dont_filter (bool)` – indicates that this request should not be filtered by the scheduler. This is used when you want to perform an identical request multiple times, to ignore the duplicates filter. Use it with care, or you will get into crawling loops. Default to `False`.\n",
    "- `errback (collections.abc.Callable)` – a function that will be called if any exception was raised while processing the request. This includes pages that failed with `404` HTTP errors and such. It receives a `Failure` as first parameter. For more information, see Using errbacks to catch exceptions in request processing below.\n",
    "\n",
    "_Changed in version 2.0: The `callback` parameter is no longer required when the `errback` parameter is specified._\n",
    "\n",
    "- `flags (list)` – Flags sent to the request, can be used for logging or similar purposes.\n",
    "- `cb_kwargs (dict)` – A `dict` with arbitrary data that will be passed as keyword arguments to the Request’s callback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa27ebc-7025-404e-ac08-d8b609950b3a",
   "metadata": {},
   "source": [
    "#### `url`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce4c107-f690-45ba-9a0c-995960c25d45",
   "metadata": {},
   "source": [
    "A string containing the URL of this request. Keep in mind that this attribute contains the escaped URL, so it can differ from the URL passed in the `__init__` method.\n",
    "\n",
    "This attribute is read-only. To change the URL of a `Request` use `replace()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f929f1-c0bf-4d45-875e-37efd56fa76d",
   "metadata": {},
   "source": [
    "#### `method`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68d7bd5-dc17-4211-9bff-71bc6c8d16fa",
   "metadata": {},
   "source": [
    "A string representing the HTTP method in the request. This is guaranteed to be uppercase. Example: \"GET\", \"POST\", \"PUT\", etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8360374d-d5cd-4129-9cc0-ad2fa8f208d0",
   "metadata": {},
   "source": [
    "#### `headers`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6c14ef-25d6-4074-bfad-0183cd44e144",
   "metadata": {},
   "source": [
    "A dictionary-like object which contains the request headers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6baba7f-abdd-4dea-92d9-ed687f16a55a",
   "metadata": {},
   "source": [
    "#### `body`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe569e1-f88f-4466-83f4-3af9f9c9acee",
   "metadata": {},
   "source": [
    "The request body as bytes.\n",
    "\n",
    "This attribute is read-only. To change the body of a `Request` use `replace()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5798bc06-2090-4c6a-975c-3c160af0fac2",
   "metadata": {},
   "source": [
    "#### `meta= {}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34513f3-fa9c-4e23-baa6-0f846e0e109c",
   "metadata": {},
   "source": [
    "A dictionary of arbitrary metadata for the request.\n",
    "\n",
    "You may extend request metadata as you see fit.\n",
    "\n",
    "Request metadata can also be accessed through the `meta` attribute of a `response`.\n",
    "\n",
    "To pass data from one spider callback to another, consider using `cb_kwargs` instead. However, request metadata may be the right choice in certain scenarios, such as to maintain some debugging data across all follow-up requests (e.g. the source URL).\n",
    "\n",
    "A common use of request metadata is to define request-specific parameters for Scrapy components (extensions, middlewares, etc.). For example, if you set `dont_retry` to `True`, `RetryMiddleware` will never retry that request, even if it fails. See `Request.meta` special keys.\n",
    "\n",
    "You may also use request metadata in your custom Scrapy components, for example, to keep request state information relevant to your component. For example, `RetryMiddleware` uses the `retry_times` metadata key to keep track of how many times a request has been retried so far.\n",
    "\n",
    "Copying all the metadata of a previous request into a new, follow-up request in a spider callback is a bad practice, because request metadata may include metadata set by Scrapy components that is not meant to be copied into other requests. For example, copying the `retry_times` metadata key into follow-up requests can lower the amount of retries allowed for those follow-up requests.\n",
    "\n",
    "You should only copy all request metadata from one request to another if the new request is meant to replace the old request, as is often the case when returning a request from a downloader middleware method.\n",
    "\n",
    "Also mind that the `copy()` and `replace()` request methods [shallow-copy](https://docs.python.org/3/library/copy.html) request metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fad960-15e6-4a57-8b2e-23991841c8c5",
   "metadata": {},
   "source": [
    "#### `cb_kwargs`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12d45c0-68ec-4dc9-8559-7dbf6a02083a",
   "metadata": {},
   "source": [
    "A dictionary that contains arbitrary metadata for this request. Its contents will be passed to the `Request`’s callback as keyword arguments. It is empty for new Requests, which means by default callbacks only get a `Response` object as argument.\n",
    "\n",
    "This `dict` is shallow copied when the request is cloned using the `copy()` or `replace()` methods, and can also be accessed, in your spider, from the `response.cb_kwargs` attribute.\n",
    "\n",
    "In case of a failure to process the request, this dict can be accessed as `failure.request.cb_kwargs` in the request’s `errback`. For more information, see **Accessing additional data in errback functions**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15ed4f9-987c-455b-87c9-c84afced55b1",
   "metadata": {},
   "source": [
    "#### `attributes`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2a0b88-98cc-416a-b162-413c9cfadc29",
   "metadata": {},
   "source": [
    "```python\n",
    "attributes: Tuple[str, ...]= ('url', 'callback', 'method', 'headers', 'body', 'cookies', 'meta', 'encoding', 'priority', 'dont_filter', 'errback', 'flags', 'cb_kwargs')\n",
    "```\n",
    "\n",
    "A tuple of `str` objects containing the name of all public attributes of the class that are also keyword parameters of the `__init__` method.\n",
    "\n",
    "Currently used by `Request.replace()`, `Request.to_dict()` and `request_from_dict()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67f3f48-dc21-48b7-928d-f24186eca7e9",
   "metadata": {},
   "source": [
    "#### `copy()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330f9881-f5d8-4e7b-8a3b-0a0b6368db73",
   "metadata": {},
   "source": [
    "[source](https://docs.scrapy.org/en/latest/_modules/scrapy/http/request.html#Request.copy)\n",
    "\n",
    "Return a new `Request` which is a copy of this `Request`. See also: **Passing additional data to callback functions**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f127ab24-42a5-468c-b357-32c9b95d19d0",
   "metadata": {},
   "source": [
    "#### `replace()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c05ae4a-4f3f-4c5a-adc6-99b9cdb3d965",
   "metadata": {},
   "source": [
    "```python\n",
    "replace([url, method, headers, body, cookies, meta, flags, encoding, priority, dont_filter, callback, errback, cb_kwargs])\n",
    "```\n",
    "[source](https://docs.scrapy.org/en/latest/_modules/scrapy/http/request.html#Request.replace)\n",
    "\n",
    "Return a `Request` object with the same members, except for those members given new values by whichever keyword arguments are specified. The `Request.cb_kwargs` and `Request.meta` attributes are shallow copied by default (unless new values are given as arguments). See also [Passing additional data to callback functions](#Passing-additional-data-to-callback-functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e82bf14-7d39-4cda-b940-ebef62d14c5b",
   "metadata": {},
   "source": [
    "#### `from_curl()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3487f9-ce11-4aef-b36a-8ead8ae7892f",
   "metadata": {},
   "source": [
    "```python\n",
    "classmethod from_curl(curl_command: str, ignore_unknown_options: bool = True, **kwargs)→ RequestTypeVar\n",
    "```\n",
    "[source](https://docs.scrapy.org/en/latest/_modules/scrapy/http/request.html#Request.from_curl)  \n",
    "Create a `Request` object from a string containing a cURL command. It populates the HTTP method, the URL, the headers, the cookies and the body. It accepts the same arguments as the `Request` class, taking preference and overriding the values of the same arguments contained in the cURL command.\n",
    "\n",
    "Unrecognized options are ignored by default. To raise an error when finding unknown options call this method by passing `ignore_unknown_options=False`.\n",
    "\n",
    "> **Caution**: Using `from_curl()` from `Request` subclasses, such as `JSONRequest`, or `XmlRpcRequest`, as well as having downloader middlewares and spider middlewares enabled, such as `DefaultHeadersMiddleware`, `UserAgentMiddleware`, or `HttpCompressionMiddleware`, may modify the `Request` object.\n",
    "\n",
    "To translate a cURL command into a Scrapy request, you may use `curl2scrapy`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b185989-9394-4942-a958-5908a9391a5c",
   "metadata": {},
   "source": [
    "#### `to_dict()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bfec36-cacd-42fb-a564-fb96ff368100",
   "metadata": {},
   "source": [
    "```python\n",
    "to_dict(*, spider: Optional[Spider] = None)→ dict\n",
    "```\n",
    "[source](https://docs.scrapy.org/en/latest/_modules/scrapy/http/request.html#Request.to_dict)  \n",
    "Return a dictionary containing the `Request`’s data.\n",
    "\n",
    "Use `request_from_dict()` to convert back into a `Request` object.\n",
    "\n",
    "If a spider is given, this method will try to find out the name of the spider methods used as callback and errback and include them in the output dict, raising an exception if they cannot be found."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58deb966-9995-42d3-9bae-a1debda6e8e2",
   "metadata": {},
   "source": [
    "### Other functions related to requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b64cad-a756-4365-944a-e21c7f76e041",
   "metadata": {},
   "source": [
    "```python\n",
    "scrapy.http.request.NO_CALLBACK(*args, **kwargs)\n",
    "```\n",
    "[source](https://docs.scrapy.org/en/latest/_modules/scrapy/http/request.html#NO_CALLBACK)  \n",
    "When assigned to the callback parameter of `Request`, it indicates that the request is not meant to have a spider callback at all.\n",
    "\n",
    "For example:\n",
    "```python\n",
    "Request(\"https://example.com\", callback=NO_CALLBACK)\n",
    "```\n",
    "\n",
    "This value should be used by components that create and handle their own requests, e.g. through `scrapy.core.engine.ExecutionEngine.download()`, so that downloader middlewares handling such requests can treat them differently from requests intended for the `parse()` callback.\n",
    "\n",
    "```python\n",
    "scrapy.utils.request.request_from_dict(d: dict, *, spider: Optional[Spider] = None)→ Request\n",
    "```\n",
    "[source](https://docs.scrapy.org/en/latest/_modules/scrapy/utils/request.html#request_from_dict)  \n",
    "Create a `Request` object from a `dict`.\n",
    "\n",
    "If a spider is given, it will try to resolve the callbacks looking at the spider for methods with the same name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecca9dc-95b5-4d64-b4aa-2630777d2e62",
   "metadata": {},
   "source": [
    "### Passing additional data to callback functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e143b8da-ea80-4905-a4bc-ec204332eb90",
   "metadata": {},
   "source": [
    "The callback of a request is a function that will be called when the response of that request is downloaded. The callback function will be called with the downloaded `Response` object as its first argument.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "def parse_page1(self, response):\n",
    "    return scrapy.Request(\n",
    "        \"http://www.example.com/some_page.html\", callback=self.parse_page2\n",
    "    )\n",
    "\n",
    "\n",
    "def parse_page2(self, response):\n",
    "    # this would log http://www.example.com/some_page.html\n",
    "    self.logger.info(\"Visited %s\", response.url)\n",
    "```\n",
    "\n",
    "In some cases you may be interested in passing arguments to those callback functions so you can receive the arguments later, in the second callback. The following example shows how to achieve this by using the `Request.cb_kwargs` attribute:\n",
    "\n",
    "```python\n",
    "def parse(self, response):\n",
    "    request = scrapy.Request(\n",
    "        \"http://www.example.com/index.html\",\n",
    "        callback=self.parse_page2,\n",
    "        cb_kwargs=dict(main_url=response.url),\n",
    "    )\n",
    "    request.cb_kwargs[\"foo\"] = \"bar\"  # add more arguments for the callback\n",
    "    yield request\n",
    "\n",
    "\n",
    "def parse_page2(self, response, main_url, foo):\n",
    "    yield dict(\n",
    "        main_url=main_url,\n",
    "        other_url=response.url,\n",
    "        foo=foo,\n",
    "    )\n",
    "```\n",
    "\n",
    "> **Caution**: `Request.cb_kwargs` was introduced in version 1.7. Prior to that, using `Request.meta` was recommended for passing information around callbacks. After 1.7, `Request.cb_kwargs` became the preferred way for handling user information, leaving `Request.meta` for communication with components like middlewares and extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b99d92-c674-4951-a6de-642a109f6e1c",
   "metadata": {},
   "source": [
    "### Using `errbacks` to catch exceptions in `request` processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d984cb15-08eb-4f6f-ba9b-d7fbca78df60",
   "metadata": {},
   "source": [
    "The `errback` of a `request` is a function that will be called when an exception is raised while processing it.\n",
    "\n",
    "It receives a `Failure` as first parameter and can be used to track connection establishment timeouts, DNS errors, etc.\n",
    "\n",
    "Here’s an example spider logging all errors and catching some specific errors if needed:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "from scrapy.spidermiddlewares.httperror import HttpError\n",
    "from twisted.internet.error import DNSLookupError\n",
    "from twisted.internet.error import TimeoutError, TCPTimedOutError\n",
    "\n",
    "\n",
    "class ErrbackSpider(scrapy.Spider):\n",
    "    name = \"errback_example\"\n",
    "    start_urls = [\n",
    "        \"http://www.httpbin.org/\",  # HTTP 200 expected\n",
    "        \"http://www.httpbin.org/status/404\",  # Not found error\n",
    "        \"http://www.httpbin.org/status/500\",  # server issue\n",
    "        \"http://www.httpbin.org:12345/\",  # non-responding host, timeout expected\n",
    "        \"https://example.invalid/\",  # DNS error expected\n",
    "    ]\n",
    "\n",
    "    def start_requests(self):\n",
    "        for u in self.start_urls:\n",
    "            yield scrapy.Request(\n",
    "                u,\n",
    "                callback=self.parse_httpbin,\n",
    "                errback=self.errback_httpbin,\n",
    "                dont_filter=True,\n",
    "            )\n",
    "\n",
    "    def parse_httpbin(self, response):\n",
    "        self.logger.info(\"Got successful response from {}\".format(response.url))\n",
    "        # do something useful here...\n",
    "\n",
    "    def errback_httpbin(self, failure):\n",
    "        # log all failures\n",
    "        self.logger.error(repr(failure))\n",
    "\n",
    "        # in case you want to do something special for some errors,\n",
    "        # you may need the failure's type:\n",
    "\n",
    "        if failure.check(HttpError):\n",
    "            # these exceptions come from HttpError spider middleware\n",
    "            # you can get the non-200 response\n",
    "            response = failure.value.response\n",
    "            self.logger.error(\"HttpError on %s\", response.url)\n",
    "\n",
    "        elif failure.check(DNSLookupError):\n",
    "            # this is the original request\n",
    "            request = failure.request\n",
    "            self.logger.error(\"DNSLookupError on %s\", request.url)\n",
    "\n",
    "        elif failure.check(TimeoutError, TCPTimedOutError):\n",
    "            request = failure.request\n",
    "            self.logger.error(\"TimeoutError on %s\", request.url)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c35629-4580-4a80-8936-fd52a64ec730",
   "metadata": {},
   "source": [
    "## `<Not finished yet>`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6ec565-2ee4-4ecf-a000-71cfa479ce44",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe054ed4-5952-449d-8fd0-95867c807ed6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c61c4351-00bc-4b01-b051-13f6e70c82f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d3857c6-2eaf-4b07-88c7-8f2dfd16bff2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "780e9c77-b5f9-4ca6-bade-53f74016c070",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "777a4ea6-beae-43e2-aab3-e2bf65381187",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22a30adf-5bcd-4a5a-af07-bc867f3f86cf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b17d1a9b-bf83-416d-ae75-2f93d6e02522",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9028247-5b1f-4e78-85bf-3461a5717878",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "562646bc-13a8-43f9-aa40-4c351cede643",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bac180c-9a62-4e4a-b401-392e605d19d9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47ac008c-937f-4c6e-9c84-47db2c817dd4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77a2a678-dcf8-4f4b-836d-f1659e121786",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a49ec48-d2c0-4035-b009-72650cf0050c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30c48a15-e2b3-4b49-abc9-6d80b585344e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "054b6290-b371-415a-8dcc-2e7d8c83b1ed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b15ecf6-12c8-424b-bcf2-dfd9f38be132",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6eaad968-ecf8-41c8-89e1-183bd63ff6da",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84d4bbf0-27b0-4a84-bdfd-6343c5f45b9f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83f42444-d95f-4615-bd64-6f3f56704092",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9c555a1-6724-4092-90f2-4561f2c03589",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc0e1325-468e-4ab2-b4fa-30a06025265d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cc1d185-f4b1-479d-b568-a2f3b3f68b4d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba4095b2-5497-42cb-a30d-2d0401c3ce43",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd4a33d0-67d1-4e3e-9abf-7745c2ca9499",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73d93bde-c734-49e1-a72b-3ae0cb85b5a5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a048823-4d9f-4764-b72c-22d6d7743280",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94130318-1e89-4935-929f-cf0a4b25f792",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba878e09-0f71-498e-87c8-7fd0c42642cb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf6ed3f0-4554-4db2-9979-797e834a99f5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "082a4072-15ce-47d4-9476-082106cdb4f5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40d37508-8f5a-4e2e-9811-37718e21b63b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cb3c4f1-54e5-4235-9fdb-9246e568fe2d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7580e4e8-f760-42c5-8e48-e8e20bfea4ef",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "077d37a9-b3b6-4f14-82a2-43521469773e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dfbe16b-1e01-421b-9ef2-28edb4c01dfa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a395d9c-3a6f-4555-9be1-2723fad2a3a3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28b50365-a583-4ca7-9a13-d6a25d99980a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35d3dd63-41bd-43e7-8b29-aec720fafec9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c27d5562-e156-434e-99f3-702561e38b6c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48e7e278-1f8d-4400-9ac3-d221f3d6b476",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f759a452-89f1-41dd-9a3c-c0f108f3d3ce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbb44a9d-3ee3-4e3f-98a0-9e060d35e865",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c6f4b0d-8f7a-4f7e-862d-35e6ed5ce229",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e0fc03e-1ebb-4f20-9852-e2dc1411f8a1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddb1b8ab-8671-4a54-a991-47292f2eb23e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c99ba22-00f1-40e2-a43b-2b2e7d590550",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bac4547a-9fce-4ebc-9da1-bbf1524e6681",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e5316c5-e24e-4cec-9bf3-6ac2abfab764",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c80c7ce5-3b7f-4d7c-b958-cef8c50f6740",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6318e904-4e37-4b11-a023-9ef5a26fdf59",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d916214-8e43-482b-9a62-2d500e8c601f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cb53f11-e473-4591-a4ee-dcf19583989a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11fb0a8e-faff-41fe-8ba8-ee8b3bf11851",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2950747f-315b-46bf-8f1e-f57d593d66c6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30e975f2-a77d-4c16-8328-20aab5673e10",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e6b296c-5b16-4bb4-8ec0-30671cc79e28",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccc0fb83-27f6-4251-8b3d-647061c5541a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d709dbb-44cd-4111-b141-2753836334d2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "460ddeb5-6d30-4d9b-b165-46a97c571f41",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbcbe43b-79ea-4a1b-91e3-3133f3d57f19",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84d0f229-ac18-4e99-95e0-235b8f7909ac",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e89f3c1-5852-453b-855c-e3dd8087b377",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36454e48-9a55-4204-978f-305382388868",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e73c475-b882-4742-b550-7b4ba162b00e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89e5bd2f-117a-4bc2-9ec0-ea86783b1a8a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f2c41f5-39a4-40a9-af7c-79fd9cacc4e3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5eaced5a-9e7b-4c18-85fd-a502b02a5f84",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3d9f129-8930-41e2-bc60-973e7fa7ea70",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db52f6c6-7bdf-4683-b424-13a2c7d5bff0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56db8467-6164-4db6-847e-63883e5b1e49",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff3fbee6-a3b2-444b-b0e7-38775df421a0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e67fe24-2342-4f58-8c23-866c51051e4a",
   "metadata": {},
   "source": [
    "# 2.10 Link Extractors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66efc3e-3ece-409d-942b-ab2cb72f7dc0",
   "metadata": {},
   "source": [
    "A **link extractor** is an object that extracts links from `response`s.\n",
    "\n",
    "The `__init__` method of `LxmlLinkExtractor` takes `settings` that determine which links may be extracted. `LxmlLinkExtractor.extract_links` returns a _list_ of matching **Link objects** from a `Response` object.\n",
    "\n",
    "Link extractors are used in `CrawlSpider` spiders through a set of `Rule` objects.\n",
    "\n",
    "You can also use link extractors in regular spiders. For example, you can instantiate `LinkExtractor` into a class variable in your spider, and use it from your spider callbacks:\n",
    "\n",
    "```python\n",
    "def parse(self, response):\n",
    "    for link in self.link_extractor.extract_links(response):\n",
    "        yield Request(link.url, callback=self.parse)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6537b5aa-2c52-4777-93a2-f54183f0b2f2",
   "metadata": {},
   "source": [
    "## Link extractor reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48271af2-6379-495f-a544-df253c6fafa1",
   "metadata": {},
   "source": [
    "The link extractor class is `scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor`. For convenience it can also be imported as `scrapy.linkextractors.LinkExtractor`:\n",
    "\n",
    "```python\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbfe7d8-c1c4-47af-9025-9e0a069a6729",
   "metadata": {},
   "source": [
    "### `LxmlLinkExtractor`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b319c1cd-9a35-4ce7-9c67-55a5aab2f8ba",
   "metadata": {},
   "source": [
    "```python\n",
    "class scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor(allow=(), deny=(), allow_domains=(), deny_domains=(), deny_extensions=None, restrict_xpaths=(), restrict_css=(), tags=('a', 'area'), attrs=('href',), canonicalize=False, unique=True, process_value=None, strip=True)\n",
    "``` \n",
    "\n",
    "[source](https://docs.scrapy.org/en/latest/_modules/scrapy/linkextractors/lxmlhtml.html#LxmlLinkExtractor)\n",
    "\n",
    "`LxmlLinkExtractor` is the recommended link extractor with handy filtering options. It is implemented using `lxml`’s robust `HTMLParser`.\n",
    "\n",
    "**Parameters**\n",
    "- `allow (str or list)` – a single regular expression (or list of regular expressions) that the (absolute) urls must match in order to be extracted. If not given (or empty), it will match all links.\n",
    "- `deny (str or list)` – a single regular expression (or list of regular expressions) that the (absolute) urls must match in order to be excluded (i.e. not extracted). It has precedence over the `allow` parameter. If not given (or empty) it won’t exclude any links.\n",
    "- `allow_domains (str or list)` – a single value or a list of string containing domains which will be considered for extracting the links\n",
    "- `deny_domains (str or list)` – a single value or a list of strings containing domains which won’t be considered for extracting the links\n",
    "- `deny_extensions (list)` – a single value or list of strings containing extensions that should be ignored when extracting links. If not given, it will default to `scrapy.linkextractors.IGNORED_EXTENSIONS`.\n",
    "\n",
    "_Changed in version 2.0:_ `IGNORED_EXTENSIONS` now includes 7z, 7zip, apk, bz2, cdr, dmg, ico, iso, tar, tar.gz, webm, and xz.\n",
    "\n",
    "- `restrict_xpaths (str or list)` – is an XPath (or list of XPath’s) which defines regions inside the response where links should be extracted from. If given, only the text selected by those XPath will be scanned for links. See examples below.\n",
    "- `restrict_css (str or list)` – a CSS selector (or list of selectors) which defines regions inside the response where links should be extracted from. Has the same behaviour as `restrict_xpaths`.\n",
    "- `restrict_text (str or list)` – a single regular expression (or list of regular expressions) that the link’s `text` must match in order to be extracted. If not given (or empty), it will match all links. If a list of regular expressions is given, the link will be extracted if it matches at least one.\n",
    "- `tags (str or list)` – a tag or a list of tags to consider when extracting links. Defaults to ('`a`', '`area`').\n",
    "- `attrs (list)` – an attribute or list of attributes which should be considered when looking for links to extract (only for those tags specified in the `tags` parameter). Defaults to `('href',)`\n",
    "- `canonicalize (bool)` – canonicalize each extracted url (using `w3lib.url.canonicalize_url)`. Defaults to `False`. Note that `canonicalize_url` is meant for duplicate checking; it can change the URL visible at server side, so the `response` can be different for requests with canonicalized and raw URLs. If you’re using `LinkExtractor` to follow links it is more robust to keep the default `canonicalize=False`.\n",
    "- `unique (bool)` – whether duplicate filtering should be applied to extracted links.\n",
    "- `process_value (collections.abc.Callable)` – a function which receives each value extracted from the `tag` and `attributes` scanned and can modify the value and return a new one, or return `None` to ignore the link altogether. If not given, `process_value` defaults to `lambda x: x`.\n",
    "\n",
    "For example, to extract links from this code:\n",
    "\n",
    "```html\n",
    "<a href=\"javascript:goToPage('../other/page.html'); return false\">Link text</a>\n",
    "```\n",
    "\n",
    "You can use the following function in `process_value`:\n",
    "\n",
    "```python\n",
    "def process_value(value):\n",
    "    m = re.search(r\"javascript:goToPage\\('(.*?)'\", value)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "```\n",
    "\n",
    "- `strip (bool)` – whether to strip whitespaces from extracted attributes. According to `HTML5` standard, leading and trailing whitespaces must be stripped from `href` attributes of `<a>`, `<area>` and many other elements, `src` attribute of `<img>`, `<iframe>` elements, etc., so `LinkExtractor` strips space chars by default. Set `strip=False` to turn it off (e.g. if you’re extracting urls from elements or attributes which allow leading/trailing whitespaces).\n",
    "\n",
    "```python\n",
    "extract_links(response)\n",
    "```\n",
    "\n",
    "[source](https://docs.scrapy.org/en/latest/_modules/scrapy/linkextractors/lxmlhtml.html#LxmlLinkExtractor.extract_links)\n",
    "- Returns a list of `Link` objects from the specified `response`.\n",
    "- Only links that match the `settings` passed to the `__init__` method of the link extractor are returned.\n",
    "- Duplicate links are omitted if the `unique` attribute is set to `True`, otherwise they are returned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25ad35d-8677-4d2d-be95-8c2491baaf39",
   "metadata": {},
   "source": [
    "### Link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18eff78-5610-4d2d-919c-877e1671dcaf",
   "metadata": {},
   "source": [
    "```python\n",
    "class scrapy.link.Link(url: str, text: str = '', fragment: str = '', nofollow: bool = False)\n",
    "```\n",
    "\n",
    "[source](https://docs.scrapy.org/en/latest/_modules/scrapy/link.html#Link)\n",
    "\n",
    "`Link` objects represent an extracted link by the `LinkExtractor`.\n",
    "\n",
    "Using the `anchor` tag sample below to illustrate the parameters:\n",
    "\n",
    "```html\n",
    "<a href=\"https://example.com/nofollow.html#foo\" rel=\"nofollow\">Dont follow this one</a>\n",
    "```\n",
    "\n",
    "**Parameters**\n",
    "- `url` – the absolute url being linked to in the `anchor` tag. From the sample, this is `https://example.com/nofollow.html`.\n",
    "- `text` – the text in the `anchor` tag. From the sample, this is Dont follow this one.\n",
    "- `fragment` – the part of the url after the `hash` symbol. From the sample, this is `foo`.\n",
    "- `nofollow` – an indication of the presence or absence of a nofollow value in the `rel` attribute of the `anchor` tag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe246e0-224f-4421-89b5-4cf89ff45756",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2.11 Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909bceb4-a7e3-46c6-9e44-509d67bb8f96",
   "metadata": {},
   "source": [
    "## Designating the settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4061dd4c-c4f4-4a50-b94b-847f86fe479d",
   "metadata": {},
   "source": [
    "## Built-in settings reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cbb154-6ac6-4f26-9f54-766c8aee6b91",
   "metadata": {},
   "source": [
    "# <b>3. Built-in Services</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07071642-bcdf-4074-9ac5-683c2c174547",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3.1 Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9547c2-32b1-4ef1-a376-58939698db56",
   "metadata": {},
   "source": [
    "## Logging from Spiders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5acafe-f8ea-475a-be3b-8cddb02400a6",
   "metadata": {},
   "source": [
    "# <b>4. Solving specific problems</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d81b28-f488-4e20-8e1d-762bbe7b6c32",
   "metadata": {},
   "source": [
    "# 4.4 Common practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acda595-139e-44f4-a3bc-7fa7c0e39bdf",
   "metadata": {},
   "source": [
    "## Run Scrapy from a script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15ae011-eaec-46c4-bee6-ef1e92905dff",
   "metadata": {},
   "source": [
    "# 4.7 Selecting dynamically-loaded content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78b4d3b-254e-4e75-bc23-9eeee1705139",
   "metadata": {},
   "source": [
    "## `scrapy-playwright` -VR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ee4249-dbe5-46c7-8b8a-7426325cbc37",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04d60ec-a5ee-4ecd-8228-a7ba3edaf414",
   "metadata": {},
   "source": [
    "```sh\n",
    "(venv) pip install scrapy-playwright\n",
    "(venv) playwright install\n",
    "```\n",
    "```python\n",
    "# settings.py\n",
    "\n",
    "TWISTED_REACTOR = \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n",
    "\n",
    "DOWNLOAD_HANDLERS = {\n",
    "        \"http\": \"scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler\",\n",
    "        \"https\": \"scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler\",\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dcd1e1-2c4e-4e1f-a665-ccfe8d0f1712",
   "metadata": {},
   "source": [
    "### `PageMethod`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e190870-b22d-441d-aede-9149563810b4",
   "metadata": {},
   "source": [
    "In `scrapy-playwright`, the PageMethod class is a wrapper around a Playwright Page object. It provides a set of methods that allow you to interact with the web page, such as navigating to URLs, clicking on elements, filling out forms, extracting content, and more.\n",
    "\n",
    "The Page class acts as a bridge between Scrapy and Playwright, enabling you to use Playwright's capabilities within your Scrapy spider. With the Page class, you can perform various actions on web pages, simulate user interactions, and extract data from the rendered page.\n",
    "\n",
    "Here are some common methods provided by the Page class in scrapy-playwright:\n",
    "\n",
    "goto(url): Navigates to the specified URL.\n",
    "content(): Retrieves the raw HTML content of the page.\n",
    "title(): Retrieves the title of the page.\n",
    "click(selector): Clicks on the element matching the specified CSS selector.\n",
    "wait_for_navigation(): Waits for the page to finish navigating.\n",
    "text_content(selector): Retrieves the text content of the element matching the specified CSS selector.\n",
    "attribute(selector, name): Retrieves the value of the specified attribute of the element matching the CSS selector.\n",
    "set_content(html, url): Sets the HTML content of the page, useful for working with responses from Scrapy.\n",
    "These methods provide you with the ability to interact with web pages in various ways, allowing you to scrape data efficiently and accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaf1cb2-dd97-4e2f-b598-8806b347bae4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 4.8 Debugging memory leaks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d0cbae-ad52-43ad-a7ec-4209b0182089",
   "metadata": {},
   "source": [
    "## Debugging memory leaks with `trackref`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08087fea-2315-437e-a6e1-68153d8c8982",
   "metadata": {},
   "source": [
    "# 4.12 Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afaa1fd-a706-4e34-85bf-fe6d65ff8dbc",
   "metadata": {},
   "source": [
    "# 4.13 Jobs: pausing and resuming crawls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52603206-5112-45aa-aac0-07802c9cbba2",
   "metadata": {},
   "source": [
    "Sometimes, for big sites, it’s desirable to pause crawls and be able to resume them later.\n",
    "\n",
    "Scrapy supports this functionality out of the box by providing the following facilities:\n",
    "- a scheduler that persists scheduled requests on disk\n",
    "- a duplicates filter that persists visited requests on disk\n",
    "- an extension that keeps some spider state (key/value pairs) persistent between batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53817aee-a4a6-4510-bb50-e705e38bc087",
   "metadata": {},
   "source": [
    "## Job directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a981626-e8c6-432f-b3c3-6972eddbd4ce",
   "metadata": {},
   "source": [
    "To enable persistence support you just need to define a job directory through the `JOBDIR` setting. This directory will be for storing all required data to keep the state of a single job (i.e. a spider run). It’s important to note that \n",
    "\n",
    "> this directory must not be shared by different spiders, or even different jobs/runs of the same spider, as it’s meant to be used for storing the state of a _**single**_ job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a9ad2a-194b-46ad-813d-59c083d17ae6",
   "metadata": {},
   "source": [
    "## How to use it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811cd4f7-0e67-4e99-9c07-57c47a177ae0",
   "metadata": {},
   "source": [
    "To start a spider with persistence support enabled, run it like this:\n",
    "\n",
    "```sh\n",
    "scrapy crawl somespider -s JOBDIR=crawls/somespider-1\n",
    "```\n",
    "Then, you can stop the spider safely at any time (by pressing `Ctrl-C` or sending a signal), and resume it later by issuing the same command:\n",
    "\n",
    "```sh\n",
    "scrapy crawl somespider -s JOBDIR=crawls/somespider-1\n",
    "```\n",
    "\n",
    "VR:  \n",
    "Or just set the `JOBDIR = \"./myjob\"` in `settings.py` to automate the process. But remember:\n",
    "\n",
    "\n",
    "_ChatGPT:_  \n",
    "When you define `JOBDIR` in the `settings.py` file, Scrapy automatically reads this setting from the file when the crawler is started. Therefore, the setting is already applied globally to all spiders within your Scrapy project.\n",
    "\n",
    "In the context of Scrapy, a \"job directory\" is a directory used to store all the required data to maintain the state of a single job, which refers to a single run of a spider. The `JOBDIR` setting allows you to specify the location of this directory.\n",
    "\n",
    "The statement \"_or even different jobs/runs of the same spider_\" refers to the fact that each run of a spider can have its own job directory, and these directories should not be shared among different runs of the same spider.\n",
    "\n",
    "Here's an explanation of the situation:\n",
    "\n",
    "- **Different jobs/runs of the same spider:**\n",
    "    - Each time you run a spider, it constitutes a separate job or run.\n",
    "    - For example, if you run a spider today and then run the same spider tomorrow with different configurations or parameters, each of these runs is considered a separate job.\n",
    "    - Each job can have its own job directory to store its state, such as crawled data, logs, and other metadata specific to that run.\n",
    "- **Why job directories should not be shared:**\n",
    "    - Job directories are meant to maintain the state of a single job, and sharing them between different jobs/runs can lead to unintended consequences.\n",
    "    - If two jobs share the same job directory, they may overwrite each other's state, leading to data corruption, incorrect results, or other issues.\n",
    "\n",
    "For example, if you run a spider today and then run the same spider again tomorrow using the same job directory, the second run may overwrite or interfere with the state of the first run, causing unexpected behavior.\n",
    "\n",
    "To avoid these issues, it's essential to ensure that each job/run of a spider has its own separate job directory. This ensures isolation between runs and maintains the integrity of the data and state for each individual job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8d9545-3f29-45c6-8a48-68aced2ada84",
   "metadata": {},
   "source": [
    "## Keeping persistent state between batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d710287-2d62-4931-856a-636a948ef426",
   "metadata": {},
   "source": [
    "Sometimes you’ll want to keep some persistent spider state between pause/resume batches. You can use the `spider.state` attribute for that, which should be a `dict`. There’s a built-in extension that takes care of serializing, storing and loading that attribute from the job directory, when the spider starts and stops.\n",
    "\n",
    "Here’s an example of a callback that uses the spider state (other spider code is omitted for brevity):\n",
    "\n",
    "```python\n",
    "def parse_item(self, response):\n",
    "    # parse item here\n",
    "    self.state[\"items_count\"] = self.state.get(\"items_count\", 0) + 1\n",
    "```\n",
    "\n",
    "Should be written in the `spider.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572d648e-8a7b-4505-b180-fb13fae63269",
   "metadata": {},
   "source": [
    "## Pass `self.state[value]` values from `parse()` to pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c484605-e095-47d9-8953-e92b3110fd8a",
   "metadata": {},
   "source": [
    "Just use `spider.state[value]` within the pipeline - it will access the `state` class argument.\n",
    "\n",
    "Be careful though - the state values cannot be effectively instantiated in the spider's `__init__` method, because the go to the pipeline's `open_spider()` method and then they just disappear. Instead, instantiate them withint the `parse` method and below (see the diagram below) - then they will be saved. Thus, all of your counters will be smashed throughout the code... \n",
    "\n",
    "```\n",
    "DEBUG: spider: SomeSpider init -> self.state = dict()\n",
    "DEBUG: pipeline: from_crawler class method\n",
    "DEBUG: pipeline: SomePipeline init\n",
    "DEBUG: pipeline: open_spider -> spider.state is accessible\n",
    "DEBUG: spider: start_requests -> self.state is not accessible anymore\n",
    "DEBUG: spider: parse\n",
    "DEBUG: spider: parse_item\n",
    "DEBUG: pipeline: process_item\n",
    "DEBUG: pipeline: close_spider\n",
    "DEBUG: spider: closed\n",
    "```\n",
    "\n",
    "> Note: If you instantiate `self.state` within the spider's `init` method, it will be counted as another `self.state` argument unlike the one that you define within the `parse` and below methods. In general, for March 1st 2024, the spider's `init` arguments are set anew each run, unlike the `state` values that instantiated within `parse` and below methods - they are saved as the permanent `state`. So, be cautious."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2703e7dc-4409-4623-a05f-f317d6bed16b",
   "metadata": {},
   "source": [
    "## Persistence gotchas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c0dae7-9327-42a1-9e13-47cdf232d810",
   "metadata": {},
   "source": [
    "There are a few things to keep in mind if you want to be able to use the Scrapy persistence support:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a710acf-7c61-42f8-9c2c-2408b630cea2",
   "metadata": {},
   "source": [
    "### Cookies expiration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2379a5-a958-4c86-a724-3f9439523b26",
   "metadata": {},
   "source": [
    "Cookies may expire. So, if you don’t resume your spider quickly the requests scheduled may no longer work. This won’t be an issue if your spider doesn’t rely on cookies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba062136-3919-40a5-b27d-eb6927171d79",
   "metadata": {},
   "source": [
    "### Request serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d0ad61-4a1e-49a2-aaaa-8d99af26e915",
   "metadata": {},
   "source": [
    "For persistence to work, `Request` objects must be serializable with [`pickle`](https://docs.python.org/3/library/pickle.html#module-pickle), except for the `callback` and `errback` values passed to their `__init__` method, which must be methods of the running `Spider` class.\n",
    "\n",
    "If you wish to log the requests that couldn’t be serialized, you can set the `SCHEDULER_DEBUG` setting to `True` in the project’s settings page. It is `False` by default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942d63ed-16e6-4ef7-ae12-73b5ea5dacc9",
   "metadata": {},
   "source": [
    "# <b>5. Extending Scrapy</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70898892-f4eb-4bcf-b4ca-fbde54e3bfdb",
   "metadata": {},
   "source": [
    "# 5.2 Add-ons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ec8d5f-1bd1-4954-9812-94c90ddbf35f",
   "metadata": {},
   "source": [
    "## Built-in downloader middleware reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab510a73-8ed6-4df9-9c06-4a5694fea893",
   "metadata": {},
   "source": [
    "### `CookiesMiddleware`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031251d6-91b1-431d-85c7-77399188ad04",
   "metadata": {},
   "source": [
    "# 5.4 Spider Middleware"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3a8f84-ccd6-4e6b-aff3-f7a1494eab73",
   "metadata": {},
   "source": [
    "# 5.8 Item Exporters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfcc4ae-f1d5-47d2-9bb5-79b232d870c2",
   "metadata": {},
   "source": [
    "## Serialization of item fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61d7184-97d7-42b7-9de6-0ed960ddc994",
   "metadata": {},
   "source": [
    "# 5.10 Core API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567a1eee-4dfd-4916-b023-a5711160ddc4",
   "metadata": {},
   "source": [
    "## Crawler API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfd090d-5027-4557-8865-67fc52ba39fd",
   "metadata": {},
   "source": [
    "# <b>6. All the rest</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa5b299-6db3-4817-9b8b-2181bf669e30",
   "metadata": {},
   "source": [
    "# <b>7. Source code</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7d5878-3fc0-43ee-b9ec-7beee99cac23",
   "metadata": {},
   "source": [
    "## `scrapy.spiders.crawl`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fe65ea-be06-4164-a3a5-8869eb302c86",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"\n",
    "This modules implements the CrawlSpider which is the recommended spider to use\n",
    "for scraping typical web sites that requires crawling pages.\n",
    "\n",
    "See documentation in docs/topics/spiders.rst\n",
    "\"\"\"\n",
    "\n",
    "import copy\n",
    "from typing import AsyncIterable, Awaitable, Sequence\n",
    "\n",
    "from scrapy.http import HtmlResponse, Request, Response\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.spiders import Spider\n",
    "from scrapy.utils.asyncgen import collect_asyncgen\n",
    "from scrapy.utils.spider import iterate_spider_output\n",
    "\n",
    "\n",
    "def _identity(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def _identity_process_request(request, response):\n",
    "    return request\n",
    "\n",
    "\n",
    "def _get_method(method, spider):\n",
    "    if callable(method):\n",
    "        return method\n",
    "    if isinstance(method, str):\n",
    "        return getattr(spider, method, None)\n",
    "\n",
    "\n",
    "_default_link_extractor = LinkExtractor()\n",
    "\n",
    "\n",
    "[docs]class Rule:\n",
    "    def __init__(\n",
    "        self,\n",
    "        link_extractor=None,\n",
    "        callback=None,\n",
    "        cb_kwargs=None,\n",
    "        follow=None,\n",
    "        process_links=None,\n",
    "        process_request=None,\n",
    "        errback=None,\n",
    "    ):\n",
    "        self.link_extractor = link_extractor or _default_link_extractor\n",
    "        self.callback = callback\n",
    "        self.errback = errback\n",
    "        self.cb_kwargs = cb_kwargs or {}\n",
    "        self.process_links = process_links or _identity\n",
    "        self.process_request = process_request or _identity_process_request\n",
    "        self.follow = follow if follow is not None else not callback\n",
    "\n",
    "    def _compile(self, spider):\n",
    "        self.callback = _get_method(self.callback, spider)\n",
    "        self.errback = _get_method(self.errback, spider)\n",
    "        self.process_links = _get_method(self.process_links, spider)\n",
    "        self.process_request = _get_method(self.process_request, spider)\n",
    "\n",
    "\n",
    "\n",
    "[docs]class CrawlSpider(Spider):\n",
    "    rules: Sequence[Rule] = ()\n",
    "\n",
    "    def __init__(self, *a, **kw):\n",
    "        super().__init__(*a, **kw)\n",
    "        self._compile_rules()\n",
    "\n",
    "    def _parse(self, response, **kwargs):\n",
    "        return self._parse_response(\n",
    "            response=response,\n",
    "            callback=self.parse_start_url,\n",
    "            cb_kwargs=kwargs,\n",
    "            follow=True,\n",
    "        )\n",
    "\n",
    "[docs]    def parse_start_url(self, response, **kwargs):\n",
    "        return []\n",
    "\n",
    "\n",
    "    def process_results(self, response: Response, results: list):\n",
    "        return results\n",
    "\n",
    "    def _build_request(self, rule_index, link):\n",
    "        return Request(\n",
    "            url=link.url,\n",
    "            callback=self._callback,\n",
    "            errback=self._errback,\n",
    "            meta=dict(rule=rule_index, link_text=link.text),\n",
    "        )\n",
    "\n",
    "    def _requests_to_follow(self, response):\n",
    "        if not isinstance(response, HtmlResponse):\n",
    "            return\n",
    "        seen = set()\n",
    "        for rule_index, rule in enumerate(self._rules):\n",
    "            links = [\n",
    "                lnk\n",
    "                for lnk in rule.link_extractor.extract_links(response)\n",
    "                if lnk not in seen\n",
    "            ]\n",
    "            for link in rule.process_links(links):\n",
    "                seen.add(link)\n",
    "                request = self._build_request(rule_index, link)\n",
    "                yield rule.process_request(request, response)\n",
    "\n",
    "    def _callback(self, response, **cb_kwargs):\n",
    "        rule = self._rules[response.meta[\"rule\"]]\n",
    "        return self._parse_response(\n",
    "            response, rule.callback, {**rule.cb_kwargs, **cb_kwargs}, rule.follow\n",
    "        )\n",
    "\n",
    "    def _errback(self, failure):\n",
    "        rule = self._rules[failure.request.meta[\"rule\"]]\n",
    "        return self._handle_failure(failure, rule.errback)\n",
    "\n",
    "    async def _parse_response(self, response, callback, cb_kwargs, follow=True):\n",
    "        if callback:\n",
    "            cb_res = callback(response, **cb_kwargs) or ()\n",
    "            if isinstance(cb_res, AsyncIterable):\n",
    "                cb_res = await collect_asyncgen(cb_res)\n",
    "            elif isinstance(cb_res, Awaitable):\n",
    "                cb_res = await cb_res\n",
    "            cb_res = self.process_results(response, cb_res)\n",
    "            for request_or_item in iterate_spider_output(cb_res):\n",
    "                yield request_or_item\n",
    "\n",
    "        if follow and self._follow_links:\n",
    "            for request_or_item in self._requests_to_follow(response):\n",
    "                yield request_or_item\n",
    "\n",
    "    def _handle_failure(self, failure, errback):\n",
    "        if errback:\n",
    "            results = errback(failure) or ()\n",
    "            for request_or_item in iterate_spider_output(results):\n",
    "                yield request_or_item\n",
    "\n",
    "    def _compile_rules(self):\n",
    "        self._rules = []\n",
    "        for rule in self.rules:\n",
    "            self._rules.append(copy.copy(rule))\n",
    "            self._rules[-1]._compile(self)\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler, *args, **kwargs):\n",
    "        spider = super().from_crawler(crawler, *args, **kwargs)\n",
    "        spider._follow_links = crawler.settings.getbool(\n",
    "            \"CRAWLSPIDER_FOLLOW_LINKS\", True\n",
    "        )\n",
    "        return spider\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c604e3-ef01-476f-a7ec-d403e5145df5",
   "metadata": {},
   "source": [
    "## `scrapy.item`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d76bc0-e1bf-4862-bca4-bb73c677c185",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"\n",
    "Scrapy Item\n",
    "\n",
    "See documentation in docs/topics/item.rst\n",
    "\"\"\"\n",
    "\n",
    "from abc import ABCMeta\n",
    "from collections.abc import MutableMapping\n",
    "from copy import deepcopy\n",
    "from pprint import pformat\n",
    "from typing import Dict\n",
    "\n",
    "from scrapy.utils.trackref import object_ref\n",
    "\n",
    "\n",
    "class Field(dict):\n",
    "    \"\"\"Container of field metadata\"\"\"\n",
    "\n",
    "\n",
    "[docs]class ItemMeta(ABCMeta):\n",
    "    \"\"\"Metaclass_ of :class:`Item` that handles field definitions.\n",
    "\n",
    "    .. _metaclass: https://realpython.com/python-metaclasses\n",
    "    \"\"\"\n",
    "\n",
    "    def __new__(mcs, class_name, bases, attrs):\n",
    "        classcell = attrs.pop(\"__classcell__\", None)\n",
    "        new_bases = tuple(base._class for base in bases if hasattr(base, \"_class\"))\n",
    "        _class = super().__new__(mcs, \"x_\" + class_name, new_bases, attrs)\n",
    "\n",
    "        fields = getattr(_class, \"fields\", {})\n",
    "        new_attrs = {}\n",
    "        for n in dir(_class):\n",
    "            v = getattr(_class, n)\n",
    "            if isinstance(v, Field):\n",
    "                fields[n] = v\n",
    "            elif n in attrs:\n",
    "                new_attrs[n] = attrs[n]\n",
    "\n",
    "        new_attrs[\"fields\"] = fields\n",
    "        new_attrs[\"_class\"] = _class\n",
    "        if classcell is not None:\n",
    "            new_attrs[\"__classcell__\"] = classcell\n",
    "        return super().__new__(mcs, class_name, bases, new_attrs)\n",
    "\n",
    "\n",
    "\n",
    "class Item(MutableMapping, object_ref, metaclass=ItemMeta):\n",
    "    \"\"\"\n",
    "    Base class for scraped items.\n",
    "\n",
    "    In Scrapy, an object is considered an ``item`` if it is an instance of either\n",
    "    :class:`Item` or :class:`dict`, or any subclass. For example, when the output of a\n",
    "    spider callback is evaluated, only instances of :class:`Item` or\n",
    "    :class:`dict` are passed to :ref:`item pipelines <topics-item-pipeline>`.\n",
    "\n",
    "    If you need instances of a custom class to be considered items by Scrapy,\n",
    "    you must inherit from either :class:`Item` or :class:`dict`.\n",
    "\n",
    "    Items must declare :class:`Field` attributes, which are processed and stored\n",
    "    in the ``fields`` attribute. This restricts the set of allowed field names\n",
    "    and prevents typos, raising ``KeyError`` when referring to undefined fields.\n",
    "    Additionally, fields can be used to define metadata and control the way\n",
    "    data is processed internally. Please refer to the :ref:`documentation\n",
    "    about fields <topics-items-fields>` for additional information.\n",
    "\n",
    "    Unlike instances of :class:`dict`, instances of :class:`Item` may be\n",
    "    :ref:`tracked <topics-leaks-trackrefs>` to debug memory leaks.\n",
    "    \"\"\"\n",
    "\n",
    "    fields: Dict[str, Field]\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self._values = {}\n",
    "        if args or kwargs:  # avoid creating dict for most common case\n",
    "            for k, v in dict(*args, **kwargs).items():\n",
    "                self[k] = v\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self._values[key]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        if key in self.fields:\n",
    "            self._values[key] = value\n",
    "        else:\n",
    "            raise KeyError(f\"{self.__class__.__name__} does not support field: {key}\")\n",
    "\n",
    "    def __delitem__(self, key):\n",
    "        del self._values[key]\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        if name in self.fields:\n",
    "            raise AttributeError(f\"Use item[{name!r}] to get field value\")\n",
    "        raise AttributeError(name)\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        if not name.startswith(\"_\"):\n",
    "            raise AttributeError(f\"Use item[{name!r}] = {value!r} to set field value\")\n",
    "        super().__setattr__(name, value)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._values)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._values)\n",
    "\n",
    "    __hash__ = object_ref.__hash__\n",
    "\n",
    "    def keys(self):\n",
    "        return self._values.keys()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return pformat(dict(self))\n",
    "\n",
    "    def copy(self):\n",
    "        return self.__class__(self)\n",
    "\n",
    "    def deepcopy(self):\n",
    "        \"\"\"Return a :func:`~copy.deepcopy` of this item.\"\"\"\n",
    "        return deepcopy(self)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01be58f8-6678-48b5-9719-adb3d19ded02",
   "metadata": {},
   "source": [
    "## `scrapy.loader`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09268466-adaa-42ad-a82b-4731b597e09e",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"\n",
    "Item Loader\n",
    "\n",
    "See documentation in docs/topics/loaders.rst\n",
    "\"\"\"\n",
    "import itemloaders\n",
    "\n",
    "from scrapy.item import Item\n",
    "from scrapy.selector import Selector\n",
    "\n",
    "\n",
    "class ItemLoader(itemloaders.ItemLoader):\n",
    "    \"\"\"\n",
    "    A user-friendly abstraction to populate an :ref:`item <topics-items>` with data\n",
    "    by applying :ref:`field processors <topics-loaders-processors>` to scraped data.\n",
    "    When instantiated with a ``selector`` or a ``response`` it supports\n",
    "    data extraction from web pages using :ref:`selectors <topics-selectors>`.\n",
    "\n",
    "    :param item: The item instance to populate using subsequent calls to\n",
    "        :meth:`~ItemLoader.add_xpath`, :meth:`~ItemLoader.add_css`,\n",
    "        or :meth:`~ItemLoader.add_value`.\n",
    "    :type item: scrapy.item.Item\n",
    "\n",
    "    :param selector: The selector to extract data from, when using the\n",
    "        :meth:`add_xpath`, :meth:`add_css`, :meth:`replace_xpath`, or\n",
    "        :meth:`replace_css` method.\n",
    "    :type selector: :class:`~scrapy.selector.Selector` object\n",
    "\n",
    "    :param response: The response used to construct the selector using the\n",
    "        :attr:`default_selector_class`, unless the selector argument is given,\n",
    "        in which case this argument is ignored.\n",
    "    :type response: :class:`~scrapy.http.Response` object\n",
    "\n",
    "    If no item is given, one is instantiated automatically using the class in\n",
    "    :attr:`default_item_class`.\n",
    "\n",
    "    The item, selector, response and remaining keyword arguments are\n",
    "    assigned to the Loader context (accessible through the :attr:`context` attribute).\n",
    "\n",
    "    .. attribute:: item\n",
    "\n",
    "        The item object being parsed by this Item Loader.\n",
    "        This is mostly used as a property so, when attempting to override this\n",
    "        value, you may want to check out :attr:`default_item_class` first.\n",
    "\n",
    "    .. attribute:: context\n",
    "\n",
    "        The currently active :ref:`Context <loaders-context>` of this Item Loader.\n",
    "\n",
    "    .. attribute:: default_item_class\n",
    "\n",
    "        An :ref:`item <topics-items>` class (or factory), used to instantiate\n",
    "        items when not given in the ``__init__`` method.\n",
    "\n",
    "    .. attribute:: default_input_processor\n",
    "\n",
    "        The default input processor to use for those fields which don't specify\n",
    "        one.\n",
    "\n",
    "    .. attribute:: default_output_processor\n",
    "\n",
    "        The default output processor to use for those fields which don't specify\n",
    "        one.\n",
    "\n",
    "    .. attribute:: default_selector_class\n",
    "\n",
    "        The class used to construct the :attr:`selector` of this\n",
    "        :class:`ItemLoader`, if only a response is given in the ``__init__`` method.\n",
    "        If a selector is given in the ``__init__`` method this attribute is ignored.\n",
    "        This attribute is sometimes overridden in subclasses.\n",
    "\n",
    "    .. attribute:: selector\n",
    "\n",
    "        The :class:`~scrapy.selector.Selector` object to extract data from.\n",
    "        It's either the selector given in the ``__init__`` method or one created from\n",
    "        the response given in the ``__init__`` method using the\n",
    "        :attr:`default_selector_class`. This attribute is meant to be\n",
    "        read-only.\n",
    "    \"\"\"\n",
    "\n",
    "    default_item_class = Item\n",
    "    default_selector_class = Selector\n",
    "\n",
    "    def __init__(self, item=None, selector=None, response=None, parent=None, **context):\n",
    "        if selector is None and response is not None:\n",
    "            try:\n",
    "                selector = self.default_selector_class(response)\n",
    "            except AttributeError:\n",
    "                selector = None\n",
    "        context.update(response=response)\n",
    "        super().__init__(item=item, selector=selector, parent=parent, **context)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53afa41e-90fd-4a5f-b2fe-5f4d31655f6b",
   "metadata": {},
   "source": [
    "## `itemadapter.adapter`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923fd1d2-62ca-441e-befb-60c89be1c1de",
   "metadata": {},
   "source": [
    "```python\n",
    "import dataclasses\n",
    "from abc import abstractmethod, ABCMeta\n",
    "from collections import deque\n",
    "from collections.abc import KeysView, MutableMapping\n",
    "from types import MappingProxyType\n",
    "from typing import Any, Deque, Iterator, Type, Optional, List\n",
    "\n",
    "from itemadapter.utils import (\n",
    "    _get_pydantic_model_metadata,\n",
    "    _is_attrs_class,\n",
    "    _is_pydantic_model,\n",
    ")\n",
    "\n",
    "from itemadapter._imports import attr, _scrapy_item_classes\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    \"AdapterInterface\",\n",
    "    \"AttrsAdapter\",\n",
    "    \"DataclassAdapter\",\n",
    "    \"DictAdapter\",\n",
    "    \"ItemAdapter\",\n",
    "    \"PydanticAdapter\",\n",
    "    \"ScrapyItemAdapter\",\n",
    "]\n",
    "\n",
    "\n",
    "class AdapterInterface(MutableMapping, metaclass=ABCMeta):\n",
    "    \"\"\"Abstract Base Class for adapters.\n",
    "\n",
    "    An adapter that handles a specific type of item should inherit from this\n",
    "    class and implement the abstract methods defined here, plus the\n",
    "    abtract methods inherited from the MutableMapping base class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, item: Any) -> None:\n",
    "        self.item = item\n",
    "\n",
    "    @classmethod\n",
    "    @abstractmethod\n",
    "    def is_item_class(cls, item_class: type) -> bool:\n",
    "        \"\"\"Return True if the adapter can handle the given item class, False otherwise.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def is_item(cls, item: Any) -> bool:\n",
    "        \"\"\"Return True if the adapter can handle the given item, False otherwise.\"\"\"\n",
    "        return cls.is_item_class(item.__class__)\n",
    "\n",
    "    @classmethod\n",
    "    def get_field_meta_from_class(cls, item_class: type, field_name: str) -> MappingProxyType:\n",
    "        return MappingProxyType({})\n",
    "\n",
    "    @classmethod\n",
    "    def get_field_names_from_class(cls, item_class: type) -> Optional[List[str]]:\n",
    "        \"\"\"Return a list of fields defined for ``item_class``.\n",
    "        If a class doesn't support fields, None is returned.\"\"\"\n",
    "        return None\n",
    "\n",
    "    def get_field_meta(self, field_name: str) -> MappingProxyType:\n",
    "        \"\"\"Return metadata for the given field name, if available.\"\"\"\n",
    "        return self.get_field_meta_from_class(self.item.__class__, field_name)\n",
    "\n",
    "    def field_names(self) -> KeysView:\n",
    "        \"\"\"Return a dynamic view of the item's field names.\"\"\"\n",
    "        return self.keys()  # type: ignore[return-value]\n",
    "\n",
    "\n",
    "class _MixinAttrsDataclassAdapter:\n",
    "\n",
    "    _fields_dict: dict\n",
    "    item: Any\n",
    "\n",
    "    def get_field_meta(self, field_name: str) -> MappingProxyType:\n",
    "        return self._fields_dict[field_name].metadata\n",
    "\n",
    "    def field_names(self) -> KeysView:\n",
    "        return KeysView(self._fields_dict)\n",
    "\n",
    "    def __getitem__(self, field_name: str) -> Any:\n",
    "        if field_name in self._fields_dict:\n",
    "            return getattr(self.item, field_name)\n",
    "        raise KeyError(field_name)\n",
    "\n",
    "    def __setitem__(self, field_name: str, value: Any) -> None:\n",
    "        if field_name in self._fields_dict:\n",
    "            setattr(self.item, field_name, value)\n",
    "        else:\n",
    "            raise KeyError(f\"{self.item.__class__.__name__} does not support field: {field_name}\")\n",
    "\n",
    "    def __delitem__(self, field_name: str) -> None:\n",
    "        if field_name in self._fields_dict:\n",
    "            try:\n",
    "                delattr(self.item, field_name)\n",
    "            except AttributeError:\n",
    "                raise KeyError(field_name)\n",
    "        else:\n",
    "            raise KeyError(f\"{self.item.__class__.__name__} does not support field: {field_name}\")\n",
    "\n",
    "    def __iter__(self) -> Iterator:\n",
    "        return iter(attr for attr in self._fields_dict if hasattr(self.item, attr))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(list(iter(self)))\n",
    "\n",
    "\n",
    "class AttrsAdapter(_MixinAttrsDataclassAdapter, AdapterInterface):\n",
    "    def __init__(self, item: Any) -> None:\n",
    "        super().__init__(item)\n",
    "        if attr is None:\n",
    "            raise RuntimeError(\"attr module is not available\")\n",
    "        # store a reference to the item's fields to avoid O(n) lookups and O(n^2) traversals\n",
    "        self._fields_dict = attr.fields_dict(self.item.__class__)\n",
    "\n",
    "    @classmethod\n",
    "    def is_item(cls, item: Any) -> bool:\n",
    "        return _is_attrs_class(item) and not isinstance(item, type)\n",
    "\n",
    "    @classmethod\n",
    "    def is_item_class(cls, item_class: type) -> bool:\n",
    "        return _is_attrs_class(item_class)\n",
    "\n",
    "    @classmethod\n",
    "    def get_field_meta_from_class(cls, item_class: type, field_name: str) -> MappingProxyType:\n",
    "        if attr is None:\n",
    "            raise RuntimeError(\"attr module is not available\")\n",
    "        try:\n",
    "            return attr.fields_dict(item_class)[field_name].metadata  # type: ignore\n",
    "        except KeyError:\n",
    "            raise KeyError(f\"{item_class.__name__} does not support field: {field_name}\")\n",
    "\n",
    "    @classmethod\n",
    "    def get_field_names_from_class(cls, item_class: type) -> Optional[List[str]]:\n",
    "        if attr is None:\n",
    "            raise RuntimeError(\"attr module is not available\")\n",
    "        return [a.name for a in attr.fields(item_class)]\n",
    "\n",
    "\n",
    "class DataclassAdapter(_MixinAttrsDataclassAdapter, AdapterInterface):\n",
    "    def __init__(self, item: Any) -> None:\n",
    "        super().__init__(item)\n",
    "        # store a reference to the item's fields to avoid O(n) lookups and O(n^2) traversals\n",
    "        self._fields_dict = {field.name: field for field in dataclasses.fields(self.item)}\n",
    "\n",
    "    @classmethod\n",
    "    def is_item(cls, item: Any) -> bool:\n",
    "        return dataclasses.is_dataclass(item) and not isinstance(item, type)\n",
    "\n",
    "    @classmethod\n",
    "    def is_item_class(cls, item_class: type) -> bool:\n",
    "        return dataclasses.is_dataclass(item_class)\n",
    "\n",
    "    @classmethod\n",
    "    def get_field_meta_from_class(cls, item_class: type, field_name: str) -> MappingProxyType:\n",
    "        for field in dataclasses.fields(item_class):\n",
    "            if field.name == field_name:\n",
    "                return field.metadata  # type: ignore\n",
    "        raise KeyError(f\"{item_class.__name__} does not support field: {field_name}\")\n",
    "\n",
    "    @classmethod\n",
    "    def get_field_names_from_class(cls, item_class: type) -> Optional[List[str]]:\n",
    "        return [a.name for a in dataclasses.fields(item_class)]\n",
    "\n",
    "\n",
    "class PydanticAdapter(AdapterInterface):\n",
    "\n",
    "    item: Any\n",
    "\n",
    "    @classmethod\n",
    "    def is_item_class(cls, item_class: type) -> bool:\n",
    "        return _is_pydantic_model(item_class)\n",
    "\n",
    "    @classmethod\n",
    "    def get_field_meta_from_class(cls, item_class: type, field_name: str) -> MappingProxyType:\n",
    "        try:\n",
    "            return _get_pydantic_model_metadata(item_class, field_name)\n",
    "        except KeyError:\n",
    "            raise KeyError(f\"{item_class.__name__} does not support field: {field_name}\")\n",
    "\n",
    "    @classmethod\n",
    "    def get_field_names_from_class(cls, item_class: type) -> Optional[List[str]]:\n",
    "        return list(item_class.__fields__.keys())  # type: ignore[attr-defined]\n",
    "\n",
    "    def field_names(self) -> KeysView:\n",
    "        return KeysView(self.item.__fields__)\n",
    "\n",
    "    def __getitem__(self, field_name: str) -> Any:\n",
    "        if field_name in self.item.__fields__:\n",
    "            return getattr(self.item, field_name)\n",
    "        raise KeyError(field_name)\n",
    "\n",
    "    def __setitem__(self, field_name: str, value: Any) -> None:\n",
    "        if field_name in self.item.__fields__:\n",
    "            setattr(self.item, field_name, value)\n",
    "        else:\n",
    "            raise KeyError(f\"{self.item.__class__.__name__} does not support field: {field_name}\")\n",
    "\n",
    "    def __delitem__(self, field_name: str) -> None:\n",
    "        if field_name in self.item.__fields__:\n",
    "            try:\n",
    "                delattr(self.item, field_name)\n",
    "            except AttributeError:\n",
    "                raise KeyError(field_name)\n",
    "        else:\n",
    "            raise KeyError(f\"{self.item.__class__.__name__} does not support field: {field_name}\")\n",
    "\n",
    "    def __iter__(self) -> Iterator:\n",
    "        return iter(attr for attr in self.item.__fields__ if hasattr(self.item, attr))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(list(iter(self)))\n",
    "\n",
    "\n",
    "class _MixinDictScrapyItemAdapter:\n",
    "\n",
    "    _fields_dict: dict\n",
    "    item: Any\n",
    "\n",
    "    def __getitem__(self, field_name: str) -> Any:\n",
    "        return self.item[field_name]\n",
    "\n",
    "    def __setitem__(self, field_name: str, value: Any) -> None:\n",
    "        self.item[field_name] = value\n",
    "\n",
    "    def __delitem__(self, field_name: str) -> None:\n",
    "        del self.item[field_name]\n",
    "\n",
    "    def __iter__(self) -> Iterator:\n",
    "        return iter(self.item)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.item)\n",
    "\n",
    "\n",
    "class DictAdapter(_MixinDictScrapyItemAdapter, AdapterInterface):\n",
    "    @classmethod\n",
    "    def is_item(cls, item: Any) -> bool:\n",
    "        return isinstance(item, dict)\n",
    "\n",
    "    @classmethod\n",
    "    def is_item_class(cls, item_class: type) -> bool:\n",
    "        return issubclass(item_class, dict)\n",
    "\n",
    "    def field_names(self) -> KeysView:\n",
    "        return KeysView(self.item)\n",
    "\n",
    "\n",
    "class ScrapyItemAdapter(_MixinDictScrapyItemAdapter, AdapterInterface):\n",
    "    @classmethod\n",
    "    def is_item(cls, item: Any) -> bool:\n",
    "        return isinstance(item, _scrapy_item_classes)\n",
    "\n",
    "    @classmethod\n",
    "    def is_item_class(cls, item_class: type) -> bool:\n",
    "        return issubclass(item_class, _scrapy_item_classes)\n",
    "\n",
    "    @classmethod\n",
    "    def get_field_meta_from_class(cls, item_class: type, field_name: str) -> MappingProxyType:\n",
    "        return MappingProxyType(item_class.fields[field_name])  # type: ignore[attr-defined]\n",
    "\n",
    "    @classmethod\n",
    "    def get_field_names_from_class(cls, item_class: type) -> Optional[List[str]]:\n",
    "        return list(item_class.fields.keys())  # type: ignore[attr-defined]\n",
    "\n",
    "    def field_names(self) -> KeysView:\n",
    "        return KeysView(self.item.fields)\n",
    "\n",
    "\n",
    "[docs]class ItemAdapter(MutableMapping):\n",
    "    \"\"\"Wrapper class to interact with data container objects. It provides a common interface\n",
    "    to extract and set data without having to take the object's type into account.\n",
    "    \"\"\"\n",
    "\n",
    "    ADAPTER_CLASSES: Deque[Type[AdapterInterface]] = deque(\n",
    "        [\n",
    "            ScrapyItemAdapter,\n",
    "            DictAdapter,\n",
    "            DataclassAdapter,\n",
    "            AttrsAdapter,\n",
    "            PydanticAdapter,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    def __init__(self, item: Any) -> None:\n",
    "        for cls in self.ADAPTER_CLASSES:\n",
    "            if cls.is_item(item):\n",
    "                self.adapter = cls(item)\n",
    "                break\n",
    "        else:\n",
    "            raise TypeError(f\"No adapter found for objects of type: {type(item)} ({item})\")\n",
    "\n",
    "    @classmethod\n",
    "    def is_item(cls, item: Any) -> bool:\n",
    "        for adapter_class in cls.ADAPTER_CLASSES:\n",
    "            if adapter_class.is_item(item):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    @classmethod\n",
    "    def is_item_class(cls, item_class: type) -> bool:\n",
    "        for adapter_class in cls.ADAPTER_CLASSES:\n",
    "            if adapter_class.is_item_class(item_class):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    @classmethod\n",
    "    def _get_adapter_class(cls, item_class: type) -> Type[AdapterInterface]:\n",
    "        for adapter_class in cls.ADAPTER_CLASSES:\n",
    "            if adapter_class.is_item_class(item_class):\n",
    "                return adapter_class\n",
    "        raise TypeError(f\"{item_class} is not a valid item class\")\n",
    "\n",
    "    @classmethod\n",
    "    def get_field_meta_from_class(cls, item_class: type, field_name: str) -> MappingProxyType:\n",
    "        adapter_class = cls._get_adapter_class(item_class)\n",
    "        return adapter_class.get_field_meta_from_class(item_class, field_name)\n",
    "\n",
    "    @classmethod\n",
    "    def get_field_names_from_class(cls, item_class: type) -> Optional[List[str]]:\n",
    "        adapter_class = cls._get_adapter_class(item_class)\n",
    "        return adapter_class.get_field_names_from_class(item_class)\n",
    "\n",
    "    @property\n",
    "    def item(self) -> Any:\n",
    "        return self.adapter.item\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        values = \", \".join([f\"{key}={value!r}\" for key, value in self.items()])\n",
    "        return f\"<{self.__class__.__name__} for {self.item.__class__.__name__}({values})>\"\n",
    "\n",
    "    def __getitem__(self, field_name: str) -> Any:\n",
    "        return self.adapter.__getitem__(field_name)\n",
    "\n",
    "    def __setitem__(self, field_name: str, value: Any) -> None:\n",
    "        self.adapter.__setitem__(field_name, value)\n",
    "\n",
    "    def __delitem__(self, field_name: str) -> None:\n",
    "        self.adapter.__delitem__(field_name)\n",
    "\n",
    "    def __iter__(self) -> Iterator:\n",
    "        return self.adapter.__iter__()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.adapter.__len__()\n",
    "\n",
    "    def get_field_meta(self, field_name: str) -> MappingProxyType:\n",
    "        \"\"\"Return metadata for the given field name.\"\"\"\n",
    "        return self.adapter.get_field_meta(field_name)\n",
    "\n",
    "    def field_names(self) -> KeysView:\n",
    "        \"\"\"Return read-only key view with the names of all the defined fields for the item.\"\"\"\n",
    "        return self.adapter.field_names()\n",
    "\n",
    "    def asdict(self) -> dict:\n",
    "        \"\"\"Return a dict object with the contents of the adapter. This works slightly different\n",
    "        than calling `dict(adapter)`: it's applied recursively to nested items (if there are any).\n",
    "        \"\"\"\n",
    "        return {key: self._asdict(value) for key, value in self.items()}\n",
    "\n",
    "    @classmethod\n",
    "    def _asdict(cls, obj: Any) -> Any:\n",
    "        if isinstance(obj, dict):\n",
    "            return {key: cls._asdict(value) for key, value in obj.items()}\n",
    "        if isinstance(obj, (list, set, tuple)):\n",
    "            return obj.__class__(cls._asdict(x) for x in obj)\n",
    "        if isinstance(obj, cls):\n",
    "            return obj.asdict()\n",
    "        if cls.is_item(obj):\n",
    "            return cls(obj).asdict()\n",
    "        return obj\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23458589-7d4a-490f-ba23-565f3fcc9b54",
   "metadata": {},
   "source": [
    "## `itemloaders`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725d3a2b-b164-43d4-b690-b8d381c5176a",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"\n",
    "Item Loader\n",
    "\n",
    "See documentation in docs/topics/loaders.rst\n",
    "\"\"\"\n",
    "from contextlib import suppress\n",
    "\n",
    "from itemadapter import ItemAdapter\n",
    "from parsel.utils import extract_regex, flatten\n",
    "\n",
    "from itemloaders.common import wrap_loader_context\n",
    "from itemloaders.processors import Identity\n",
    "from itemloaders.utils import arg_to_iter\n",
    "\n",
    "\n",
    "def unbound_method(method):\n",
    "    \"\"\"\n",
    "    Allow to use single-argument functions as input or output processors\n",
    "    (no need to define an unused first 'self' argument)\n",
    "    \"\"\"\n",
    "    with suppress(AttributeError):\n",
    "        if \".\" not in method.__qualname__:\n",
    "            return method.__func__\n",
    "    return method\n",
    "\n",
    "\n",
    "class ItemLoader:\n",
    "    \"\"\"\n",
    "    Return a new Item Loader for populating the given item. If no item is\n",
    "    given, one is instantiated automatically using the class in\n",
    "    :attr:`default_item_class`.\n",
    "\n",
    "    When instantiated with a :param ``selector`` parameter the :class:`ItemLoader` class\n",
    "    provides convenient mechanisms for extracting data from web pages\n",
    "    using parsel_ selectors.\n",
    "\n",
    "    :param item: The item instance to populate using subsequent calls to\n",
    "        :meth:`~ItemLoader.add_xpath`, :meth:`~ItemLoader.add_css`,\n",
    "        :meth:`~ItemLoader.add_jmes` or :meth:`~ItemLoader.add_value`.\n",
    "    :type item: :class:`dict` object\n",
    "\n",
    "    :param selector: The selector to extract data from, when using the\n",
    "        :meth:`add_xpath` (resp. :meth:`add_css`, :meth:`add_jmes`) or :meth:`replace_xpath`\n",
    "        (resp. :meth:`replace_css`, :meth:`replace_jmes`) method.\n",
    "    :type selector: :class:`~parsel.selector.Selector` object\n",
    "\n",
    "    The item, selector and the remaining keyword arguments are\n",
    "    assigned to the Loader context (accessible through the :attr:`context` attribute).\n",
    "\n",
    "    .. attribute:: item\n",
    "\n",
    "        The item object being parsed by this Item Loader.\n",
    "        This is mostly used as a property so when attempting to override this\n",
    "        value, you may want to check out :attr:`default_item_class` first.\n",
    "\n",
    "    .. attribute:: context\n",
    "\n",
    "        The currently active :ref:`Context <loaders-context>` of this Item Loader.\n",
    "        Refer to <loaders-context> for more information about the Loader Context.\n",
    "\n",
    "    .. attribute:: default_item_class\n",
    "\n",
    "        An Item class (or factory), used to instantiate items when not given in\n",
    "        the ``__init__`` method.\n",
    "\n",
    "        .. warning:: Currently, this factory/class needs to be\n",
    "            callable/instantiated without any arguments.\n",
    "            If you are using ``dataclasses``, please consider the following\n",
    "            alternative::\n",
    "\n",
    "                from dataclasses import dataclass, field\n",
    "                from typing import Optional\n",
    "\n",
    "                @dataclass\n",
    "                class Product:\n",
    "                    name: Optional[str] = field(default=None)\n",
    "                    price: Optional[float] = field(default=None)\n",
    "\n",
    "    .. attribute:: default_input_processor\n",
    "\n",
    "        The default input processor to use for those fields which don't specify\n",
    "        one.\n",
    "\n",
    "    .. attribute:: default_output_processor\n",
    "\n",
    "        The default output processor to use for those fields which don't specify\n",
    "        one.\n",
    "\n",
    "    .. attribute:: selector\n",
    "\n",
    "        The :class:`~parsel.selector.Selector` object to extract data from.\n",
    "        It's the selector given in the ``__init__`` method.\n",
    "        This attribute is meant to be read-only.\n",
    "\n",
    "    .. _parsel: https://parsel.readthedocs.io/en/latest/\n",
    "    \"\"\"\n",
    "\n",
    "    default_item_class = dict\n",
    "    default_input_processor = Identity()\n",
    "    default_output_processor = Identity()\n",
    "\n",
    "    def __init__(self, item=None, selector=None, parent=None, **context):\n",
    "        self.selector = selector\n",
    "        context.update(selector=selector)\n",
    "        if item is None:\n",
    "            item = self.default_item_class()\n",
    "        self._local_item = item\n",
    "        context[\"item\"] = item\n",
    "        self.context = context\n",
    "        self.parent = parent\n",
    "        self._local_values = {}\n",
    "        # values from initial item\n",
    "        for field_name, value in ItemAdapter(item).items():\n",
    "            self._values.setdefault(field_name, [])\n",
    "            self._values[field_name] += arg_to_iter(value)\n",
    "\n",
    "    @property\n",
    "    def _values(self):\n",
    "        if self.parent is not None:\n",
    "            return self.parent._values\n",
    "        else:\n",
    "            return self._local_values\n",
    "\n",
    "    @property\n",
    "    def item(self):\n",
    "        if self.parent is not None:\n",
    "            return self.parent.item\n",
    "        else:\n",
    "            return self._local_item\n",
    "\n",
    "    def nested_xpath(self, xpath, **context):\n",
    "        \"\"\"\n",
    "        Create a nested loader with an xpath selector.\n",
    "        The supplied selector is applied relative to selector associated\n",
    "        with this :class:`ItemLoader`. The nested loader shares the item\n",
    "        with the parent :class:`ItemLoader` so calls to :meth:`add_xpath`,\n",
    "        :meth:`add_value`, :meth:`replace_value`, etc. will behave as expected.\n",
    "        \"\"\"\n",
    "        selector = self.selector.xpath(xpath)\n",
    "        context.update(selector=selector)\n",
    "        subloader = self.__class__(item=self.item, parent=self, **context)\n",
    "        return subloader\n",
    "\n",
    "\n",
    "    def nested_css(self, css, **context):\n",
    "        \"\"\"\n",
    "        Create a nested loader with a css selector.\n",
    "        The supplied selector is applied relative to selector associated\n",
    "        with this :class:`ItemLoader`. The nested loader shares the item\n",
    "        with the parent :class:`ItemLoader` so calls to :meth:`add_xpath`,\n",
    "        :meth:`add_value`, :meth:`replace_value`, etc. will behave as expected.\n",
    "        \"\"\"\n",
    "        selector = self.selector.css(css)\n",
    "        context.update(selector=selector)\n",
    "        subloader = self.__class__(item=self.item, parent=self, **context)\n",
    "        return subloader\n",
    "\n",
    "\n",
    "    def add_value(self, field_name, value, *processors, re=None, **kw):\n",
    "        \"\"\"\n",
    "        Process and then add the given ``value`` for the given field.\n",
    "\n",
    "        The value is first passed through :meth:`get_value` by giving the\n",
    "        ``processors`` and ``kwargs``, and then passed through the\n",
    "        :ref:`field input processor <processors>` and its result\n",
    "        appended to the data collected for that field. If the field already\n",
    "        contains collected data, the new data is added.\n",
    "\n",
    "        The given ``field_name`` can be ``None``, in which case values for\n",
    "        multiple fields may be added. And the processed value should be a dict\n",
    "        with field_name mapped to values.\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            loader.add_value('name', 'Color TV')\n",
    "            loader.add_value('colours', ['white', 'blue'])\n",
    "            loader.add_value('length', '100')\n",
    "            loader.add_value('name', 'name: foo', TakeFirst(), re='name: (.+)')\n",
    "            loader.add_value(None, {'name': 'foo', 'sex': 'male'})\n",
    "        \"\"\"\n",
    "        value = self.get_value(value, *processors, re=re, **kw)\n",
    "        if value is None:\n",
    "            return\n",
    "        if not field_name:\n",
    "            for k, v in value.items():\n",
    "                self._add_value(k, v)\n",
    "        else:\n",
    "            self._add_value(field_name, value)\n",
    "\n",
    "\n",
    "    def replace_value(self, field_name, value, *processors, re=None, **kw):\n",
    "        \"\"\"\n",
    "        Similar to :meth:`add_value` but replaces the collected data with the\n",
    "        new value instead of adding it.\n",
    "        \"\"\"\n",
    "        value = self.get_value(value, *processors, re=re, **kw)\n",
    "        if value is None:\n",
    "            return\n",
    "        if not field_name:\n",
    "            for k, v in value.items():\n",
    "                self._replace_value(k, v)\n",
    "        else:\n",
    "            self._replace_value(field_name, value)\n",
    "\n",
    "\n",
    "    def _add_value(self, field_name, value):\n",
    "        value = arg_to_iter(value)\n",
    "        processed_value = self._process_input_value(field_name, value)\n",
    "        if processed_value:\n",
    "            self._values.setdefault(field_name, [])\n",
    "            self._values[field_name] += arg_to_iter(processed_value)\n",
    "\n",
    "    def _replace_value(self, field_name, value):\n",
    "        self._values.pop(field_name, None)\n",
    "        self._add_value(field_name, value)\n",
    "\n",
    "    def get_value(self, value, *processors, re=None, **kw):\n",
    "        \"\"\"\n",
    "        Process the given ``value`` by the given ``processors`` and keyword\n",
    "        arguments.\n",
    "\n",
    "        Available keyword arguments:\n",
    "\n",
    "        :param re: a regular expression to use for extracting data from the\n",
    "            given value using :func:`~parsel.utils.extract_regex` method,\n",
    "            applied before processors\n",
    "        :type re: str or typing.Pattern\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        >>> from itemloaders import ItemLoader\n",
    "        >>> from itemloaders.processors import TakeFirst\n",
    "        >>> loader = ItemLoader()\n",
    "        >>> loader.get_value('name: foo', TakeFirst(), str.upper, re='name: (.+)')\n",
    "        'FOO'\n",
    "        \"\"\"\n",
    "        if re:\n",
    "            value = arg_to_iter(value)\n",
    "            value = flatten(extract_regex(re, x) for x in value)\n",
    "\n",
    "        for proc in processors:\n",
    "            if value is None:\n",
    "                break\n",
    "            _proc = proc\n",
    "            proc = wrap_loader_context(proc, self.context)\n",
    "            try:\n",
    "                value = proc(value)\n",
    "            except Exception as e:\n",
    "                raise ValueError(\n",
    "                    \"Error with processor %s value=%r error='%s: %s'\"\n",
    "                    % (_proc.__class__.__name__, value, type(e).__name__, str(e))\n",
    "                ) from e\n",
    "        return value\n",
    "\n",
    "\n",
    "    def load_item(self):\n",
    "        \"\"\"\n",
    "        Populate the item with the data collected so far, and return it. The\n",
    "        data collected is first passed through the :ref:`output processors\n",
    "        <processors>` to get the final value to assign to each item field.\n",
    "        \"\"\"\n",
    "        adapter = ItemAdapter(self.item)\n",
    "        for field_name in tuple(self._values):\n",
    "            value = self.get_output_value(field_name)\n",
    "            if value is not None:\n",
    "                adapter[field_name] = value\n",
    "\n",
    "        return adapter.item\n",
    "\n",
    "\n",
    "    def get_output_value(self, field_name):\n",
    "        \"\"\"\n",
    "        Return the collected values parsed using the output processor, for the\n",
    "        given field. This method doesn't populate or modify the item at all.\n",
    "        \"\"\"\n",
    "        proc = self.get_output_processor(field_name)\n",
    "        proc = wrap_loader_context(proc, self.context)\n",
    "        value = self._values.get(field_name, [])\n",
    "        try:\n",
    "            return proc(value)\n",
    "        except Exception as e:\n",
    "            raise ValueError(\n",
    "                \"Error with output processor: field=%r value=%r error='%s: %s'\"\n",
    "                % (field_name, value, type(e).__name__, str(e))\n",
    "            ) from e\n",
    "\n",
    "\n",
    "    def get_collected_values(self, field_name):\n",
    "        \"\"\"Return the collected values for the given field.\"\"\"\n",
    "        return self._values.get(field_name, [])\n",
    "\n",
    "\n",
    "    def get_input_processor(self, field_name):\n",
    "        proc = getattr(self, \"%s_in\" % field_name, None)\n",
    "        if not proc:\n",
    "            proc = self._get_item_field_attr(\n",
    "                field_name, \"input_processor\", self.default_input_processor\n",
    "            )\n",
    "        return unbound_method(proc)\n",
    "\n",
    "    def get_output_processor(self, field_name):\n",
    "        proc = getattr(self, \"%s_out\" % field_name, None)\n",
    "        if not proc:\n",
    "            proc = self._get_item_field_attr(\n",
    "                field_name, \"output_processor\", self.default_output_processor\n",
    "            )\n",
    "        return unbound_method(proc)\n",
    "\n",
    "    def _get_item_field_attr(self, field_name, key, default=None):\n",
    "        field_meta = ItemAdapter(self.item).get_field_meta(field_name)\n",
    "        return field_meta.get(key, default)\n",
    "\n",
    "    def _process_input_value(self, field_name, value):\n",
    "        proc = self.get_input_processor(field_name)\n",
    "        _proc = proc\n",
    "        proc = wrap_loader_context(proc, self.context)\n",
    "        try:\n",
    "            return proc(value)\n",
    "        except Exception as e:\n",
    "            raise ValueError(\n",
    "                \"Error with input processor %s: field=%r value=%r \"\n",
    "                \"error='%s: %s'\"\n",
    "                % (\n",
    "                    _proc.__class__.__name__,\n",
    "                    field_name,\n",
    "                    value,\n",
    "                    type(e).__name__,\n",
    "                    str(e),\n",
    "                )\n",
    "            ) from e\n",
    "\n",
    "    def _check_selector_method(self):\n",
    "        if self.selector is None:\n",
    "            raise RuntimeError(\n",
    "                \"To use XPath or CSS selectors, %s \"\n",
    "                \"must be instantiated with a selector\" % self.__class__.__name__\n",
    "            )\n",
    "\n",
    "    def add_xpath(self, field_name, xpath, *processors, re=None, **kw):\n",
    "        \"\"\"\n",
    "        Similar to :meth:`ItemLoader.add_value` but receives an XPath instead of a\n",
    "        value, which is used to extract a list of strings from the\n",
    "        selector associated with this :class:`ItemLoader`.\n",
    "\n",
    "        See :meth:`get_xpath` for ``kwargs``.\n",
    "\n",
    "        :param xpath: the XPath to extract data from\n",
    "        :type xpath: str\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            # HTML snippet: <p class=\"product-name\">Color TV</p>\n",
    "            loader.add_xpath('name', '//p[@class=\"product-name\"]')\n",
    "            # HTML snippet: <p id=\"price\">the price is $1200</p>\n",
    "            loader.add_xpath('price', '//p[@id=\"price\"]', re='the price is (.*)')\n",
    "\n",
    "        \"\"\"\n",
    "        values = self._get_xpathvalues(xpath, **kw)\n",
    "        self.add_value(field_name, values, *processors, re=re, **kw)\n",
    "\n",
    "\n",
    "    def replace_xpath(self, field_name, xpath, *processors, re=None, **kw):\n",
    "        \"\"\"\n",
    "        Similar to :meth:`add_xpath` but replaces collected data instead of adding it.\n",
    "        \"\"\"\n",
    "        values = self._get_xpathvalues(xpath, **kw)\n",
    "        self.replace_value(field_name, values, *processors, re=re, **kw)\n",
    "\n",
    "\n",
    "    def get_xpath(self, xpath, *processors, re=None, **kw):\n",
    "        \"\"\"\n",
    "        Similar to :meth:`ItemLoader.get_value` but receives an XPath instead of a\n",
    "        value, which is used to extract a list of unicode strings from the\n",
    "        selector associated with this :class:`ItemLoader`.\n",
    "\n",
    "        :param xpath: the XPath to extract data from\n",
    "        :type xpath: str\n",
    "\n",
    "        :param re: a regular expression to use for extracting data from the\n",
    "            selected XPath region\n",
    "        :type re: str or typing.Pattern\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            # HTML snippet: <p class=\"product-name\">Color TV</p>\n",
    "            loader.get_xpath('//p[@class=\"product-name\"]')\n",
    "            # HTML snippet: <p id=\"price\">the price is $1200</p>\n",
    "            loader.get_xpath('//p[@id=\"price\"]', TakeFirst(), re='the price is (.*)')\n",
    "\n",
    "        \"\"\"\n",
    "        values = self._get_xpathvalues(xpath, **kw)\n",
    "        return self.get_value(values, *processors, re=re, **kw)\n",
    "\n",
    "\n",
    "    def _get_xpathvalues(self, xpaths, **kw):\n",
    "        self._check_selector_method()\n",
    "        xpaths = arg_to_iter(xpaths)\n",
    "        return flatten(self.selector.xpath(xpath, **kw).getall() for xpath in xpaths)\n",
    "\n",
    "    def add_css(self, field_name, css, *processors, re=None, **kw):\n",
    "        \"\"\"\n",
    "        Similar to :meth:`ItemLoader.add_value` but receives a CSS selector\n",
    "        instead of a value, which is used to extract a list of unicode strings\n",
    "        from the selector associated with this :class:`ItemLoader`.\n",
    "\n",
    "        See :meth:`get_css` for ``kwargs``.\n",
    "\n",
    "        :param css: the CSS selector to extract data from\n",
    "        :type css: str\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            # HTML snippet: <p class=\"product-name\">Color TV</p>\n",
    "            loader.add_css('name', 'p.product-name')\n",
    "            # HTML snippet: <p id=\"price\">the price is $1200</p>\n",
    "            loader.add_css('price', 'p#price', re='the price is (.*)')\n",
    "        \"\"\"\n",
    "        values = self._get_cssvalues(css)\n",
    "        self.add_value(field_name, values, *processors, re=re, **kw)\n",
    "\n",
    "\n",
    "    def replace_css(self, field_name, css, *processors, re=None, **kw):\n",
    "        \"\"\"\n",
    "        Similar to :meth:`add_css` but replaces collected data instead of adding it.\n",
    "        \"\"\"\n",
    "        values = self._get_cssvalues(css)\n",
    "        self.replace_value(field_name, values, *processors, re=re, **kw)\n",
    "\n",
    "\n",
    "    def get_css(self, css, *processors, re=None, **kw):\n",
    "        \"\"\"\n",
    "        Similar to :meth:`ItemLoader.get_value` but receives a CSS selector\n",
    "        instead of a value, which is used to extract a list of unicode strings\n",
    "        from the selector associated with this :class:`ItemLoader`.\n",
    "\n",
    "        :param css: the CSS selector to extract data from\n",
    "        :type css: str\n",
    "\n",
    "        :param re: a regular expression to use for extracting data from the\n",
    "            selected CSS region\n",
    "        :type re: str or typing.Pattern\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            # HTML snippet: <p class=\"product-name\">Color TV</p>\n",
    "            loader.get_css('p.product-name')\n",
    "            # HTML snippet: <p id=\"price\">the price is $1200</p>\n",
    "            loader.get_css('p#price', TakeFirst(), re='the price is (.*)')\n",
    "        \"\"\"\n",
    "        values = self._get_cssvalues(css)\n",
    "        return self.get_value(values, *processors, re=re, **kw)\n",
    "\n",
    "\n",
    "    def _get_cssvalues(self, csss):\n",
    "        self._check_selector_method()\n",
    "        csss = arg_to_iter(csss)\n",
    "        return flatten(self.selector.css(css).getall() for css in csss)\n",
    "\n",
    "    def add_jmes(self, field_name, jmes, *processors, re=None, **kw):\n",
    "        \"\"\"\n",
    "        Similar to :meth:`ItemLoader.add_value` but receives a JMESPath selector\n",
    "        instead of a value, which is used to extract a list of unicode strings\n",
    "        from the selector associated with this :class:`ItemLoader`.\n",
    "\n",
    "        See :meth:`get_jmes` for ``kwargs``.\n",
    "\n",
    "        :param jmes: the JMESPath selector to extract data from\n",
    "        :type jmes: str\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            # HTML snippet: {\"name\": \"Color TV\"}\n",
    "            loader.add_jmes('name')\n",
    "            # HTML snippet: {\"price\": the price is $1200\"}\n",
    "            loader.add_jmes('price', TakeFirst(), re='the price is (.*)')\n",
    "        \"\"\"\n",
    "        values = self._get_jmesvalues(jmes)\n",
    "        self.add_value(field_name, values, *processors, re=re, **kw)\n",
    "\n",
    "\n",
    "    def replace_jmes(self, field_name, jmes, *processors, re=None, **kw):\n",
    "        \"\"\"\n",
    "        Similar to :meth:`add_jmes` but replaces collected data instead of adding it.\n",
    "        \"\"\"\n",
    "        values = self._get_jmesvalues(jmes)\n",
    "        self.replace_value(field_name, values, *processors, re=re, **kw)\n",
    "\n",
    "\n",
    "    def get_jmes(self, jmes, *processors, re=None, **kw):\n",
    "        \"\"\"\n",
    "        Similar to :meth:`ItemLoader.get_value` but receives a JMESPath selector\n",
    "        instead of a value, which is used to extract a list of unicode strings\n",
    "        from the selector associated with this :class:`ItemLoader`.\n",
    "\n",
    "        :param jmes: the JMESPath selector to extract data from\n",
    "        :type jmes: str\n",
    "\n",
    "        :param re: a regular expression to use for extracting data from the\n",
    "            selected JMESPath\n",
    "        :type re: str or typing.Pattern\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            # HTML snippet: {\"name\": \"Color TV\"}\n",
    "            loader.get_jmes('name')\n",
    "            # HTML snippet: {\"price\": the price is $1200\"}\n",
    "            loader.get_jmes('price', TakeFirst(), re='the price is (.*)')\n",
    "        \"\"\"\n",
    "        values = self._get_jmesvalues(jmes)\n",
    "        return self.get_value(values, *processors, re=re, **kw)\n",
    "\n",
    "\n",
    "    def _get_jmesvalues(self, jmess):\n",
    "        self._check_selector_method()\n",
    "        jmess = arg_to_iter(jmess)\n",
    "        if not hasattr(self.selector, \"jmespath\"):\n",
    "            raise AttributeError(\n",
    "                \"Please install parsel >= 1.8.1 to get jmespath support\"\n",
    "            )\n",
    "        return flatten(self.selector.jmespath(jmes).getall() for jmes in jmess)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2758e087-2ac4-4bfe-90bd-92122080988f",
   "metadata": {},
   "source": [
    "## `itemloaders.processors`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e6cae2-f373-4750-98ec-8e4b1ec95787",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"\n",
    "This module provides some commonly used processors for Item Loaders.\n",
    "\n",
    "See documentation in docs/topics/loaders.rst\n",
    "\"\"\"\n",
    "from collections import ChainMap\n",
    "\n",
    "from itemloaders.common import wrap_loader_context\n",
    "from itemloaders.utils import arg_to_iter\n",
    "\n",
    "\n",
    "class MapCompose:\n",
    "    \"\"\"\n",
    "    A processor which is constructed from the composition of the given\n",
    "    functions, similar to the :class:`Compose` processor. The difference with\n",
    "    this processor is the way internal results are passed among functions,\n",
    "    which is as follows:\n",
    "\n",
    "    The input value of this processor is *iterated* and the first function is\n",
    "    applied to each element. The results of these function calls (one for each element)\n",
    "    are concatenated to construct a new iterable, which is then used to apply the\n",
    "    second function, and so on, until the last function is applied to each\n",
    "    value of the list of values collected so far. The output values of the last\n",
    "    function are concatenated together to produce the output of this processor.\n",
    "\n",
    "    Each particular function can return a value or a list of values, which is\n",
    "    flattened with the list of values returned by the same function applied to\n",
    "    the other input values. The functions can also return ``None`` in which\n",
    "    case the output of that function is ignored for further processing over the\n",
    "    chain.\n",
    "\n",
    "    This processor provides a convenient way to compose functions that only\n",
    "    work with single values (instead of iterables). For this reason the\n",
    "    :class:`MapCompose` processor is typically used as input processor, since\n",
    "    data is often extracted using the\n",
    "    :meth:`~parsel.selector.Selector.extract` method of `parsel selectors`_,\n",
    "    which returns a list of unicode strings.\n",
    "\n",
    "    The example below should clarify how it works:\n",
    "\n",
    "    >>> def filter_world(x):\n",
    "    ...     return None if x == 'world' else x\n",
    "    ...\n",
    "    >>> from itemloaders.processors import MapCompose\n",
    "    >>> proc = MapCompose(filter_world, str.upper)\n",
    "    >>> proc(['hello', 'world', 'this', 'is', 'something'])\n",
    "    ['HELLO', 'THIS', 'IS', 'SOMETHING']\n",
    "\n",
    "    As with the Compose processor, functions can receive Loader contexts, and\n",
    "    ``__init__`` method keyword arguments are used as default context values.\n",
    "    See :class:`Compose` processor for more info.\n",
    "\n",
    "    .. _`parsel selectors`: https://parsel.readthedocs.io/en/latest/parsel.html#parsel.selector.Selector.extract\n",
    "    \"\"\"  # noqa\n",
    "\n",
    "    def __init__(self, *functions, **default_loader_context):\n",
    "        self.functions = functions\n",
    "        self.default_loader_context = default_loader_context\n",
    "\n",
    "    def __call__(self, value, loader_context=None):\n",
    "        values = arg_to_iter(value)\n",
    "        if loader_context:\n",
    "            context = ChainMap(loader_context, self.default_loader_context)\n",
    "        else:\n",
    "            context = self.default_loader_context\n",
    "        wrapped_funcs = [wrap_loader_context(f, context) for f in self.functions]\n",
    "        for func in wrapped_funcs:\n",
    "            next_values = []\n",
    "            for v in values:\n",
    "                try:\n",
    "                    next_values += arg_to_iter(func(v))\n",
    "                except Exception as e:\n",
    "                    raise ValueError(\n",
    "                        \"Error in MapCompose with \"\n",
    "                        \"%s value=%r error='%s: %s'\"\n",
    "                        % (str(func), value, type(e).__name__, str(e))\n",
    "                    ) from e\n",
    "            values = next_values\n",
    "        return values\n",
    "\n",
    "\n",
    "\n",
    "class Compose:\n",
    "    \"\"\"\n",
    "    A processor which is constructed from the composition of the given\n",
    "    functions. This means that each input value of this processor is passed to\n",
    "    the first function, and the result of that function is passed to the second\n",
    "    function, and so on, until the last function returns the output value of\n",
    "    this processor.\n",
    "\n",
    "    By default, stop process on ``None`` value. This behaviour can be changed by\n",
    "    passing keyword argument ``stop_on_none=False``.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    >>> from itemloaders.processors import Compose\n",
    "    >>> proc = Compose(lambda v: v[0], str.upper)\n",
    "    >>> proc(['hello', 'world'])\n",
    "    'HELLO'\n",
    "\n",
    "    Each function can optionally receive a ``loader_context`` parameter. For\n",
    "    those which do, this processor will pass the currently active :ref:`Loader\n",
    "    context <loaders-context>` through that parameter.\n",
    "\n",
    "    The keyword arguments passed in the ``__init__`` method are used as the default\n",
    "    Loader context values passed to each function call. However, the final\n",
    "    Loader context values passed to functions are overridden with the currently\n",
    "    active Loader context accessible through the :attr:`ItemLoader.context\n",
    "    <itemloaders.ItemLoader.context>` attribute.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *functions, **default_loader_context):\n",
    "        self.functions = functions\n",
    "        self.stop_on_none = default_loader_context.get(\"stop_on_none\", True)\n",
    "        self.default_loader_context = default_loader_context\n",
    "\n",
    "    def __call__(self, value, loader_context=None):\n",
    "        if loader_context:\n",
    "            context = ChainMap(loader_context, self.default_loader_context)\n",
    "        else:\n",
    "            context = self.default_loader_context\n",
    "        wrapped_funcs = [wrap_loader_context(f, context) for f in self.functions]\n",
    "        for func in wrapped_funcs:\n",
    "            if value is None and self.stop_on_none:\n",
    "                break\n",
    "            try:\n",
    "                value = func(value)\n",
    "            except Exception as e:\n",
    "                raise ValueError(\n",
    "                    \"Error in Compose with \"\n",
    "                    \"%s value=%r error='%s: %s'\"\n",
    "                    % (str(func), value, type(e).__name__, str(e))\n",
    "                ) from e\n",
    "        return value\n",
    "\n",
    "\n",
    "\n",
    "class TakeFirst:\n",
    "    \"\"\"\n",
    "    Returns the first non-null/non-empty value from the values received,\n",
    "    so it's typically used as an output processor to single-valued fields.\n",
    "    It doesn't receive any ``__init__`` method arguments, nor does it accept Loader contexts.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    >>> from itemloaders.processors import TakeFirst\n",
    "    >>> proc = TakeFirst()\n",
    "    >>> proc(['', 'one', 'two', 'three'])\n",
    "    'one'\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, values):\n",
    "        for value in values:\n",
    "            if value is not None and value != \"\":\n",
    "                return value\n",
    "\n",
    "\n",
    "\n",
    "class Identity:\n",
    "    \"\"\"\n",
    "    The simplest processor, which doesn't do anything. It returns the original\n",
    "    values unchanged. It doesn't receive any ``__init__`` method arguments, nor does it\n",
    "    accept Loader contexts.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    >>> from itemloaders.processors import Identity\n",
    "    >>> proc = Identity()\n",
    "    >>> proc(['one', 'two', 'three'])\n",
    "    ['one', 'two', 'three']\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, values):\n",
    "        return values\n",
    "\n",
    "\n",
    "\n",
    "class SelectJmes:\n",
    "    \"\"\"\n",
    "    Query the input string for the jmespath (given at instantiation), and return the answer\n",
    "    Requires : jmespath(https://github.com/jmespath/jmespath)\n",
    "    Note: SelectJmes accepts only one input element at a time.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    >>> from itemloaders.processors import SelectJmes, Compose, MapCompose\n",
    "    >>> proc = SelectJmes(\"foo\") #for direct use on lists and dictionaries\n",
    "    >>> proc({'foo': 'bar'})\n",
    "    'bar'\n",
    "    >>> proc({'foo': {'bar': 'baz'}})\n",
    "    {'bar': 'baz'}\n",
    "\n",
    "    Working with Json:\n",
    "\n",
    "    >>> import json\n",
    "    >>> proc_single_json_str = Compose(json.loads, SelectJmes(\"foo\"))\n",
    "    >>> proc_single_json_str('{\"foo\": \"bar\"}')\n",
    "    'bar'\n",
    "    >>> proc_json_list = Compose(json.loads, MapCompose(SelectJmes('foo')))\n",
    "    >>> proc_json_list('[{\"foo\":\"bar\"}, {\"baz\":\"tar\"}]')\n",
    "    ['bar']\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, json_path):\n",
    "        self.json_path = json_path\n",
    "        import jmespath\n",
    "\n",
    "        self.compiled_path = jmespath.compile(self.json_path)\n",
    "\n",
    "    def __call__(self, value):\n",
    "        \"\"\"Query value for the jmespath query and return answer\n",
    "        :param value: a data structure (dict, list) to extract from\n",
    "        :return: Element extracted according to jmespath query\n",
    "        \"\"\"\n",
    "        return self.compiled_path.search(value)\n",
    "\n",
    "\n",
    "\n",
    "class Join:\n",
    "    \"\"\"\n",
    "    Returns the values joined with the separator given in the ``__init__`` method, which\n",
    "    defaults to ``' '``. It doesn't accept Loader contexts.\n",
    "\n",
    "    When using the default separator, this processor is equivalent to the\n",
    "    function: ``' '.join``\n",
    "\n",
    "    Examples:\n",
    "\n",
    "    >>> from itemloaders.processors import Join\n",
    "    >>> proc = Join()\n",
    "    >>> proc(['one', 'two', 'three'])\n",
    "    'one two three'\n",
    "    >>> proc = Join('<br>')\n",
    "    >>> proc(['one', 'two', 'three'])\n",
    "    'one<br>two<br>three'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, separator=\" \"):\n",
    "        self.separator = separator\n",
    "\n",
    "    def __call__(self, values):\n",
    "        return self.separator.join(values)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216424a9-9dd4-43fb-b742-18e12b209bc7",
   "metadata": {},
   "source": [
    "## `scrapy.http.request`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadc0572-f799-471a-a1dc-e7670f3c8ac2",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"\n",
    "This module implements the Request class which is used to represent HTTP\n",
    "requests in Scrapy.\n",
    "\n",
    "See documentation in docs/topics/request-response.rst\n",
    "\"\"\"\n",
    "import inspect\n",
    "from typing import Callable, List, Optional, Tuple, Type, TypeVar, Union\n",
    "\n",
    "from w3lib.url import safe_url_string\n",
    "\n",
    "import scrapy\n",
    "from scrapy.http.common import obsolete_setter\n",
    "from scrapy.http.headers import Headers\n",
    "from scrapy.utils.curl import curl_to_request_kwargs\n",
    "from scrapy.utils.python import to_bytes\n",
    "from scrapy.utils.trackref import object_ref\n",
    "from scrapy.utils.url import escape_ajax\n",
    "\n",
    "RequestTypeVar = TypeVar(\"RequestTypeVar\", bound=\"Request\")\n",
    "\n",
    "\n",
    "def NO_CALLBACK(*args, **kwargs):\n",
    "    \"\"\"When assigned to the ``callback`` parameter of\n",
    "    :class:`~scrapy.http.Request`, it indicates that the request is not meant\n",
    "    to have a spider callback at all.\n",
    "\n",
    "    For example:\n",
    "\n",
    "    .. code-block:: python\n",
    "\n",
    "       Request(\"https://example.com\", callback=NO_CALLBACK)\n",
    "\n",
    "    This value should be used by :ref:`components <topics-components>` that\n",
    "    create and handle their own requests, e.g. through\n",
    "    :meth:`scrapy.core.engine.ExecutionEngine.download`, so that downloader\n",
    "    middlewares handling such requests can treat them differently from requests\n",
    "    intended for the :meth:`~scrapy.Spider.parse` callback.\n",
    "    \"\"\"\n",
    "    raise RuntimeError(\n",
    "        \"The NO_CALLBACK callback has been called. This is a special callback \"\n",
    "        \"value intended for requests whose callback is never meant to be \"\n",
    "        \"called.\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "class Request(object_ref):\n",
    "    \"\"\"Represents an HTTP request, which is usually generated in a Spider and\n",
    "    executed by the Downloader, thus generating a :class:`Response`.\n",
    "    \"\"\"\n",
    "\n",
    "    attributes: Tuple[str, ...] = (\n",
    "        \"url\",\n",
    "        \"callback\",\n",
    "        \"method\",\n",
    "        \"headers\",\n",
    "        \"body\",\n",
    "        \"cookies\",\n",
    "        \"meta\",\n",
    "        \"encoding\",\n",
    "        \"priority\",\n",
    "        \"dont_filter\",\n",
    "        \"errback\",\n",
    "        \"flags\",\n",
    "        \"cb_kwargs\",\n",
    "    )\n",
    "    \"\"\"A tuple of :class:`str` objects containing the name of all public\n",
    "    attributes of the class that are also keyword parameters of the\n",
    "    ``__init__`` method.\n",
    "\n",
    "    Currently used by :meth:`Request.replace`, :meth:`Request.to_dict` and\n",
    "    :func:`~scrapy.utils.request.request_from_dict`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        url: str,\n",
    "        callback: Optional[Callable] = None,\n",
    "        method: str = \"GET\",\n",
    "        headers: Optional[dict] = None,\n",
    "        body: Optional[Union[bytes, str]] = None,\n",
    "        cookies: Optional[Union[dict, List[dict]]] = None,\n",
    "        meta: Optional[dict] = None,\n",
    "        encoding: str = \"utf-8\",\n",
    "        priority: int = 0,\n",
    "        dont_filter: bool = False,\n",
    "        errback: Optional[Callable] = None,\n",
    "        flags: Optional[List[str]] = None,\n",
    "        cb_kwargs: Optional[dict] = None,\n",
    "    ) -> None:\n",
    "        self._encoding = encoding  # this one has to be set first\n",
    "        self.method = str(method).upper()\n",
    "        self._set_url(url)\n",
    "        self._set_body(body)\n",
    "        if not isinstance(priority, int):\n",
    "            raise TypeError(f\"Request priority not an integer: {priority!r}\")\n",
    "        self.priority = priority\n",
    "\n",
    "        if not (callable(callback) or callback is None):\n",
    "            raise TypeError(\n",
    "                f\"callback must be a callable, got {type(callback).__name__}\"\n",
    "            )\n",
    "        if not (callable(errback) or errback is None):\n",
    "            raise TypeError(f\"errback must be a callable, got {type(errback).__name__}\")\n",
    "        self.callback = callback\n",
    "        self.errback = errback\n",
    "\n",
    "        self.cookies = cookies or {}\n",
    "        self.headers = Headers(headers or {}, encoding=encoding)\n",
    "        self.dont_filter = dont_filter\n",
    "\n",
    "        self._meta = dict(meta) if meta else None\n",
    "        self._cb_kwargs = dict(cb_kwargs) if cb_kwargs else None\n",
    "        self.flags = [] if flags is None else list(flags)\n",
    "\n",
    "    @property\n",
    "    def cb_kwargs(self) -> dict:\n",
    "        if self._cb_kwargs is None:\n",
    "            self._cb_kwargs = {}\n",
    "        return self._cb_kwargs\n",
    "\n",
    "    @property\n",
    "    def meta(self) -> dict:\n",
    "        if self._meta is None:\n",
    "            self._meta = {}\n",
    "        return self._meta\n",
    "\n",
    "    def _get_url(self) -> str:\n",
    "        return self._url\n",
    "\n",
    "    def _set_url(self, url: str) -> None:\n",
    "        if not isinstance(url, str):\n",
    "            raise TypeError(f\"Request url must be str, got {type(url).__name__}\")\n",
    "\n",
    "        s = safe_url_string(url, self.encoding)\n",
    "        self._url = escape_ajax(s)\n",
    "\n",
    "        if (\n",
    "            \"://\" not in self._url\n",
    "            and not self._url.startswith(\"about:\")\n",
    "            and not self._url.startswith(\"data:\")\n",
    "        ):\n",
    "            raise ValueError(f\"Missing scheme in request url: {self._url}\")\n",
    "\n",
    "    url = property(_get_url, obsolete_setter(_set_url, \"url\"))\n",
    "\n",
    "    def _get_body(self) -> bytes:\n",
    "        return self._body\n",
    "\n",
    "    def _set_body(self, body: Optional[Union[str, bytes]]) -> None:\n",
    "        self._body = b\"\" if body is None else to_bytes(body, self.encoding)\n",
    "\n",
    "    body = property(_get_body, obsolete_setter(_set_body, \"body\"))\n",
    "\n",
    "    @property\n",
    "    def encoding(self) -> str:\n",
    "        return self._encoding\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"<{self.method} {self.url}>\"\n",
    "\n",
    "    def copy(self) -> \"Request\":\n",
    "        return self.replace()\n",
    "\n",
    "\n",
    "    def replace(self, *args, **kwargs) -> \"Request\":\n",
    "        \"\"\"Create a new Request with the same attributes except for those given new values\"\"\"\n",
    "        for x in self.attributes:\n",
    "            kwargs.setdefault(x, getattr(self, x))\n",
    "        cls = kwargs.pop(\"cls\", self.__class__)\n",
    "        return cls(*args, **kwargs)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_curl(\n",
    "        cls: Type[RequestTypeVar],\n",
    "        curl_command: str,\n",
    "        ignore_unknown_options: bool = True,\n",
    "        **kwargs,\n",
    "    ) -> RequestTypeVar:\n",
    "        \"\"\"Create a Request object from a string containing a `cURL\n",
    "        <https://curl.haxx.se/>`_ command. It populates the HTTP method, the\n",
    "        URL, the headers, the cookies and the body. It accepts the same\n",
    "        arguments as the :class:`Request` class, taking preference and\n",
    "        overriding the values of the same arguments contained in the cURL\n",
    "        command.\n",
    "\n",
    "        Unrecognized options are ignored by default. To raise an error when\n",
    "        finding unknown options call this method by passing\n",
    "        ``ignore_unknown_options=False``.\n",
    "\n",
    "        .. caution:: Using :meth:`from_curl` from :class:`~scrapy.http.Request`\n",
    "                     subclasses, such as :class:`~scrapy.http.JSONRequest`, or\n",
    "                     :class:`~scrapy.http.XmlRpcRequest`, as well as having\n",
    "                     :ref:`downloader middlewares <topics-downloader-middleware>`\n",
    "                     and\n",
    "                     :ref:`spider middlewares <topics-spider-middleware>`\n",
    "                     enabled, such as\n",
    "                     :class:`~scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware`,\n",
    "                     :class:`~scrapy.downloadermiddlewares.useragent.UserAgentMiddleware`,\n",
    "                     or\n",
    "                     :class:`~scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware`,\n",
    "                     may modify the :class:`~scrapy.http.Request` object.\n",
    "\n",
    "        To translate a cURL command into a Scrapy request,\n",
    "        you may use `curl2scrapy <https://michael-shub.github.io/curl2scrapy/>`_.\n",
    "        \"\"\"\n",
    "        request_kwargs = curl_to_request_kwargs(curl_command, ignore_unknown_options)\n",
    "        request_kwargs.update(kwargs)\n",
    "        return cls(**request_kwargs)\n",
    "\n",
    "\n",
    "    def to_dict(self, *, spider: Optional[\"scrapy.Spider\"] = None) -> dict:\n",
    "        \"\"\"Return a dictionary containing the Request's data.\n",
    "\n",
    "        Use :func:`~scrapy.utils.request.request_from_dict` to convert back into a :class:`~scrapy.Request` object.\n",
    "\n",
    "        If a spider is given, this method will try to find out the name of the spider methods used as callback\n",
    "        and errback and include them in the output dict, raising an exception if they cannot be found.\n",
    "        \"\"\"\n",
    "        d = {\n",
    "            \"url\": self.url,  # urls are safe (safe_string_url)\n",
    "            \"callback\": _find_method(spider, self.callback)\n",
    "            if callable(self.callback)\n",
    "            else self.callback,\n",
    "            \"errback\": _find_method(spider, self.errback)\n",
    "            if callable(self.errback)\n",
    "            else self.errback,\n",
    "            \"headers\": dict(self.headers),\n",
    "        }\n",
    "        for attr in self.attributes:\n",
    "            d.setdefault(attr, getattr(self, attr))\n",
    "        if type(self) is not Request:  # pylint: disable=unidiomatic-typecheck\n",
    "            d[\"_class\"] = self.__module__ + \".\" + self.__class__.__name__\n",
    "        return d\n",
    "\n",
    "\n",
    "\n",
    "def _find_method(obj, func):\n",
    "    \"\"\"Helper function for Request.to_dict\"\"\"\n",
    "    # Only instance methods contain ``__func__``\n",
    "    if obj and hasattr(func, \"__func__\"):\n",
    "        members = inspect.getmembers(obj, predicate=inspect.ismethod)\n",
    "        for name, obj_func in members:\n",
    "            # We need to use __func__ to access the original function object because instance\n",
    "            # method objects are generated each time attribute is retrieved from instance.\n",
    "            #\n",
    "            # Reference: The standard type hierarchy\n",
    "            # https://docs.python.org/3/reference/datamodel.html\n",
    "            if obj_func.__func__ is func.__func__:\n",
    "                return name\n",
    "    raise ValueError(f\"Function {func} is not an instance method in: {obj}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414c2fa9-83ef-4dfb-abc1-74c881edfa01",
   "metadata": {},
   "source": [
    "## `scrapy.linkextractors.lxmlhtml`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20a108c-7fc2-40c7-879c-329010edd376",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"\n",
    "Link extractor based on lxml.html\n",
    "\"\"\"\n",
    "import logging\n",
    "import operator\n",
    "from functools import partial\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "from lxml import etree\n",
    "from parsel.csstranslator import HTMLTranslator\n",
    "from w3lib.html import strip_html5_whitespace\n",
    "from w3lib.url import canonicalize_url, safe_url_string\n",
    "\n",
    "from scrapy.link import Link\n",
    "from scrapy.linkextractors import (\n",
    "    IGNORED_EXTENSIONS,\n",
    "    _is_valid_url,\n",
    "    _matches,\n",
    "    _re_type,\n",
    "    re,\n",
    ")\n",
    "from scrapy.utils.misc import arg_to_iter, rel_has_nofollow\n",
    "from scrapy.utils.python import unique as unique_list\n",
    "from scrapy.utils.response import get_base_url\n",
    "from scrapy.utils.url import url_has_any_extension, url_is_from_any_domain\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# from lxml/src/lxml/html/__init__.py\n",
    "XHTML_NAMESPACE = \"http://www.w3.org/1999/xhtml\"\n",
    "\n",
    "_collect_string_content = etree.XPath(\"string()\")\n",
    "\n",
    "\n",
    "def _nons(tag):\n",
    "    if isinstance(tag, str):\n",
    "        if tag[0] == \"{\" and tag[1 : len(XHTML_NAMESPACE) + 1] == XHTML_NAMESPACE:\n",
    "            return tag.split(\"}\")[-1]\n",
    "    return tag\n",
    "\n",
    "\n",
    "def _identity(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def _canonicalize_link_url(link):\n",
    "    return canonicalize_url(link.url, keep_fragments=True)\n",
    "\n",
    "\n",
    "class LxmlParserLinkExtractor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tag=\"a\",\n",
    "        attr=\"href\",\n",
    "        process=None,\n",
    "        unique=False,\n",
    "        strip=True,\n",
    "        canonicalized=False,\n",
    "    ):\n",
    "        self.scan_tag = tag if callable(tag) else partial(operator.eq, tag)\n",
    "        self.scan_attr = attr if callable(attr) else partial(operator.eq, attr)\n",
    "        self.process_attr = process if callable(process) else _identity\n",
    "        self.unique = unique\n",
    "        self.strip = strip\n",
    "        self.link_key = (\n",
    "            operator.attrgetter(\"url\") if canonicalized else _canonicalize_link_url\n",
    "        )\n",
    "\n",
    "    def _iter_links(self, document):\n",
    "        for el in document.iter(etree.Element):\n",
    "            if not self.scan_tag(_nons(el.tag)):\n",
    "                continue\n",
    "            attribs = el.attrib\n",
    "            for attrib in attribs:\n",
    "                if not self.scan_attr(attrib):\n",
    "                    continue\n",
    "                yield (el, attrib, attribs[attrib])\n",
    "\n",
    "    def _extract_links(self, selector, response_url, response_encoding, base_url):\n",
    "        links = []\n",
    "        # hacky way to get the underlying lxml parsed document\n",
    "        for el, attr, attr_val in self._iter_links(selector.root):\n",
    "            # pseudo lxml.html.HtmlElement.make_links_absolute(base_url)\n",
    "            try:\n",
    "                if self.strip:\n",
    "                    attr_val = strip_html5_whitespace(attr_val)\n",
    "                attr_val = urljoin(base_url, attr_val)\n",
    "            except ValueError:\n",
    "                continue  # skipping bogus links\n",
    "            else:\n",
    "                url = self.process_attr(attr_val)\n",
    "                if url is None:\n",
    "                    continue\n",
    "            try:\n",
    "                url = safe_url_string(url, encoding=response_encoding)\n",
    "            except ValueError:\n",
    "                logger.debug(f\"Skipping extraction of link with bad URL {url!r}\")\n",
    "                continue\n",
    "\n",
    "            # to fix relative links after process_value\n",
    "            url = urljoin(response_url, url)\n",
    "            link = Link(\n",
    "                url,\n",
    "                _collect_string_content(el) or \"\",\n",
    "                nofollow=rel_has_nofollow(el.get(\"rel\")),\n",
    "            )\n",
    "            links.append(link)\n",
    "        return self._deduplicate_if_needed(links)\n",
    "\n",
    "    def extract_links(self, response):\n",
    "        base_url = get_base_url(response)\n",
    "        return self._extract_links(\n",
    "            response.selector, response.url, response.encoding, base_url\n",
    "        )\n",
    "\n",
    "    def _process_links(self, links):\n",
    "        \"\"\"Normalize and filter extracted links\n",
    "\n",
    "        The subclass should override it if necessary\n",
    "        \"\"\"\n",
    "        return self._deduplicate_if_needed(links)\n",
    "\n",
    "    def _deduplicate_if_needed(self, links):\n",
    "        if self.unique:\n",
    "            return unique_list(links, key=self.link_key)\n",
    "        return links\n",
    "\n",
    "\n",
    "[docs]class LxmlLinkExtractor:\n",
    "    _csstranslator = HTMLTranslator()\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        allow=(),\n",
    "        deny=(),\n",
    "        allow_domains=(),\n",
    "        deny_domains=(),\n",
    "        restrict_xpaths=(),\n",
    "        tags=(\"a\", \"area\"),\n",
    "        attrs=(\"href\",),\n",
    "        canonicalize=False,\n",
    "        unique=True,\n",
    "        process_value=None,\n",
    "        deny_extensions=None,\n",
    "        restrict_css=(),\n",
    "        strip=True,\n",
    "        restrict_text=None,\n",
    "    ):\n",
    "        tags, attrs = set(arg_to_iter(tags)), set(arg_to_iter(attrs))\n",
    "        self.link_extractor = LxmlParserLinkExtractor(\n",
    "            tag=partial(operator.contains, tags),\n",
    "            attr=partial(operator.contains, attrs),\n",
    "            unique=unique,\n",
    "            process=process_value,\n",
    "            strip=strip,\n",
    "            canonicalized=canonicalize,\n",
    "        )\n",
    "        self.allow_res = [\n",
    "            x if isinstance(x, _re_type) else re.compile(x) for x in arg_to_iter(allow)\n",
    "        ]\n",
    "        self.deny_res = [\n",
    "            x if isinstance(x, _re_type) else re.compile(x) for x in arg_to_iter(deny)\n",
    "        ]\n",
    "\n",
    "        self.allow_domains = set(arg_to_iter(allow_domains))\n",
    "        self.deny_domains = set(arg_to_iter(deny_domains))\n",
    "\n",
    "        self.restrict_xpaths = tuple(arg_to_iter(restrict_xpaths))\n",
    "        self.restrict_xpaths += tuple(\n",
    "            map(self._csstranslator.css_to_xpath, arg_to_iter(restrict_css))\n",
    "        )\n",
    "\n",
    "        if deny_extensions is None:\n",
    "            deny_extensions = IGNORED_EXTENSIONS\n",
    "        self.canonicalize = canonicalize\n",
    "        self.deny_extensions = {\".\" + e for e in arg_to_iter(deny_extensions)}\n",
    "        self.restrict_text = [\n",
    "            x if isinstance(x, _re_type) else re.compile(x)\n",
    "            for x in arg_to_iter(restrict_text)\n",
    "        ]\n",
    "\n",
    "    def _link_allowed(self, link):\n",
    "        if not _is_valid_url(link.url):\n",
    "            return False\n",
    "        if self.allow_res and not _matches(link.url, self.allow_res):\n",
    "            return False\n",
    "        if self.deny_res and _matches(link.url, self.deny_res):\n",
    "            return False\n",
    "        parsed_url = urlparse(link.url)\n",
    "        if self.allow_domains and not url_is_from_any_domain(\n",
    "            parsed_url, self.allow_domains\n",
    "        ):\n",
    "            return False\n",
    "        if self.deny_domains and url_is_from_any_domain(parsed_url, self.deny_domains):\n",
    "            return False\n",
    "        if self.deny_extensions and url_has_any_extension(\n",
    "            parsed_url, self.deny_extensions\n",
    "        ):\n",
    "            return False\n",
    "        if self.restrict_text and not _matches(link.text, self.restrict_text):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def matches(self, url):\n",
    "        if self.allow_domains and not url_is_from_any_domain(url, self.allow_domains):\n",
    "            return False\n",
    "        if self.deny_domains and url_is_from_any_domain(url, self.deny_domains):\n",
    "            return False\n",
    "\n",
    "        allowed = (\n",
    "            (regex.search(url) for regex in self.allow_res)\n",
    "            if self.allow_res\n",
    "            else [True]\n",
    "        )\n",
    "        denied = (regex.search(url) for regex in self.deny_res) if self.deny_res else []\n",
    "        return any(allowed) and not any(denied)\n",
    "\n",
    "    def _process_links(self, links):\n",
    "        links = [x for x in links if self._link_allowed(x)]\n",
    "        if self.canonicalize:\n",
    "            for link in links:\n",
    "                link.url = canonicalize_url(link.url)\n",
    "        links = self.link_extractor._process_links(links)\n",
    "        return links\n",
    "\n",
    "    def _extract_links(self, *args, **kwargs):\n",
    "        return self.link_extractor._extract_links(*args, **kwargs)\n",
    "\n",
    "[docs]    def extract_links(self, response):\n",
    "        \"\"\"Returns a list of :class:`~scrapy.link.Link` objects from the\n",
    "        specified :class:`response <scrapy.http.Response>`.\n",
    "\n",
    "        Only links that match the settings passed to the ``__init__`` method of\n",
    "        the link extractor are returned.\n",
    "\n",
    "        Duplicate links are omitted if the ``unique`` attribute is set to ``True``,\n",
    "        otherwise they are returned.\n",
    "        \"\"\"\n",
    "        base_url = get_base_url(response)\n",
    "        if self.restrict_xpaths:\n",
    "            docs = [\n",
    "                subdoc for x in self.restrict_xpaths for subdoc in response.xpath(x)\n",
    "            ]\n",
    "        else:\n",
    "            docs = [response.selector]\n",
    "        all_links = []\n",
    "        for doc in docs:\n",
    "            links = self._extract_links(doc, response.url, response.encoding, base_url)\n",
    "            all_links.extend(self._process_links(links))\n",
    "        if self.link_extractor.unique:\n",
    "            return unique_list(all_links)\n",
    "        return all_links\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e0c62f-a418-4236-92de-9b15b144a445",
   "metadata": {},
   "source": [
    "## `scrapy.link`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fba1ab-b2f5-4818-910b-0392fc1cd818",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"\n",
    "This module defines the Link object used in Link extractors.\n",
    "\n",
    "For actual link extractors implementation see scrapy.linkextractors, or\n",
    "its documentation in: docs/topics/link-extractors.rst\n",
    "\"\"\"\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "[docs]class Link:\n",
    "    \"\"\"Link objects represent an extracted link by the LinkExtractor.\n",
    "\n",
    "    Using the anchor tag sample below to illustrate the parameters::\n",
    "\n",
    "            <a href=\"https://example.com/nofollow.html#foo\" rel=\"nofollow\">Dont follow this one</a>\n",
    "\n",
    "    :param url: the absolute url being linked to in the anchor tag.\n",
    "                From the sample, this is ``https://example.com/nofollow.html``.\n",
    "\n",
    "    :param text: the text in the anchor tag. From the sample, this is ``Dont follow this one``.\n",
    "\n",
    "    :param fragment: the part of the url after the hash symbol. From the sample, this is ``foo``.\n",
    "\n",
    "    :param nofollow: an indication of the presence or absence of a nofollow value in the ``rel`` attribute\n",
    "                    of the anchor tag.\n",
    "    \"\"\"\n",
    "\n",
    "    __slots__ = [\"url\", \"text\", \"fragment\", \"nofollow\"]\n",
    "\n",
    "    def __init__(\n",
    "        self, url: str, text: str = \"\", fragment: str = \"\", nofollow: bool = False\n",
    "    ):\n",
    "        if not isinstance(url, str):\n",
    "            got = url.__class__.__name__\n",
    "            raise TypeError(f\"Link urls must be str objects, got {got}\")\n",
    "        self.url: str = url\n",
    "        self.text: str = text\n",
    "        self.fragment: str = fragment\n",
    "        self.nofollow: bool = nofollow\n",
    "\n",
    "    def __eq__(self, other: Any) -> bool:\n",
    "        if not isinstance(other, Link):\n",
    "            raise NotImplementedError\n",
    "        return (\n",
    "            self.url == other.url\n",
    "            and self.text == other.text\n",
    "            and self.fragment == other.fragment\n",
    "            and self.nofollow == other.nofollow\n",
    "        )\n",
    "\n",
    "    def __hash__(self) -> int:\n",
    "        return (\n",
    "            hash(self.url) ^ hash(self.text) ^ hash(self.fragment) ^ hash(self.nofollow)\n",
    "        )\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (\n",
    "            f\"Link(url={self.url!r}, text={self.text!r}, \"\n",
    "            f\"fragment={self.fragment!r}, nofollow={self.nofollow!r})\"\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb881491-fd84-4b29-a5fb-fee985ad96bb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e936ef46-109c-4de9-9506-c0733572634d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d124c33-d47c-47a9-a074-591c7b848819",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7972708a-47b8-452b-9392-ea12104b0c55",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "298337ff-5d60-4495-be9a-b076eabe85b8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "973eb397-17f7-4766-a31e-8b328f1660f4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c618290-1313-4223-8574-a1257864c22c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcd9cde4-b0a9-47ce-8e1a-566e584f4b3c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18cbaacd-c56b-4df4-a8d1-c03f5b5fa05f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b3d7b5c-ceb7-41e6-a129-3162c4c8d68a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a0f813b-33a9-4044-95e2-d780bf967ee0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "677b6177-7e5f-446e-bf72-96ad5ae717ca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cad7ed3c-25ac-476b-bbcd-6668807863ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20dbbaa1-1768-4adb-a85f-4d08269fa1d8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ed00abc-dbf8-42f3-899b-a0d27bb0266f",
   "metadata": {},
   "source": [
    "# <b>(FORGET) Additional</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a179bc2-3bcc-4353-8412-5bf253a7bb25",
   "metadata": {},
   "source": [
    "|bash|description|\n",
    "|-|-|\n",
    "|`scrapy startproject <name>`|start a new scrapy project|\n",
    "|`scrapy genspider <spider_name> <domain>`|generate a spider in the `spider` dir|\n",
    "|`scrapy runspider <spider_file>.py`|start the crawler|\n",
    "|||\n",
    "|||\n",
    "|||"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54b00cd-908f-4697-bd45-4210bd92753f",
   "metadata": {},
   "source": [
    "# 3. Creating a Scrapy project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c0b41c-7421-492e-bd84-2de27858413c",
   "metadata": {},
   "source": [
    "You should work in the virtual environment.\n",
    "\n",
    "```sh\n",
    "pip install --upgrade pip\n",
    "pip install scrapy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bddfd4-0ed5-410c-8203-5edfe65075c5",
   "metadata": {},
   "source": [
    "A **spider** is a Scrapy project that, like its arachnid namesake, is designed to crawl webs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56816e0-5467-4284-bbf3-ec9c5ae29d4b",
   "metadata": {},
   "source": [
    "```sh\n",
    "$ scrapy startproject test1\n",
    "```\n",
    "```\n",
    "New Scrapy project 'test1', using template directory '/home/commi/venv/venv3.12/lib/python3.12/site-packages/scrapy/templates/project', created in:\n",
    "    /home/commi/Yandex.Disk/it_learning/08_parsing_data/data/test1\n",
    "\n",
    "You can start your first spider with:\n",
    "    cd test1\n",
    "    scrapy genspider example example.com\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ce26a-c0cf-48bb-973d-9733f527a6dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Project dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ecea08b8-a347-44ee-b752-d7f9c0e8a704",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:30:54.703573Z",
     "iopub.status.busy": "2024-01-15T13:30:54.703002Z",
     "iopub.status.idle": "2024-01-15T13:30:54.808659Z",
     "shell.execute_reply": "2024-01-15T13:30:54.807658Z",
     "shell.execute_reply.started": "2024-01-15T13:30:54.703536Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cd /home/commi/Yandex.Disk/it_learning/08_parsing_data/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "379af829-3fb8-4e35-a5e7-d22faf074e50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:30:55.263463Z",
     "iopub.status.busy": "2024-01-15T13:30:55.262717Z",
     "iopub.status.idle": "2024-01-15T13:30:55.381147Z",
     "shell.execute_reply": "2024-01-15T13:30:55.380041Z",
     "shell.execute_reply.started": "2024-01-15T13:30:55.263422Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtest1\u001b[0m\n",
      "├── scrapy.cfg\n",
      "└── \u001b[01;34mtest1\u001b[0m\n",
      "    ├── __init__.py\n",
      "    ├── items.py\n",
      "    ├── middlewares.py\n",
      "    ├── pipelines.py\n",
      "    ├── settings.py\n",
      "    └── \u001b[01;34mspiders\u001b[0m\n",
      "        └── __init__.py\n",
      "\n",
      "3 directories, 7 files\n"
     ]
    }
   ],
   "source": [
    "tree test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d98d0f98-06b3-4f4d-b26a-4416c914f619",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:30:57.259154Z",
     "iopub.status.busy": "2024-01-15T13:30:57.258366Z",
     "iopub.status.idle": "2024-01-15T13:30:57.408565Z",
     "shell.execute_reply": "2024-01-15T13:30:57.407181Z",
     "shell.execute_reply.started": "2024-01-15T13:30:57.259088Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Automatically created by: scrapy startproject\n",
      "#\n",
      "# For more information about the [deploy] section see:\n",
      "# https://scrapyd.readthedocs.io/en/latest/deploy.html\n",
      "\n",
      "[settings]\n",
      "default = test1.settings\n",
      "\n",
      "[deploy]\n",
      "#url = http://localhost:6800/\n",
      "project = test1\n"
     ]
    }
   ],
   "source": [
    "cat test1/scrapy.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302e9893-67d6-45dd-9d7e-27fc53d0e030",
   "metadata": {},
   "source": [
    "### Deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "13f95372-ca7a-4e93-89b2-d204b8247c76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:30:59.654728Z",
     "iopub.status.busy": "2024-01-15T13:30:59.653435Z",
     "iopub.status.idle": "2024-01-15T13:30:59.765945Z",
     "shell.execute_reply": "2024-01-15T13:30:59.764669Z",
     "shell.execute_reply.started": "2024-01-15T13:30:59.654676Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtest1/test1\u001b[0m\n",
      "├── __init__.py\n",
      "├── items.py\n",
      "├── middlewares.py\n",
      "├── pipelines.py\n",
      "├── settings.py\n",
      "└── \u001b[01;34mspiders\u001b[0m\n",
      "    └── __init__.py\n",
      "\n",
      "2 directories, 6 files\n"
     ]
    }
   ],
   "source": [
    "tree test1/test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f60b913-9ac9-4002-9848-2583c61b947c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:31:02.723037Z",
     "iopub.status.busy": "2024-01-15T13:31:02.722598Z",
     "iopub.status.idle": "2024-01-15T13:31:02.836435Z",
     "shell.execute_reply": "2024-01-15T13:31:02.835624Z",
     "shell.execute_reply.started": "2024-01-15T13:31:02.723003Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Define here the models for your scraped items\n",
      "#\n",
      "# See documentation in:\n",
      "# https://docs.scrapy.org/en/latest/topics/items.html\n",
      "\n",
      "import scrapy\n",
      "\n",
      "\n",
      "class Test1Item(scrapy.Item):\n",
      "    # define the fields for your item here like:\n",
      "    # name = scrapy.Field()\n",
      "    pass\n"
     ]
    }
   ],
   "source": [
    "cat test1/test1/items.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d8f0246-7466-4db3-9666-f499c65503dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:31:05.807978Z",
     "iopub.status.busy": "2024-01-15T13:31:05.807297Z",
     "iopub.status.idle": "2024-01-15T13:31:06.006539Z",
     "shell.execute_reply": "2024-01-15T13:31:06.005002Z",
     "shell.execute_reply.started": "2024-01-15T13:31:05.807945Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Define here the models for your spider middleware\n",
      "#\n",
      "# See documentation in:\n",
      "# https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
      "\n",
      "from scrapy import signals\n",
      "\n",
      "# useful for handling different item types with a single interface\n",
      "from itemadapter import is_item, ItemAdapter\n",
      "\n",
      "\n",
      "class Test1SpiderMiddleware:\n",
      "    # Not all methods need to be defined. If a method is not defined,\n",
      "    # scrapy acts as if the spider middleware does not modify the\n",
      "    # passed objects.\n",
      "\n",
      "    @classmethod\n",
      "    def from_crawler(cls, crawler):\n",
      "        # This method is used by Scrapy to create your spiders.\n",
      "        s = cls()\n",
      "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
      "        return s\n",
      "\n",
      "    def process_spider_input(self, response, spider):\n",
      "        # Called for each response that goes through the spider\n",
      "        # middleware and into the spider.\n",
      "\n",
      "        # Should return None or raise an exception.\n",
      "        return None\n",
      "\n",
      "    def process_spider_output(self, response, result, spider):\n",
      "        # Called with the results returned from the Spider, after\n",
      "        # it has processed the response.\n",
      "\n",
      "        # Must return an iterable of Request, or item objects.\n",
      "        for i in result:\n",
      "            yield i\n",
      "\n",
      "    def process_spider_exception(self, response, exception, spider):\n",
      "        # Called when a spider or process_spider_input() method\n",
      "        # (from other spider middleware) raises an exception.\n",
      "\n",
      "        # Should return either None or an iterable of Request or item objects.\n",
      "        pass\n",
      "\n",
      "    def process_start_requests(self, start_requests, spider):\n",
      "        # Called with the start requests of the spider, and works\n",
      "        # similarly to the process_spider_output() method, except\n",
      "        # that it doesn’t have a response associated.\n",
      "\n",
      "        # Must return only requests (not items).\n",
      "        for r in start_requests:\n",
      "            yield r\n",
      "\n",
      "    def spider_opened(self, spider):\n",
      "        spider.logger.info(\"Spider opened: %s\" % spider.name)\n",
      "\n",
      "\n",
      "class Test1DownloaderMiddleware:\n",
      "    # Not all methods need to be defined. If a method is not defined,\n",
      "    # scrapy acts as if the downloader middleware does not modify the\n",
      "    # passed objects.\n",
      "\n",
      "    @classmethod\n",
      "    def from_crawler(cls, crawler):\n",
      "        # This method is used by Scrapy to create your spiders.\n",
      "        s = cls()\n",
      "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
      "        return s\n",
      "\n",
      "    def process_request(self, request, spider):\n",
      "        # Called for each request that goes through the downloader\n",
      "        # middleware.\n",
      "\n",
      "        # Must either:\n",
      "        # - return None: continue processing this request\n",
      "        # - or return a Response object\n",
      "        # - or return a Request object\n",
      "        # - or raise IgnoreRequest: process_exception() methods of\n",
      "        #   installed downloader middleware will be called\n",
      "        return None\n",
      "\n",
      "    def process_response(self, request, response, spider):\n",
      "        # Called with the response returned from the downloader.\n",
      "\n",
      "        # Must either;\n",
      "        # - return a Response object\n",
      "        # - return a Request object\n",
      "        # - or raise IgnoreRequest\n",
      "        return response\n",
      "\n",
      "    def process_exception(self, request, exception, spider):\n",
      "        # Called when a download handler or a process_request()\n",
      "        # (from other downloader middleware) raises an exception.\n",
      "\n",
      "        # Must either:\n",
      "        # - return None: continue processing this exception\n",
      "        # - return a Response object: stops process_exception() chain\n",
      "        # - return a Request object: stops process_exception() chain\n",
      "        pass\n",
      "\n",
      "    def spider_opened(self, spider):\n",
      "        spider.logger.info(\"Spider opened: %s\" % spider.name)\n"
     ]
    }
   ],
   "source": [
    "cat test1/test1/middlewares.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70eff45e-8748-48a5-a3d3-0f29e86cddc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:31:08.926648Z",
     "iopub.status.busy": "2024-01-15T13:31:08.925824Z",
     "iopub.status.idle": "2024-01-15T13:31:09.038088Z",
     "shell.execute_reply": "2024-01-15T13:31:09.036692Z",
     "shell.execute_reply.started": "2024-01-15T13:31:08.926579Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Define your item pipelines here\n",
      "#\n",
      "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
      "# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
      "\n",
      "\n",
      "# useful for handling different item types with a single interface\n",
      "from itemadapter import ItemAdapter\n",
      "\n",
      "\n",
      "class Test1Pipeline:\n",
      "    def process_item(self, item, spider):\n",
      "        return item\n"
     ]
    }
   ],
   "source": [
    "cat test1/test1/pipelines.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f5d4940-928f-431c-a4d0-db14cf71b459",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:31:09.533481Z",
     "iopub.status.busy": "2024-01-15T13:31:09.532629Z",
     "iopub.status.idle": "2024-01-15T13:31:09.721049Z",
     "shell.execute_reply": "2024-01-15T13:31:09.719769Z",
     "shell.execute_reply.started": "2024-01-15T13:31:09.533446Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Scrapy settings for test1 project\n",
      "#\n",
      "# For simplicity, this file contains only settings considered important or\n",
      "# commonly used. You can find more settings consulting the documentation:\n",
      "#\n",
      "#     https://docs.scrapy.org/en/latest/topics/settings.html\n",
      "#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
      "#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
      "\n",
      "BOT_NAME = \"test1\"\n",
      "\n",
      "SPIDER_MODULES = [\"test1.spiders\"]\n",
      "NEWSPIDER_MODULE = \"test1.spiders\"\n",
      "\n",
      "\n",
      "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
      "#USER_AGENT = \"test1 (+http://www.yourdomain.com)\"\n",
      "\n",
      "# Obey robots.txt rules\n",
      "ROBOTSTXT_OBEY = True\n",
      "\n",
      "# Configure maximum concurrent requests performed by Scrapy (default: 16)\n",
      "#CONCURRENT_REQUESTS = 32\n",
      "\n",
      "# Configure a delay for requests for the same website (default: 0)\n",
      "# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n",
      "# See also autothrottle settings and docs\n",
      "#DOWNLOAD_DELAY = 3\n",
      "# The download delay setting will honor only one of:\n",
      "#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n",
      "#CONCURRENT_REQUESTS_PER_IP = 16\n",
      "\n",
      "# Disable cookies (enabled by default)\n",
      "#COOKIES_ENABLED = False\n",
      "\n",
      "# Disable Telnet Console (enabled by default)\n",
      "#TELNETCONSOLE_ENABLED = False\n",
      "\n",
      "# Override the default request headers:\n",
      "#DEFAULT_REQUEST_HEADERS = {\n",
      "#    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
      "#    \"Accept-Language\": \"en\",\n",
      "#}\n",
      "\n",
      "# Enable or disable spider middlewares\n",
      "# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
      "#SPIDER_MIDDLEWARES = {\n",
      "#    \"test1.middlewares.Test1SpiderMiddleware\": 543,\n",
      "#}\n",
      "\n",
      "# Enable or disable downloader middlewares\n",
      "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
      "#DOWNLOADER_MIDDLEWARES = {\n",
      "#    \"test1.middlewares.Test1DownloaderMiddleware\": 543,\n",
      "#}\n",
      "\n",
      "# Enable or disable extensions\n",
      "# See https://docs.scrapy.org/en/latest/topics/extensions.html\n",
      "#EXTENSIONS = {\n",
      "#    \"scrapy.extensions.telnet.TelnetConsole\": None,\n",
      "#}\n",
      "\n",
      "# Configure item pipelines\n",
      "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
      "#ITEM_PIPELINES = {\n",
      "#    \"test1.pipelines.Test1Pipeline\": 300,\n",
      "#}\n",
      "\n",
      "# Enable and configure the AutoThrottle extension (disabled by default)\n",
      "# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n",
      "#AUTOTHROTTLE_ENABLED = True\n",
      "# The initial download delay\n",
      "#AUTOTHROTTLE_START_DELAY = 5\n",
      "# The maximum download delay to be set in case of high latencies\n",
      "#AUTOTHROTTLE_MAX_DELAY = 60\n",
      "# The average number of requests Scrapy should be sending in parallel to\n",
      "# each remote server\n",
      "#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n",
      "# Enable showing throttling stats for every response received:\n",
      "#AUTOTHROTTLE_DEBUG = False\n",
      "\n",
      "# Enable and configure HTTP caching (disabled by default)\n",
      "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n",
      "#HTTPCACHE_ENABLED = True\n",
      "#HTTPCACHE_EXPIRATION_SECS = 0\n",
      "#HTTPCACHE_DIR = \"httpcache\"\n",
      "#HTTPCACHE_IGNORE_HTTP_CODES = []\n",
      "#HTTPCACHE_STORAGE = \"scrapy.extensions.httpcache.FilesystemCacheStorage\"\n",
      "\n",
      "# Set settings whose default value is deprecated to a future-proof value\n",
      "REQUEST_FINGERPRINTER_IMPLEMENTATION = \"2.7\"\n",
      "TWISTED_REACTOR = \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n",
      "FEED_EXPORT_ENCODING = \"utf-8\"\n"
     ]
    }
   ],
   "source": [
    "cat test1/test1/settings.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9bec3b-c7d0-4785-865c-9324c62a282b",
   "metadata": {},
   "source": [
    "### Even deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "376e12d3-10e9-4912-9fb0-17dc2587138e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:31:15.439231Z",
     "iopub.status.busy": "2024-01-15T13:31:15.438518Z",
     "iopub.status.idle": "2024-01-15T13:31:15.547558Z",
     "shell.execute_reply": "2024-01-15T13:31:15.546528Z",
     "shell.execute_reply.started": "2024-01-15T13:31:15.439172Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtest1/test1/spiders\u001b[0m\n",
      "└── __init__.py\n",
      "\n",
      "1 directory, 1 file\n"
     ]
    }
   ],
   "source": [
    "tree test1/test1/spiders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba4805d6-5240-48ab-a6a2-782d1497fe24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:31:19.282686Z",
     "iopub.status.busy": "2024-01-15T13:31:19.281995Z",
     "iopub.status.idle": "2024-01-15T13:31:19.394365Z",
     "shell.execute_reply": "2024-01-15T13:31:19.393434Z",
     "shell.execute_reply.started": "2024-01-15T13:31:19.282629Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# This package will contain the spiders of your Scrapy project\n",
      "#\n",
      "# Please refer to the documentation for information on how to create and manage\n",
      "# your spiders.\n"
     ]
    }
   ],
   "source": [
    "cat test1/test1/spiders/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da861669-3563-45aa-899a-161f78cc54c3",
   "metadata": {},
   "source": [
    "# 4. Write a Simple Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa054743-1223-4d6e-904a-16739c75b092",
   "metadata": {},
   "source": [
    "To create a crawler, you will add a new file inside the spiders directory at test1/test1/spiders/bookspider.py."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8b64b2-cd87-4ee0-8c09-eed573ba84a9",
   "metadata": {},
   "source": [
    "```sh\n",
    "$ cd test1/test1/spiders/\n",
    "$ scrapy genspider bookspider books.toscrape.com\n",
    "```\n",
    "```\n",
    "Created spider 'bookspider' using template 'basic' in module:\n",
    "  test1.spiders.bookspider\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "abd55e5b-8d6f-4c17-af1c-75a2154b264f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:31:48.403396Z",
     "iopub.status.busy": "2024-01-15T13:31:48.402628Z",
     "iopub.status.idle": "2024-01-15T13:31:48.514078Z",
     "shell.execute_reply": "2024-01-15T13:31:48.512387Z",
     "shell.execute_reply.started": "2024-01-15T13:31:48.403331Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtest1/test1/spiders/\u001b[0m\n",
      "├── bookspider.py\n",
      "├── __init__.py\n",
      "└── \u001b[01;34m__pycache__\u001b[0m\n",
      "    └── __init__.cpython-312.pyc\n",
      "\n",
      "2 directories, 3 files\n"
     ]
    }
   ],
   "source": [
    "tree test1/test1/spiders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "30875633-3fa4-4bd9-bee8-60dba3b0e327",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:32:14.207312Z",
     "iopub.status.busy": "2024-01-15T13:32:14.206579Z",
     "iopub.status.idle": "2024-01-15T13:32:14.318134Z",
     "shell.execute_reply": "2024-01-15T13:32:14.315643Z",
     "shell.execute_reply.started": "2024-01-15T13:32:14.207255Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import scrapy\n",
      "\n",
      "\n",
      "class BookspiderSpider(scrapy.Spider):\n",
      "    name = \"bookspider\"\n",
      "    allowed_domains = [\"books.toscrape.com\"]\n",
      "    start_urls = [\"https://books.toscrape.com\"]\n",
      "\n",
      "    def parse(self, response):\n",
      "        pass\n"
     ]
    }
   ],
   "source": [
    "cat test1/test1/spiders/bookspider.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3.11",
   "language": "python",
   "name": "venv3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
