{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "968ee719-bf68-4e29-9408-ec844121f4c9",
   "metadata": {},
   "source": [
    "# SCRAPY [DOCUMENTATION](https://docs.scrapy.org/en/latest/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01be5cd1-1600-4414-a5de-aab700913727",
   "metadata": {},
   "source": [
    "Scrapy is a fast high-level web crawling and web scraping framework, used to crawl websites and extract structured data from their pages. It can be used for a wide range of purposes, from data mining to monitoring and automated testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920f46d7-496c-4360-b7a5-451108e6d521",
   "metadata": {},
   "source": [
    "# Basic commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dcc10f-fcb3-4ac9-8f77-872712283f6c",
   "metadata": {},
   "source": [
    "## Scrapy command line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b22b9f-ec2a-4403-b3d9-806eeaa5c25c",
   "metadata": {},
   "source": [
    "|Scrapy command line|Description|\n",
    "|-|-|\n",
    "|**Help**||\n",
    "|`scrapy -h`|(`--help`) list all the available commands (run from the project's directory)|\n",
    "|`scrapy <command> -h`|help on the given command|\n",
    "|**List info**||\n",
    "|`scrapy list`|list all available crawlers (run from the project's deirectory)|\n",
    "|**Crawl a random webpage in shell**||\n",
    "|`scrapy shell <url>`||\n",
    "|(inside shell) `view(response)`|open the resonse object in your browser|\n",
    "|**Project**||\n",
    "|`scrapy startproject project_name [project_dir]`|create a new project|\n",
    "|`scrapy genspider [-t template] <name> <domain or URL>`|Create a new spider in the current folder or in the current project’s spiders folder,|\n",
    "|`scrapy crawl spider_name`|run the spider (from the project's top level dir)|\n",
    "|||\n",
    "|||\n",
    "|||\n",
    "|||\n",
    "\n",
    "```\n",
    "scrapy --help\n",
    "Scrapy 2.11.0 - active project: tutorial\n",
    "\n",
    "Usage:\n",
    "  scrapy <command> [options] [args]\n",
    "\n",
    "Available commands:\n",
    "  bench         Run quick benchmark test\n",
    "  check         Check spider contracts\n",
    "  crawl         Run a spider\n",
    "  edit          Edit spider\n",
    "  fetch         Fetch a URL using the Scrapy downloader\n",
    "  genspider     Generate new spider using pre-defined templates\n",
    "  list          List available spiders\n",
    "  parse         Parse URL (using its spider) and print the results\n",
    "  runspider     Run a self-contained spider (without creating a project)\n",
    "  settings      Get settings values\n",
    "  shell         Interactive scraping console\n",
    "  startproject  Create new project\n",
    "  version       Print Scrapy version\n",
    "  view          Open URL in browser, as seen by Scrapy\n",
    "\n",
    "Use \"scrapy <command> -h\" to see more info about a command\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d33a9d-eda2-4f44-b287-6512d7cef5ff",
   "metadata": {},
   "source": [
    "## Scrapy extraction most common tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558e8530-49b4-4ec5-b347-b719697157af",
   "metadata": {},
   "source": [
    "|Scrapy extraction tools|Description|\n",
    "|-|-|\n",
    "|`view(response)`|open the response page from the shell in your web browser|\n",
    "|**Response status codes**||\n",
    "|`response.status`||\n",
    "|**CSS selectors**||\n",
    "|`response.css`||\n",
    "|`response.css(\"title::text\").getall()`|get only text from the SelectorList|\n",
    "|||\n",
    "|||\n",
    "|||"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00a0466-e563-49de-a9d1-b380ced0c1f9",
   "metadata": {},
   "source": [
    "# <b>1. Scrapy tutorial</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1eb925-6bb2-4b85-8261-56c0b924f4b6",
   "metadata": {},
   "source": [
    "This tutorial will walk you through these tasks:\n",
    "\n",
    "- Creating a new Scrapy project\n",
    "- Writing a spider to crawl a site and extract data\n",
    "- Exporting the scraped data using the command line\n",
    "- Changing spider to recursively follow links\n",
    "- Using spider arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ca1bcf-ddc7-4728-8b21-6f94070a72b1",
   "metadata": {},
   "source": [
    "# 1.0 Installation guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe05960c-b29b-4731-aae8-35713f2b0606",
   "metadata": {},
   "source": [
    "We strongly recommend that you install Scrapy in a dedicated virtualenv, to avoid conflicting with your system packages.\n",
    "\n",
    "```sh\n",
    "(venv) $ pip install Scrapy\n",
    "```\n",
    "\n",
    "Scrapy is written in pure Python and depends on a few key Python packages (among others):\n",
    "\n",
    "- lxml, an efficient XML and HTML parser\n",
    "- parsel, an HTML/XML data extraction library written on top of lxml,\n",
    "- w3lib, a multi-purpose helper for dealing with URLs and web page encodings\n",
    "- twisted, an asynchronous networking framework\n",
    "- cryptography and pyOpenSSL, to deal with various network-level security needs\n",
    "\n",
    "Some of these packages themselves depend on non-Python packages that might require additional installation steps depending on your platform. Please check [platform-specific guides](https://docs.scrapy.org/en/latest/intro/install.html#intro-install-platform-notes).\n",
    "\n",
    "In case of any trouble related to these dependencies, please refer to their respective installation instructions:\n",
    "\n",
    "- [lxml installation](https://lxml.de/installation.html)\n",
    "- [cryptography installation](https://cryptography.io/en/latest/installation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122d7112-8b38-4950-a190-6272f74f6903",
   "metadata": {},
   "source": [
    "# 1.1 Creating a project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7312e67a-4e29-47dd-9799-01db5661b765",
   "metadata": {},
   "source": [
    "Before you start scraping, you will have to set up a new Scrapy project. Enter a directory where you’d like to store your code and run:\n",
    "\n",
    "```sh\n",
    "scrapy startproject tutorial\n",
    "```\n",
    "```\n",
    "New Scrapy project 'tutorial', using template directory '/home/commi/venv/venv3.11/lib/python3.11/site-packages/scrapy/templates/project', created in:\n",
    "    /home/commi/Yandex.Disk/it_learning/08_web_scraping/02_scrapy/data/tutorial\n",
    "\n",
    "You can start your first spider with:\n",
    "    cd tutorial\n",
    "    scrapy genspider example example.com\n",
    "```\n",
    "\n",
    "This will create a `tutorial` directory with the following contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8e616c4-6e21-4f23-8aef-6a47dd54ed32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-04T20:41:14.321005Z",
     "iopub.status.busy": "2024-02-04T20:41:14.320026Z",
     "iopub.status.idle": "2024-02-04T20:41:14.486706Z",
     "shell.execute_reply": "2024-02-04T20:41:14.484683Z",
     "shell.execute_reply.started": "2024-02-04T20:41:14.320929Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtutorial\u001b[0m\n",
      "├── scrapy.cfg\n",
      "└── \u001b[01;34mtutorial\u001b[0m\n",
      "    ├── __init__.py\n",
      "    ├── items.py\n",
      "    ├── middlewares.py\n",
      "    ├── pipelines.py\n",
      "    ├── settings.py\n",
      "    └── \u001b[01;34mspiders\u001b[0m\n",
      "        └── __init__.py\n",
      "\n",
      "3 directories, 7 files\n"
     ]
    }
   ],
   "source": [
    "cd ./data\n",
    "tree tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f47d0d-42a0-45d1-81bc-19405b2282cd",
   "metadata": {},
   "source": [
    "```\n",
    "tutorial/\n",
    "    scrapy.cfg            # deploy configuration file\n",
    "\n",
    "    tutorial/             # project's Python module, you'll import your code from here\n",
    "        __init__.py\n",
    "\n",
    "        items.py          # project items definition file\n",
    "\n",
    "        middlewares.py    # project middlewares file\n",
    "\n",
    "        pipelines.py      # project pipelines file\n",
    "\n",
    "        settings.py       # project settings file\n",
    "\n",
    "        spiders/          # a directory where you'll later put your spiders\n",
    "            __init__.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4fa774-eea7-4e39-992d-743fb81519f4",
   "metadata": {},
   "source": [
    "# 1.2 Our first Spider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1932bf53-e3c9-4209-9533-3590627dd4b7",
   "metadata": {},
   "source": [
    "**Spiders** are classes that you define and that Scrapy uses to scrape information from a website (or a group of websites). They must subclass `Spider` and define the initial requests to make, optionally \n",
    "- how to follow links in the pages, and \n",
    "- how to parse the downloaded page content to extract data.\n",
    "\n",
    "This is the code for our first `Spider`. Save it in a file named `quotes_spider.py` under the `tutorial/spiders` directory in your project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d31f0595-b402-48ee-8f1b-fbffacb2d778",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-04T20:43:53.627476Z",
     "iopub.status.busy": "2024-02-04T20:43:53.625433Z",
     "iopub.status.idle": "2024-02-04T20:43:53.742875Z",
     "shell.execute_reply": "2024-02-04T20:43:53.741806Z",
     "shell.execute_reply.started": "2024-02-04T20:43:53.627403Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".:\n",
      "draft.py  \u001b[0m\u001b[01;34m__pycache__\u001b[0m  quotes.jsonl  quotes_spider.py  \u001b[01;34mtutorial\u001b[0m\n",
      "\n",
      "./__pycache__:\n",
      "draft.cpython-311.pyc\n",
      "\n",
      "./tutorial:\n",
      "scrapy.cfg  \u001b[01;34mtutorial\u001b[0m\n",
      "\n",
      "./tutorial/tutorial:\n",
      "__init__.py  items.py  middlewares.py  pipelines.py  settings.py  \u001b[01;34mspiders\u001b[0m\n",
      "\n",
      "./tutorial/tutorial/spiders:\n",
      "__init__.py\n"
     ]
    }
   ],
   "source": [
    "ls -R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fd6194-8510-423d-b568-2aedbd634900",
   "metadata": {},
   "source": [
    "```python\n",
    "from pathlib import Path\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            \"https://quotes.toscrape.com/page/1/\",\n",
    "            \"https://quotes.toscrape.com/page/2/\",\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = f\"quotes-{page}.html\"\n",
    "        Path(filename).write_bytes(response.body)\n",
    "        self.log(f\"Saved file {filename}\")\n",
    "```\n",
    "\n",
    "As you can see, our Spider subclasses `scrapy.Spider` and defines some attributes and methods:\n",
    "\n",
    "- `name`: identifies the Spider. It **must be unique within a project**, that is, you can’t set the same name for different Spiders.\n",
    "\n",
    "- `start_requests()`: must return an iterable of `Request`s (you can return a **list** of requests or write a **generator** function) which the Spider will begin to crawl from. Subsequent requests will be generated successively from these initial requests.\n",
    "\n",
    "- `parse()`: a method that will be called to handle the response downloaded for each of the requests made. The `response` parameter is an instance of `TextResponse` that holds the page content and has further helpful methods to handle it.\n",
    "\n",
    "The `parse()` method usually parses the `response`, extracting the scraped data as dicts and also finding new URLs to follow and creating new requests (`Request`) from them.\n",
    "\n",
    "_ChatGPT:_  \n",
    "In Scrapy, when you use the `yield` statement within a Spider callback method like `parse`, the yielded items are not stored directly. Instead, they are processed by the Scrapy framework, typically passed to [Item Pipeline](#2.7-Item-Pipeline) components.\n",
    "\n",
    "Here's what happens when you yield items in Scrapy:\n",
    "- When you `yield` an item from a Spider callback method like `parse`, Scrapy will send that item to the Item Pipeline.\n",
    "- The Item Pipeline is a mechanism for processing the items scraped by the Spider. It allows you to perform various tasks on the scraped items, such as cleaning, validation, and persistence.\n",
    "- Each item that is `yield`ed is processed through the Item Pipeline sequentially, allowing you to define various stages of processing.\n",
    "- The Item Pipeline can perform operations like validation and transformation on the items before they are saved to a storage backend such as a database, JSON file, or CSV file.\n",
    "- You can define your own custom Item Pipeline to process the scraped items according to your requirements.\n",
    "\n",
    "So, in summary, the yielded data from a Scrapy Spider is not stored directly within the Spider itself; instead, it's passed through the Item Pipeline for further processing and eventual storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93f33ea-00e0-464f-a5c2-35378c70e7af",
   "metadata": {},
   "source": [
    "## How to run our spider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b388f81-c947-4fea-95f8-4a94758c5fd1",
   "metadata": {},
   "source": [
    "To put our spider to work, go to the project’s top level directory and run:\n",
    "\n",
    "```sh\n",
    "scrapy crawl quotes\n",
    "```\n",
    "This command runs the spider with name `quotes` that we’ve just added, that will send some requests for the `quotes.toscrape.com` domain. You will get an output similar to this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e7e96b-75f6-4259-9664-5ddc3d3c8bc6",
   "metadata": {},
   "source": [
    "```\n",
    "2024-02-05 01:57:09 [scrapy.utils.log] INFO: Scrapy 2.11.0 started (bot: tutorial)\n",
    "2024-02-05 01:57:09 [scrapy.utils.log] INFO: Versions: lxml 5.1.0.0, libxml2 2.12.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.11.2 (main, Mar 13 2023, 12:18:29) [GCC 12.2.0], pyOpenSSL 24.0.0 (OpenSSL 3.2.1 30 Jan 2024), cryptography 42.0.2, Platform Linux-6.1.0-17-amd64-x86_64-with-glibc2.36\n",
    "2024-02-05 01:57:09 [scrapy.addons] INFO: Enabled addons:\n",
    "[]\n",
    "2024-02-05 01:57:09 [asyncio] DEBUG: Using selector: EpollSelector\n",
    "2024-02-05 01:57:09 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
    "2024-02-05 01:57:09 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
    "2024-02-05 01:57:09 [scrapy.extensions.telnet] INFO: Telnet Password: b9a7a03404bf04d7\n",
    "2024-02-05 01:57:09 [scrapy.middleware] INFO: Enabled extensions:\n",
    "['scrapy.extensions.corestats.CoreStats',\n",
    " 'scrapy.extensions.telnet.TelnetConsole',\n",
    " 'scrapy.extensions.memusage.MemoryUsage',\n",
    " 'scrapy.extensions.logstats.LogStats']\n",
    "2024-02-05 01:57:09 [scrapy.crawler] INFO: Overridden settings:\n",
    "{'BOT_NAME': 'tutorial',\n",
    " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
    " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
    " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
    " 'ROBOTSTXT_OBEY': True,\n",
    " 'SPIDER_MODULES': ['tutorial.spiders'],\n",
    " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
    "2024-02-05 01:57:09 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
    "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
    " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
    " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
    " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
    " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
    " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
    " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
    " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
    " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
    " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
    " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
    " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
    "2024-02-05 01:57:09 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
    "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
    " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
    " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
    " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
    " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
    "2024-02-05 01:57:09 [scrapy.middleware] INFO: Enabled item pipelines:\n",
    "[]\n",
    "2024-02-05 01:57:09 [scrapy.core.engine] INFO: Spider opened\n",
    "2024-02-05 01:57:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
    "2024-02-05 01:57:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
    "2024-02-05 01:57:09 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://quotes.toscrape.com/robots.txt> (referer: None)\n",
    "2024-02-05 01:57:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/1/> (referer: None)\n",
    "2024-02-05 01:57:10 [quotes] DEBUG: Saved file quotes-1.html\n",
    "2024-02-05 01:57:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/2/> (referer: None)\n",
    "2024-02-05 01:57:10 [quotes] DEBUG: Saved file quotes-2.html\n",
    "2024-02-05 01:57:10 [scrapy.core.engine] INFO: Closing spider (finished)\n",
    "2024-02-05 01:57:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
    "{'downloader/request_bytes': 684,\n",
    " 'downloader/request_count': 3,\n",
    " 'downloader/request_method_count/GET': 3,\n",
    " 'downloader/response_bytes': 25556,\n",
    " 'downloader/response_count': 3,\n",
    " 'downloader/response_status_count/200': 2,\n",
    " 'downloader/response_status_count/404': 1,\n",
    " 'elapsed_time_seconds': 1.235283,\n",
    " 'finish_reason': 'finished',\n",
    " 'finish_time': datetime.datetime(2024, 2, 4, 20, 57, 10, 482312, tzinfo=datetime.timezone.utc),\n",
    " 'log_count/DEBUG': 8,\n",
    " 'log_count/INFO': 10,\n",
    " 'memusage/max': 65585152,\n",
    " 'memusage/startup': 65585152,\n",
    " 'response_received_count': 3,\n",
    " 'robotstxt/request_count': 1,\n",
    " 'robotstxt/response_count': 1,\n",
    " 'robotstxt/response_status_count/404': 1,\n",
    " 'scheduler/dequeued': 2,\n",
    " 'scheduler/dequeued/memory': 2,\n",
    " 'scheduler/enqueued': 2,\n",
    " 'scheduler/enqueued/memory': 2,\n",
    " 'start_time': datetime.datetime(2024, 2, 4, 20, 57, 9, 247029, tzinfo=datetime.timezone.utc)}\n",
    "2024-02-05 01:57:10 [scrapy.core.engine] INFO: Spider closed (finished)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9498b6dc-d419-43a6-975f-c2fe9809be7a",
   "metadata": {},
   "source": [
    "Now, check the files in the current directory. You should notice that two new files have been created: \n",
    "- quotes-1.html and \n",
    "- quotes-2.html, \n",
    "\n",
    "with the content for the respective URLs, as our parse method instructs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "898572ae-7c3c-4152-9ec0-c516d00087ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-04T22:18:03.312774Z",
     "iopub.status.busy": "2024-02-04T22:18:03.312022Z",
     "iopub.status.idle": "2024-02-04T22:18:03.422712Z",
     "shell.execute_reply": "2024-02-04T22:18:03.421645Z",
     "shell.execute_reply.started": "2024-02-04T22:18:03.312747Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotes-1.html  quotes-2.html  scrapy.cfg  \u001b[0m\u001b[01;34mtutorial\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ls ./tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a184ab66-4896-47d7-9c7e-1643e6ca04b5",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-02-04T22:18:41.703321Z",
     "iopub.status.busy": "2024-02-04T22:18:41.702513Z",
     "iopub.status.idle": "2024-02-04T22:18:41.914813Z",
     "shell.execute_reply": "2024-02-04T22:18:41.913439Z",
     "shell.execute_reply.started": "2024-02-04T22:18:41.703251Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "<head>\n",
      "\t<meta charset=\"UTF-8\">\n",
      "\t<title>Quotes to Scrape</title>\n",
      "    <link rel=\"stylesheet\" href=\"/static/bootstrap.min.css\">\n",
      "    <link rel=\"stylesheet\" href=\"/static/main.css\">\n",
      "</head>\n",
      "<body>\n",
      "    <div class=\"container\">\n",
      "        <div class=\"row header-box\">\n",
      "            <div class=\"col-md-8\">\n",
      "                <h1>\n",
      "                    <a href=\"/\" style=\"text-decoration: none\">Quotes to Scrape</a>\n",
      "                </h1>\n",
      "            </div>\n",
      "            <div class=\"col-md-4\">\n",
      "                <p>\n",
      "                \n",
      "                    <a href=\"/login\">Login</a>\n",
      "                \n",
      "                </p>\n",
      "            </div>\n",
      "        </div>\n",
      "    \n",
      "\n",
      "<div class=\"row\">\n",
      "    <div class=\"col-md-8\">\n",
      "\n",
      "    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n",
      "        <span class=\"text\" itemprop=\"text\">“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”</span>\n",
      "        <span>by <small class=\"author\" itemprop=\"author\">Albert Einstein</small>\n",
      "        <a href=\"/author/Albert-Einstein\">(about)</a>\n",
      "        </span>\n",
      "        <div class=\"tags\">\n",
      "            Tags:\n",
      "            <meta class=\"keywords\" itemprop=\"keywords\" content=\"change,deep-thoughts,thinking,world\" /    > \n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/change/page/1/\">change</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/deep-thoughts/page/1/\">deep-thoughts</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/thinking/page/1/\">thinking</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/world/page/1/\">world</a>\n",
      "            \n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n",
      "        <span class=\"text\" itemprop=\"text\">“It is our choices, Harry, that show what we truly are, far more than our abilities.”</span>\n",
      "        <span>by <small class=\"author\" itemprop=\"author\">J.K. Rowling</small>\n",
      "        <a href=\"/author/J-K-Rowling\">(about)</a>\n",
      "        </span>\n",
      "        <div class=\"tags\">\n",
      "            Tags:\n",
      "            <meta class=\"keywords\" itemprop=\"keywords\" content=\"abilities,choices\" /    > \n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/abilities/page/1/\">abilities</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/choices/page/1/\">choices</a>\n",
      "            \n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n",
      "        <span class=\"text\" itemprop=\"text\">“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”</span>\n",
      "        <span>by <small class=\"author\" itemprop=\"author\">Albert Einstein</small>\n",
      "        <a href=\"/author/Albert-Einstein\">(about)</a>\n",
      "        </span>\n",
      "        <div class=\"tags\">\n",
      "            Tags:\n",
      "            <meta class=\"keywords\" itemprop=\"keywords\" content=\"inspirational,life,live,miracle,miracles\" /    > \n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/inspirational/page/1/\">inspirational</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/life/page/1/\">life</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/live/page/1/\">live</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/miracle/page/1/\">miracle</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/miracles/page/1/\">miracles</a>\n",
      "            \n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n",
      "        <span class=\"text\" itemprop=\"text\">“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”</span>\n",
      "        <span>by <small class=\"author\" itemprop=\"author\">Jane Austen</small>\n",
      "        <a href=\"/author/Jane-Austen\">(about)</a>\n",
      "        </span>\n",
      "        <div class=\"tags\">\n",
      "            Tags:\n",
      "            <meta class=\"keywords\" itemprop=\"keywords\" content=\"aliteracy,books,classic,humor\" /    > \n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/aliteracy/page/1/\">aliteracy</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/books/page/1/\">books</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/classic/page/1/\">classic</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/humor/page/1/\">humor</a>\n",
      "            \n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n",
      "        <span class=\"text\" itemprop=\"text\">“Imperfection is beauty, madness is genius and it&#39;s better to be absolutely ridiculous than absolutely boring.”</span>\n",
      "        <span>by <small class=\"author\" itemprop=\"author\">Marilyn Monroe</small>\n",
      "        <a href=\"/author/Marilyn-Monroe\">(about)</a>\n",
      "        </span>\n",
      "        <div class=\"tags\">\n",
      "            Tags:\n",
      "            <meta class=\"keywords\" itemprop=\"keywords\" content=\"be-yourself,inspirational\" /    > \n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/be-yourself/page/1/\">be-yourself</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/inspirational/page/1/\">inspirational</a>\n",
      "            \n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n",
      "        <span class=\"text\" itemprop=\"text\">“Try not to become a man of success. Rather become a man of value.”</span>\n",
      "        <span>by <small class=\"author\" itemprop=\"author\">Albert Einstein</small>\n",
      "        <a href=\"/author/Albert-Einstein\">(about)</a>\n",
      "        </span>\n",
      "        <div class=\"tags\">\n",
      "            Tags:\n",
      "            <meta class=\"keywords\" itemprop=\"keywords\" content=\"adulthood,success,value\" /    > \n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/adulthood/page/1/\">adulthood</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/success/page/1/\">success</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/value/page/1/\">value</a>\n",
      "            \n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n",
      "        <span class=\"text\" itemprop=\"text\">“It is better to be hated for what you are than to be loved for what you are not.”</span>\n",
      "        <span>by <small class=\"author\" itemprop=\"author\">André Gide</small>\n",
      "        <a href=\"/author/Andre-Gide\">(about)</a>\n",
      "        </span>\n",
      "        <div class=\"tags\">\n",
      "            Tags:\n",
      "            <meta class=\"keywords\" itemprop=\"keywords\" content=\"life,love\" /    > \n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/life/page/1/\">life</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/love/page/1/\">love</a>\n",
      "            \n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n",
      "        <span class=\"text\" itemprop=\"text\">“I have not failed. I&#39;ve just found 10,000 ways that won&#39;t work.”</span>\n",
      "        <span>by <small class=\"author\" itemprop=\"author\">Thomas A. Edison</small>\n",
      "        <a href=\"/author/Thomas-A-Edison\">(about)</a>\n",
      "        </span>\n",
      "        <div class=\"tags\">\n",
      "            Tags:\n",
      "            <meta class=\"keywords\" itemprop=\"keywords\" content=\"edison,failure,inspirational,paraphrased\" /    > \n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/edison/page/1/\">edison</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/failure/page/1/\">failure</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/inspirational/page/1/\">inspirational</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/paraphrased/page/1/\">paraphrased</a>\n",
      "            \n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n",
      "        <span class=\"text\" itemprop=\"text\">“A woman is like a tea bag; you never know how strong it is until it&#39;s in hot water.”</span>\n",
      "        <span>by <small class=\"author\" itemprop=\"author\">Eleanor Roosevelt</small>\n",
      "        <a href=\"/author/Eleanor-Roosevelt\">(about)</a>\n",
      "        </span>\n",
      "        <div class=\"tags\">\n",
      "            Tags:\n",
      "            <meta class=\"keywords\" itemprop=\"keywords\" content=\"misattributed-eleanor-roosevelt\" /    > \n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/misattributed-eleanor-roosevelt/page/1/\">misattributed-eleanor-roosevelt</a>\n",
      "            \n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n",
      "        <span class=\"text\" itemprop=\"text\">“A day without sunshine is like, you know, night.”</span>\n",
      "        <span>by <small class=\"author\" itemprop=\"author\">Steve Martin</small>\n",
      "        <a href=\"/author/Steve-Martin\">(about)</a>\n",
      "        </span>\n",
      "        <div class=\"tags\">\n",
      "            Tags:\n",
      "            <meta class=\"keywords\" itemprop=\"keywords\" content=\"humor,obvious,simile\" /    > \n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/humor/page/1/\">humor</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/obvious/page/1/\">obvious</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/simile/page/1/\">simile</a>\n",
      "            \n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <nav>\n",
      "        <ul class=\"pager\">\n",
      "            \n",
      "            \n",
      "            <li class=\"next\">\n",
      "                <a href=\"/page/2/\">Next <span aria-hidden=\"true\">&rarr;</span></a>\n",
      "            </li>\n",
      "            \n",
      "        </ul>\n",
      "    </nav>\n",
      "    </div>\n",
      "    <div class=\"col-md-4 tags-box\">\n",
      "        \n",
      "            <h2>Top Ten tags</h2>\n",
      "            \n",
      "            <span class=\"tag-item\">\n",
      "            <a class=\"tag\" style=\"font-size: 28px\" href=\"/tag/love/\">love</a>\n",
      "            </span>\n",
      "            \n",
      "            <span class=\"tag-item\">\n",
      "            <a class=\"tag\" style=\"font-size: 26px\" href=\"/tag/inspirational/\">inspirational</a>\n",
      "            </span>\n",
      "            \n",
      "            <span class=\"tag-item\">\n",
      "            <a class=\"tag\" style=\"font-size: 26px\" href=\"/tag/life/\">life</a>\n",
      "            </span>\n",
      "            \n",
      "            <span class=\"tag-item\">\n",
      "            <a class=\"tag\" style=\"font-size: 24px\" href=\"/tag/humor/\">humor</a>\n",
      "            </span>\n",
      "            \n",
      "            <span class=\"tag-item\">\n",
      "            <a class=\"tag\" style=\"font-size: 22px\" href=\"/tag/books/\">books</a>\n",
      "            </span>\n",
      "            \n",
      "            <span class=\"tag-item\">\n",
      "            <a class=\"tag\" style=\"font-size: 14px\" href=\"/tag/reading/\">reading</a>\n",
      "            </span>\n",
      "            \n",
      "            <span class=\"tag-item\">\n",
      "            <a class=\"tag\" style=\"font-size: 10px\" href=\"/tag/friendship/\">friendship</a>\n",
      "            </span>\n",
      "            \n",
      "            <span class=\"tag-item\">\n",
      "            <a class=\"tag\" style=\"font-size: 8px\" href=\"/tag/friends/\">friends</a>\n",
      "            </span>\n",
      "            \n",
      "            <span class=\"tag-item\">\n",
      "            <a class=\"tag\" style=\"font-size: 8px\" href=\"/tag/truth/\">truth</a>\n",
      "            </span>\n",
      "            \n",
      "            <span class=\"tag-item\">\n",
      "            <a class=\"tag\" style=\"font-size: 6px\" href=\"/tag/simile/\">simile</a>\n",
      "            </span>\n",
      "            \n",
      "        \n",
      "    </div>\n",
      "</div>\n",
      "\n",
      "    </div>\n",
      "    <footer class=\"footer\">\n",
      "        <div class=\"container\">\n",
      "            <p class=\"text-muted\">\n",
      "                Quotes by: <a href=\"https://www.goodreads.com/quotes\">GoodReads.com</a>\n",
      "            </p>\n",
      "            <p class=\"copyright\">\n",
      "                Made with <span class='zyte'>❤</span> by <a class='zyte' href=\"https://www.zyte.com\">Zyte</a>\n",
      "            </p>\n",
      "        </div>\n",
      "    </footer>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "cat ./tutorial/quotes-1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc12bb0-3ad8-475c-8c62-f34a35fd7775",
   "metadata": {},
   "source": [
    "## What just happened under the hood?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbf9bea-4482-499c-b60a-8e16f745f855",
   "metadata": {},
   "source": [
    "Scrapy schedules the `scrapy.Request` objects returned by the `start_requests` method of the Spider. Upon receiving a `response` for each one, it instantiates `Response` objects and calls the callback method associated with the `request` (in this case, the `parse` method) passing the response as argument."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1817e5-2ae7-4871-84e6-bdceceb6cad1",
   "metadata": {},
   "source": [
    "## A shortcut to the `start_requests` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d87e87-96ac-4dda-8585-daf4d42c0839",
   "metadata": {},
   "source": [
    "Instead of implementing a `start_requests()` method that generates `scrapy.Request` objects from URLs, you can just define a `start_urls` class attribute with a list of URLs. This list will then be used by the default implementation of `start_requests()` to create the initial requests for your spider.\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    urls = [\n",
    "        \"https://quotes.toscrape.com/page/1/\",\n",
    "        \"https://quotes.toscrape.com/page/2/\",\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = f\"quotes-{page}.html\"\n",
    "        Path(filename).write_bytes(response.body)\n",
    "        self.log(f\"Saved file {filename}\")\n",
    "```\n",
    "\n",
    "The `parse()` method will be called to handle each of the requests for those URLs, even though we haven’t explicitly told Scrapy to do so. This happens because `parse()` is Scrapy’s default callback method, which is called for requests without an explicitly assigned callback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7cb801-1c53-4875-8d28-06c6864a880d",
   "metadata": {},
   "source": [
    "## Extracting data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5074321-f17a-4daf-bdb7-9d4e84341021",
   "metadata": {},
   "source": [
    "The best way to learn how to extract data with Scrapy is trying **selectors** using the Scrapy shell. Run:\n",
    "\n",
    "> Note: Remember to always enclose urls in quotes when running Scrapy shell from command-line, otherwise urls containing arguments (i.e. & character) will not work.<br>\n",
    "</br>\n",
    "On Windows, use double quotes instead:<br>\n",
    "</br>\n",
    "`scrapy shell \"https://quotes.toscrape.com/page/1/\"`\n",
    "\n",
    "```sh\n",
    "scrapy shell 'https://quotes.toscrape.com/page/1/'\n",
    "```\n",
    "```\n",
    "2024-02-06 14:47:14 [scrapy.utils.log] INFO: Scrapy 2.11.0 started (bot: tutorial)\n",
    "2024-02-06 14:47:14 [scrapy.utils.log] INFO: Versions: lxml 5.1.0.0, libxml2 2.12.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.11.2 (main, Mar 13 2023, 12:18:29) [GCC 12.2.0], pyOpenSSL 24.0.0 (OpenSSL 3.2.1 30 Jan 2024), cryptography 42.0.2, Platform Linux-6.1.0-17-amd64-x86_64-with-glibc2.36\n",
    "2024-02-06 14:47:14 [scrapy.addons] INFO: Enabled addons:\n",
    "[]\n",
    "2024-02-06 14:47:14 [asyncio] DEBUG: Using selector: EpollSelector\n",
    "2024-02-06 14:47:14 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
    "2024-02-06 14:47:14 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
    "2024-02-06 14:47:14 [scrapy.extensions.telnet] INFO: Telnet Password: 53d4e3939b5fb7e7\n",
    "2024-02-06 14:47:14 [scrapy.middleware] INFO: Enabled extensions:\n",
    "['scrapy.extensions.corestats.CoreStats',\n",
    " 'scrapy.extensions.telnet.TelnetConsole',\n",
    " 'scrapy.extensions.memusage.MemoryUsage']\n",
    "2024-02-06 14:47:14 [scrapy.crawler] INFO: Overridden settings:\n",
    "{'BOT_NAME': 'tutorial',\n",
    " 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',\n",
    " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
    " 'LOGSTATS_INTERVAL': 0,\n",
    " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
    " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
    " 'ROBOTSTXT_OBEY': True,\n",
    " 'SPIDER_MODULES': ['tutorial.spiders'],\n",
    " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
    "2024-02-06 14:47:14 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
    "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
    " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
    " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
    " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
    " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
    " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
    " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
    " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
    " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
    " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
    " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
    " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
    "2024-02-06 14:47:14 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
    "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
    " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
    " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
    " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
    " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
    "2024-02-06 14:47:14 [scrapy.middleware] INFO: Enabled item pipelines:\n",
    "[]\n",
    "2024-02-06 14:47:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
    "2024-02-06 14:47:14 [scrapy.core.engine] INFO: Spider opened\n",
    "2024-02-06 14:47:15 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://quotes.toscrape.com/robots.txt> (referer: None)\n",
    "2024-02-06 14:47:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/1/> (referer: None)\n",
    "[s] Available Scrapy objects:\n",
    "[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\n",
    "[s]   crawler    <scrapy.crawler.Crawler object at 0x7f5b3ece0cd0>\n",
    "[s]   item       {}\n",
    "[s]   request    <GET https://quotes.toscrape.com/page/1/>\n",
    "[s]   response   <200 https://quotes.toscrape.com/page/1/>\n",
    "[s]   settings   <scrapy.settings.Settings object at 0x7f5b3ffddc10>\n",
    "[s]   spider     <DefaultSpider 'default' at 0x7f5b3e7fa950>\n",
    "[s] Useful shortcuts:\n",
    "[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)\n",
    "[s]   fetch(req)                  Fetch a scrapy.Request and update local objects \n",
    "[s]   shelp()           Shell help (print this help)\n",
    "[s]   view(response)    View response in a browser\n",
    "2024-02-06 14:47:16 [asyncio] DEBUG: Using selector: EpollSelector\n",
    "```\n",
    "```ipython\n",
    "In [1]: \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724096bd-8857-43c4-914f-5f51b35222d9",
   "metadata": {},
   "source": [
    "Using the shell, you can try selecting elements using [CSS](https://www.w3.org/TR/selectors) with the `response` object:\n",
    "\n",
    "```ipython\n",
    "In [1]: response.css(\"title\")\n",
    "Out[1]: [<Selector query='descendant-or-self::title' data='<title>Quotes to Scrape</title>'>]\n",
    "\n",
    "In [2]: response.status\n",
    "Out[2]: 200\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b34527-4301-4b70-be21-f50099b57f90",
   "metadata": {},
   "source": [
    "### `get_all()` and `get()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86093451-8b67-4636-98ed-91283d3236a6",
   "metadata": {},
   "source": [
    "The result of running `response.css('title')` is a list-like object called **SelectorList**, which represents a list of Selector objects that wrap around XML/HTML elements and allow you to run further queries to fine-grain the selection or extract the data.\n",
    "\n",
    "To extract the text from the title above, you can do:\n",
    "\n",
    "```ipython\n",
    "In [9]: response.css(\"title::text\").getall()\n",
    "Out[9]: ['Quotes to Scrape']\n",
    "```\n",
    "\n",
    "There are two things to note here: one is that we’ve added `::text` to the CSS query, to mean we want to select only the text elements directly inside `<title>` element. If we don’t specify `::text`, we’d get the full title element, including its tags:\n",
    "\n",
    "```ipython\n",
    "In [11]: response.css(\"title\").getall()\n",
    "Out[11]: ['<title>Quotes to Scrape</title>']\n",
    "```\n",
    "\n",
    "The other thing is that the result of calling `.getall()` is a _list_: it is possible that a selector returns more than one result, so we extract them all. When you know you just want the first result, as in this case, you can do:\n",
    "\n",
    "```ipython\n",
    "In [12]: response.css(\"title::text\").get()\n",
    "Out[12]: 'Quotes to Scrape'\n",
    "```\n",
    "\n",
    "As an alternative, you could’ve written:\n",
    "\n",
    "```ipython\n",
    "In [16]: response.css(\"title::text\")[0].get()\n",
    "Out[16]: 'Quotes to Scrape'\n",
    "```\n",
    "\n",
    "Accessing an index on a SelectorList instance will raise an `IndexError` exception if there are no results. You might want to use `.get()` directly on the SelectorList instance instead, which returns `None` if there are no results:\n",
    "\n",
    "```ipython\n",
    "In [17]: response.css(\"noelement\").get()\n",
    "In [18]: response.css(\"noelement\")[0].get()\n",
    "---------------------------------------------------------------------------\n",
    "IndexError                                Traceback (most recent call last)\n",
    "...\n",
    "IndexError: list index out of range\n",
    "```\n",
    "\n",
    "There’s a lesson here: for most scraping code, you want it to be resilient to errors due to things not being found on a page, so that even if some parts fail to be scraped, you can at least get some data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55e7840-8d39-445e-8504-060085130009",
   "metadata": {},
   "source": [
    "### `re()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7055b91-e2f2-431e-9d30-c3b36a2750f8",
   "metadata": {},
   "source": [
    "Besides the `getall()` and `get()` methods, you can also use the `re()` method to extract using [regular expressions](https://docs.python.org/3/library/re.html):\n",
    "\n",
    "```ipython\n",
    "In [20]: response.css(\"title::text\").re(r\".*uot.*\")\n",
    "Out[20]: ['Quotes to Scrape']\n",
    "\n",
    "In [21]: response.css(\"title::text\").re(r\"Q\\w+\")\n",
    "Out[21]: ['Quotes']\n",
    "\n",
    "In [22]: response.css(\"title::text\").re(r\"(\\w+) to (\\w+)\")\n",
    "Out[22]: ['Quotes', 'Scrape']\n",
    "```\n",
    "\n",
    "- `\\w` represents any alphanumeric character (equivalent to `[a-zA-Z0-9_]`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dadabc-6323-4054-9a69-20753f17d10a",
   "metadata": {},
   "source": [
    "### `view(response)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de88dc81-81b3-4be4-b122-93cb530599a3",
   "metadata": {},
   "source": [
    "In order to find the proper CSS selectors to use, you might find it useful to open the `response` page from the shell in your web browser using `view(response)`. You can use your browser’s developer tools to inspect the HTML and come up with a selector (see [Using your browser’s Developer Tools for scraping](https://docs.scrapy.org/en/latest/topics/developer-tools.html#topics-developer-tools)).\n",
    "\n",
    "[Selector Gadget](https://selectorgadget.com/) is also a nice tool to quickly find CSS selector for visually selected elements, which works in many browsers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3677014c-d779-438a-b57d-dee723631675",
   "metadata": {},
   "source": [
    "### `XPath`: a brief intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74464531-cb38-4b0f-9444-3c2533fbf37f",
   "metadata": {},
   "source": [
    "See [XPath](../XPath_tutorial.ipynb#XPath).\n",
    "\n",
    "Besides CSS, Scrapy selectors also support using XPath expressions:\n",
    "\n",
    "```ipython\n",
    "In [24]: response.xpath(\"//title\")\n",
    "Out[24]: [<Selector query='//title' data='<title>Quotes to Scrape</title>'>]\n",
    "\n",
    "In [25]: response.xpath(\"//title/text()\").get()\n",
    "Out[25]: 'Quotes to Scrape'\n",
    "```\n",
    "\n",
    "XPath expressions are very powerful, and are the foundation of Scrapy Selectors. In fact, CSS selectors are converted to XPath under-the-hood. [You can see that](#Extracting-data) if you read closely the text representation of the selector objects in the shell.\n",
    "\n",
    "While perhaps not as popular as CSS selectors, XPath expressions offer more power because besides navigating the structure, it can also look at the content. Using XPath, you’re able to select things like: _select the link that contains the text “Next Page”_. This makes XPath very fitting to the task of scraping, and we encourage you to learn XPath even if you already know how to construct CSS selectors, it will make scraping much easier.\n",
    "\n",
    "We won’t cover much of XPath here, but you can read more about using [XPath with Scrapy Selectors](https://docs.scrapy.org/en/latest/topics/selectors.html#topics-selectors). To learn more about XPath, we recommend this [tutorial to learn XPath through examples](http://zvon.org/comp/r/tut-XPath_1.html), and this tutorial to learn [“how to think in XPath”](http://plasmasturm.org/log/xpath101/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fe8ac6-9ebe-4464-b0a9-7b0dea6e4569",
   "metadata": {},
   "source": [
    "### Extracting quotes and authors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035b167e-98de-4068-9c34-61ecdb081034",
   "metadata": {},
   "source": [
    "Now that you know a bit about selection and extraction, let’s complete our spider by writing the code to extract the quotes from the web page.\n",
    "\n",
    "Each quote in `https://quotes.toscrape.com` is represented by HTML elements that look like this:\n",
    "\n",
    "```html\n",
    "<div class=\"quote\">\n",
    "    <span class=\"text\">“The world as we have created it is a process of our\n",
    "    thinking. It cannot be changed without changing our thinking.”</span>\n",
    "    <span>\n",
    "        by <small class=\"author\">Albert Einstein</small>\n",
    "        <a href=\"/author/Albert-Einstein\">(about)</a>\n",
    "    </span>\n",
    "    <div class=\"tags\">\n",
    "        Tags:\n",
    "        <a class=\"tag\" href=\"/tag/change/page/1/\">change</a>\n",
    "        <a class=\"tag\" href=\"/tag/deep-thoughts/page/1/\">deep-thoughts</a>\n",
    "        <a class=\"tag\" href=\"/tag/thinking/page/1/\">thinking</a>\n",
    "        <a class=\"tag\" href=\"/tag/world/page/1/\">world</a>\n",
    "    </div>\n",
    "</div>\n",
    "```\n",
    "\n",
    "Let’s open up `scrapy shell` and play a bit to find out how to extract the data we want. We get a list of selectors for the quote HTML elements with:\n",
    "\n",
    "```sh\n",
    "scrapy shell 'https://quotes.toscrape.com'\n",
    "```\n",
    "```ipython\n",
    "In [1]: response.css(\"div.quote\")\n",
    "Out[1]: \n",
    "[<Selector query=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>,\n",
    " <Selector query=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>,\n",
    " <Selector query=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>,\n",
    " <Selector query=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>,\n",
    " <Selector query=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>,\n",
    " <Selector query=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>,\n",
    " <Selector query=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>,\n",
    " <Selector query=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>,\n",
    " <Selector query=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>,\n",
    " <Selector query=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>]\n",
    "```\n",
    "\n",
    "Each of the selectors returned by the query above allows us to run further queries over their sub-elements. Let’s assign the first selector to a variable, so that we can run our CSS selectors directly on a particular quote:\n",
    "\n",
    "```ipython\n",
    "In [3]: quote = response.css(\"div.quote\")[0]\n",
    "```\n",
    "\n",
    "Now, let’s extract `text`, `author` and the `tags` from that quote using the `quote` object we just created:\n",
    "\n",
    "```ipython\n",
    "In [4]: text = quote.css(\"span.text::text\").get()\n",
    "In [5]: text\n",
    "Out[5]: '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”'\n",
    "```\n",
    "\n",
    "Given that the `tags` are a list of strings, we can use the `.getall()` method to get all of them:\n",
    "\n",
    "```ipython\n",
    "In [6]: tags = quote.css(\"div.tags a.tag::text\").getall()\n",
    "In [7]: tags\n",
    "Out[7]: ['change', 'deep-thoughts', 'thinking', 'world']\n",
    "```\n",
    "\n",
    "Having figured out how to extract each bit, we can now iterate over all the quotes elements and put them together into a Python dictionary:\n",
    "\n",
    "```ipython\n",
    "In [8]: for quote in response.css(\"div.quote\"):\n",
    "   ...:     text = quote.css(\"span.text::text\").get()\n",
    "   ...:     author = quote.css(\"small.author::text\").get()\n",
    "   ...:     tags = quote.css(\"div.tags a.tag::text\").getall()\n",
    "   ...:     print(dict(text=text, author=author, tags=tags))\n",
    "   ...: \n",
    "{'text': '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”', 'author': 'Albert Einstein', 'tags': ['change', 'deep-thoughts', 'thinking', 'world']}\n",
    "{'text': '“It is our choices, Harry, that show what we truly are, far more than our abilities.”', 'author': 'J.K. Rowling', 'tags': ['abilities', 'choices']}\n",
    "{'text': '“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”', 'author': 'Albert Einstein', 'tags': ['inspirational', 'life', 'live', 'miracle', 'miracles']}\n",
    "{'text': '“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”', 'author': 'Jane Austen', 'tags': ['aliteracy', 'books', 'classic', 'humor']}\n",
    "{'text': \"“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\", 'author': 'Marilyn Monroe', 'tags': ['be-yourself', 'inspirational']}\n",
    "{'text': '“Try not to become a man of success. Rather become a man of value.”', 'author': 'Albert Einstein', 'tags': ['adulthood', 'success', 'value']}\n",
    "{'text': '“It is better to be hated for what you are than to be loved for what you are not.”', 'author': 'André Gide', 'tags': ['life', 'love']}\n",
    "{'text': \"“I have not failed. I've just found 10,000 ways that won't work.”\", 'author': 'Thomas A. Edison', 'tags': ['edison', 'failure', 'inspirational', 'paraphrased']}\n",
    "{'text': \"“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\", 'author': 'Eleanor Roosevelt', 'tags': ['misattributed-eleanor-roosevelt']}\n",
    "{'text': '“A day without sunshine is like, you know, night.”', 'author': 'Steve Martin', 'tags': ['humor', 'obvious', 'simile']}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc322623-939f-49b5-940d-ab90693866c1",
   "metadata": {},
   "source": [
    "### Extracting data in our spider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eaf8d7-9f0e-4a5e-b4a0-5491acdf0a4e",
   "metadata": {},
   "source": [
    "Let’s get back to our spider. Until now, it doesn’t extract any data in particular, just saves the whole HTML page to a local file. Let’s integrate the extraction logic above into our spider.\n",
    "\n",
    "A Scrapy spider typically generates many dictionaries containing the data extracted from the page. To do that, we use the `yield` Python keyword in the `callback`, as you can see below:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    urls = [\n",
    "        \"https://quotes.toscrape.com/page/1/\",\n",
    "        \"https://quotes.toscrape.com/page/2/\",\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css(\"div.quote\"):\n",
    "            yield {\n",
    "                \"text\": quote.css(\"span.text::text\").get(),\n",
    "                \"author\": quote.css(\"small.author::text\").get(),\n",
    "                \"tags\": quote.css(\"div.tags a.tag::text\").getall(),\n",
    "            }\n",
    "```\n",
    "\n",
    "To run this spider, exit the `scrapy shell` and run the crawler:\n",
    "\n",
    "```sh\n",
    "quit()\n",
    "scrapy crawl quotes\n",
    "```\n",
    "```\n",
    "2024-02-06 23:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
    "{'text': '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”', 'author': 'Albert Einstein', 'tags': ['change', 'deep-thoughts', 'thinking', 'world']}\n",
    "2024-02-06 23:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
    "{'text': '“It is our choices, Harry, that show what we truly are, far more than our abilities.”', 'author': 'J.K. Rowling', 'tags': ['abilities', 'choices']}\n",
    "2024-02-06 23:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
    "{'text': '“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”', 'author': 'Albert Einstein', 'tags': ['inspirational', 'life', 'live', 'miracle', 'miracles']}\n",
    "2024-02-06 23:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
    "{'text': '“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”', 'author': 'Jane Austen', 'tags': ['aliteracy', 'books', 'classic', 'humor']}\n",
    "2024-02-06 23:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
    "{'text': \"“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\", 'author': 'Marilyn Monroe', 'tags': ['be-yourself', 'inspirational']}\n",
    "2024-02-06 23:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
    "{'text': '“Try not to become a man of success. Rather become a man of value.”', 'author': 'Albert Einstein', 'tags': ['adulthood', 'success', 'value']}\n",
    "2024-02-06 23:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
    "{'text': '“It is better to be hated for what you are than to be loved for what you are not.”', 'author': 'André Gide', 'tags': ['life', 'love']}\n",
    "2024-02-06 23:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
    "{'text': \"“I have not failed. I've just found 10,000 ways that won't work.”\", 'author': 'Thomas A. Edison', 'tags': ['edison', 'failure', 'inspirational', 'paraphrased']}\n",
    "2024-02-06 23:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
    "{'text': \"“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\", 'author': 'Eleanor Roosevelt', 'tags': ['misattributed-eleanor-roosevelt']}\n",
    "2024-02-06 23:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
    "{'text': '“A day without sunshine is like, you know, night.”', 'author': 'Steve Martin', 'tags': ['humor', 'obvious', 'simile']}\n",
    "2024-02-06 23:53:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/2/> (referer: None)\n",
    "2024-02-06 23:53:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
    "{'text': \"“This life is what you make it. No matter what, you're going to mess up sometimes, it's a universal truth. But the good part is you get to decide how you're going to mess it up. Girls will be your friends - they'll act like it anyway. But just remember, some come, some go. The ones that stay with you through everything - they're your true best friends. Don't let go of them. Also remember, sisters make the best friends in the world. As for lovers, well, they'll come and go too. And baby, I hate to say it, most of them - actually pretty much all of them are going to break your heart, but you can't give up because if you give up, you'll never find your soulmate. You'll never find that half who makes you whole and that goes for everything. Just because you fail once, doesn't mean you're gonna fail at everything. Keep trying, hold on, and always, always, always believe in yourself, because if you don't, then who will, sweetie? So keep your head high, keep your chin up, and most importantly, keep smiling, because life's a beautiful thing and there's so much to smile about.”\", 'author': 'Marilyn Monroe', 'tags': ['friends', 'heartbreak', 'inspirational', 'life', 'love', 'sisters']}\n",
    "2024-02-06 23:53:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
    "{'text': '“It takes a great deal of bravery to stand up to our enemies, but just as much to stand up to our friends.”', 'author': 'J.K. Rowling', 'tags': ['courage', 'friends']}\n",
    "2024-02-06 23:53:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
    "{'text': \"“If you can't explain it to a six year old, you don't understand it yourself.”\", 'author': 'Albert Einstein', 'tags': ['simplicity', 'understand']}\n",
    "2024-02-06 23:53:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
    "{'text': \"“You may not be her first, her last, or her only. She loved before she may love again. But if she loves you now, what else matters? She's not perfect—you aren't either, and the two of you may never be perfect together but if she can make you laugh, cause you to think twice, and admit to being human and making mistakes, hold onto her and give her the most you can. She may not be thinking about you every second of the day, but she will give you a part of her that she knows you can break—her heart. So don't hurt her, don't change her, don't analyze and don't expect more than she can give. Smile when she makes you happy, let her know when she makes you mad, and miss her when she's not there.”\", 'author': 'Bob Marley', 'tags': ['love']}\n",
    "2024-02-06 23:53:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
    "{'text': '“I like nonsense, it wakes up the brain cells. Fantasy is a necessary ingredient in living.”', 'author': 'Dr. Seuss', 'tags': ['fantasy']}\n",
    "2024-02-06 23:53:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
    "{'text': '“I may not have gone where I intended to go, but I think I have ended up where I needed to be.”', 'author': 'Douglas Adams', 'tags': ['life', 'navigation']}\n",
    "2024-02-06 23:53:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
    "{'text': \"“The opposite of love is not hate, it's indifference. The opposite of art is not ugliness, it's indifference. The opposite of faith is not heresy, it's indifference. And the opposite of life is not death, it's indifference.”\", 'author': 'Elie Wiesel', 'tags': ['activism', 'apathy', 'hate', 'indifference', 'inspirational', 'love', 'opposite', 'philosophy']}\n",
    "2024-02-06 23:53:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
    "{'text': '“It is not a lack of love, but a lack of friendship that makes unhappy marriages.”', 'author': 'Friedrich Nietzsche', 'tags': ['friendship', 'lack-of-friendship', 'lack-of-love', 'love', 'marriage', 'unhappy-marriage']}\n",
    "2024-02-06 23:53:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
    "{'text': '“Good friends, good books, and a sleepy conscience: this is the ideal life.”', 'author': 'Mark Twain', 'tags': ['books', 'contentment', 'friends', 'friendship', 'life']}\n",
    "2024-02-06 23:53:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
    "{'text': '“Life is what happens to us while we are making other plans.”', 'author': 'Allen Saunders', 'tags': ['fate', 'life', 'misattributed-john-lennon', 'planning', 'plans']}\n",
    "2024-02-06 23:53:53 [scrapy.core.engine] INFO: Closing spider (finished)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0468328-06de-44fa-a1ed-8cff4be03ee4",
   "metadata": {},
   "source": [
    "## Storing the scraped data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c31e6f5-1a84-4e62-bbed-e2a0e860f768",
   "metadata": {},
   "source": [
    "The simplest way to store the scraped data is by using [Feed exports](#2.8-Feed-exports), with the following command:\n",
    "\n",
    "```sh\n",
    "scrapy crawl quotes -O quotes.json\n",
    "```\n",
    "\n",
    "That will generate a `quotes.json` file containing all scraped items, serialized in JSON.\n",
    "\n",
    "The `-O` command-line switch overwrites any existing file; use `-o` instead to append new content to any existing file. However, appending to a JSON file makes the file contents invalid JSON. When appending to a file, consider using a different serialization format, such as `JSON Lines`:\n",
    "\n",
    "```sh\n",
    "scrapy crawl quotes -o quotes.jsonl\n",
    "```\n",
    "\n",
    "The [JSON Lines format](http://jsonlines.org/) is useful because it’s stream-like, you can easily append new records to it. It doesn’t have the same problem of JSON when you run twice. Also, as each record is a separate line, you can process big files without having to fit everything in memory, there are tools like `JQ` to help do that at the command-line.\n",
    "\n",
    "In small projects (like the one in this tutorial), that should be enough. However, if you want to perform more complex things with the scraped items, you can write an [Item Pipeline](#2.7-Item-Pipeline). A placeholder file for Item Pipelines has been set up for you when the project is created, in `tutorial/pipelines.py`. Though you don’t need to implement any item pipelines if you just want to store the scraped items.\n",
    "\n",
    "```sh\n",
    "$ scrapy crawl -h\n",
    "```\n",
    "```\n",
    "Usage\n",
    "=====\n",
    "  scrapy crawl [options] <spider>\n",
    "\n",
    "Run a spider\n",
    "\n",
    "Options\n",
    "=======\n",
    "  -h, --help            show this help message and exit\n",
    "  -a NAME=VALUE         set spider argument (may be repeated)\n",
    "  -o FILE, --output FILE\n",
    "                        append scraped items to the end of FILE (use - for stdout), to define format set a colon at the end of the output\n",
    "                        URI (i.e. -o FILE:FORMAT)\n",
    "  -O FILE, --overwrite-output FILE\n",
    "                        dump scraped items into FILE, overwriting any existing file, to define format set a colon at the end of the\n",
    "                        output URI (i.e. -O FILE:FORMAT)\n",
    "  -t FORMAT, --output-format FORMAT\n",
    "                        format to use for dumping items\n",
    "\n",
    "Global Options\n",
    "--------------\n",
    "  --logfile FILE        log file. if omitted stderr will be used\n",
    "  -L LEVEL, --loglevel LEVEL\n",
    "                        log level (default: DEBUG)\n",
    "  --nolog               disable logging completely\n",
    "  --profile FILE        write python cProfile stats to FILE\n",
    "  --pidfile FILE        write process ID to FILE\n",
    "  -s NAME=VALUE, --set NAME=VALUE\n",
    "                        set/override setting (may be repeated)\n",
    "  --pdb                 enable pdb on failure\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1edd287b-96a0-4af3-a5f5-95f69dc6e739",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T19:19:42.039144Z",
     "iopub.status.busy": "2024-02-06T19:19:42.037500Z",
     "iopub.status.idle": "2024-02-06T19:19:42.052495Z",
     "shell.execute_reply": "2024-02-06T19:19:42.048680Z",
     "shell.execute_reply.started": "2024-02-06T19:19:42.039015Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# man jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d7db66-c191-448e-9218-a196f8ff5fbe",
   "metadata": {},
   "source": [
    "## Following links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f35f7cb-a8f6-4334-9b0f-9051deb9ed3f",
   "metadata": {},
   "source": [
    "Let’s say, instead of just scraping the stuff from the first two pages from https://quotes.toscrape.com, you want quotes from all the pages in the website.\n",
    "\n",
    "Now that you know how to extract data from pages, let’s see how to follow links from them.\n",
    "\n",
    "First thing is to extract the link to the page we want to follow. Examining our page, we can see there is a link to the next page with the following markup:\n",
    "\n",
    "```html\n",
    "<ul class=\"pager\">\n",
    "    <li class=\"next\">\n",
    "        <a href=\"/page/2/\">Next <span aria-hidden=\"true\">&rarr;</span></a>\n",
    "    </li>\n",
    "</ul>\n",
    "```\n",
    "\n",
    "We can try extracting it in the shell:\n",
    "\n",
    "```ipython\n",
    "In [1]: response.css(\"li.next a\")\n",
    "Out[1]: [<Selector query=\"descendant-or-self::li[@class and contains(concat(' ', normalize-space(@class), ' '), ' next ')]/descendant-or-self::*/a\" data='<a href=\"/page/2/\">Next <span aria-hi...'>]\n",
    "\n",
    "In [2]: response.css(\"li.next a\").get()\n",
    "Out[2]: '<a href=\"/page/2/\">Next <span aria-hidden=\"true\">→</span></a>'\n",
    "```\n",
    "\n",
    "This gets the anchor element, but we want the attribute `href`. For that, Scrapy supports a CSS extension that lets you select the attribute contents, like this:\n",
    "\n",
    "```ipython\n",
    "In [3]: response.css(\"li.next a::attr(href)\").get()\n",
    "Out[3]: '/page/2/'\n",
    "```\n",
    "There is also an `attrib` property available (see [Selecting element attributes](#Selecting-element-attributes) for more):\n",
    "\n",
    "```ipython\n",
    "In [4]: response.css(\"li.next a\").attrib[\"href\"]\n",
    "Out[4]: '/page/2/'\n",
    "```\n",
    "\n",
    "Let’s see now our spider modified to recursively follow the link to the next page, extracting data from it:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        \"https://quotes.toscrape.com/page/1/\",\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css(\"div.quote\"):\n",
    "            yield {\n",
    "                \"text\": quote.css(\"span.text::text\").get(),\n",
    "                \"author\": quote.css(\"small.author::text\").get(),\n",
    "                \"tags\": quote.css(\"div.tags a.tag::text\").getall(),\n",
    "            }\n",
    "\n",
    "        next_page = response.css(\"li.next a::attr(href)\").get()\n",
    "        if next_page is not None:\n",
    "            next_page = response.urljoin(next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "```\n",
    "\n",
    "Now, after extracting the data, the `parse()` method looks for the link to the next page, builds a full absolute URL using the `urljoin()` method (since the links can be relative) and `yield`s a new request to the next page, registering itself as callback to handle the data extraction for the next page and to keep the crawling going through all the pages.\n",
    "\n",
    "What you see here is Scrapy’s mechanism of following links: when you `yield` a `Request` in a callback method, Scrapy will schedule that request to be sent and register a callback method to be executed when that request finishes.\n",
    "\n",
    "Using this, you can build complex crawlers that follow links according to rules you define, and extract different kinds of data depending on the page it’s visiting.\n",
    "\n",
    "In our example, it creates a sort of loop, following all the links to the next page until it doesn’t find one – handy for crawling blogs, forums and other sites with **pagination**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcfc124-ed0e-4829-aee8-0adb585de4b3",
   "metadata": {},
   "source": [
    "### A shortcut for creating Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d1ec72-a2f2-48e9-a8e0-2a32a371c98d",
   "metadata": {},
   "source": [
    "As a shortcut for creating `Request` objects you can use `response.follow`:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        \"https://quotes.toscrape.com/page/1/\",\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css(\"div.quote\"):\n",
    "            yield {\n",
    "                \"text\": quote.css(\"span.text::text\").get(),\n",
    "                \"author\": quote.css(\"span small::text\").get(),\n",
    "                \"tags\": quote.css(\"div.tags a.tag::text\").getall(),\n",
    "            }\n",
    "\n",
    "        next_page = response.css(\"li.next a::attr(href)\").get()\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, callback=self.parse)\n",
    "```\n",
    "\n",
    "Unlike `scrapy.Request`, `response.follow` supports relative URLs directly - no need to call `urljoin`. Note that `response.follow` just returns a `Request` instance; you still have to `yield` this `Request`.\n",
    "\n",
    "You can also pass a selector to `response.follow` instead of a string; this selector should extract necessary attributes:\n",
    "\n",
    "```python\n",
    "for href in response.css(\"ul.pager a::attr(href)\"):\n",
    "    yield response.follow(href, callback=self.parse)\n",
    "```\n",
    "\n",
    "For `<a>` elements there is a shortcut: `response.follow` uses their `href` attribute automatically. So the code can be shortened further:\n",
    "\n",
    "```python\n",
    "for a in response.css(\"ul.pager a\"):\n",
    "    yield response.follow(a, callback=self.parse)\n",
    "```\n",
    "\n",
    "To create multiple requests from an iterable, you can use `response.follow_all` instead:\n",
    "\n",
    "```python\n",
    "anchors = response.css(\"ul.pager a\")\n",
    "yield from response.follow_all(anchors, callback=self.parse)\n",
    "```\n",
    "\n",
    "or, shortening it further:\n",
    "\n",
    "```python\n",
    "yield from response.follow_all(css=\"ul.pager a\", callback=self.parse)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3462a3bf-fff6-4f92-9fde-ee15248b9743",
   "metadata": {},
   "source": [
    "## More examples and patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523e5ad1-05c5-4636-98c9-8a016fae4ead",
   "metadata": {},
   "source": [
    "Here is another spider that illustrates callbacks and following links, this time for scraping author information:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class AuthorSpider(scrapy.Spider):\n",
    "    name = \"author\"\n",
    "\n",
    "    start_urls = [\"https://quotes.toscrape.com/\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        author_page_links = response.css(\".author + a\")\n",
    "        yield from resonse.follow_all(author_page_links, self.parse_author)\n",
    "\n",
    "        pagination_links = response.css(\"li.next a\")\n",
    "        yield from response.follow_all(pagination_links, self.parse)\n",
    "\n",
    "    def parse_author(self, response):\n",
    "        def extract_with_css(query):\n",
    "            return response.css(query).get(default=\"\").strip()\n",
    "\n",
    "        yield {\n",
    "            \"name\": extract_with_css(\"h3.author-title::text\"),\n",
    "            \"birthdate\": extract_with_css(\".author-born-date::text\"),\n",
    "            \"bio\": extract_with_css(\".author-description::text\"),\n",
    "        }\n",
    "```\n",
    "\n",
    "This spider will start from the main page, it will follow all the links to the authors pages calling the `parse_author` callback for each of them, and also the pagination links with the `parse` callback as we saw before.\n",
    "\n",
    "Here we’re passing callbacks to `response.follow_all` as positional arguments to make the code shorter; it also works for `Request`.\n",
    "\n",
    "The `parse_author` callback defines a helper function to extract and cleanup the data from a CSS query and `yield`s the Python `dict` with the author data.\n",
    "\n",
    "Another interesting thing this spider demonstrates is that, even if there are many quotes from the same author, we don’t need to worry about visiting the same author page multiple times. By default, Scrapy filters out duplicated requests to URLs already visited, avoiding the problem of hitting servers too much because of a programming mistake. This can be configured by the setting `DUPEFILTER_CLASS`.\n",
    "\n",
    "Hopefully by now you have a good understanding of how to use the mechanism of following links and callbacks with Scrapy.\n",
    "\n",
    "As yet another example spider that leverages the mechanism of following links, check out the `CrawlSpider` class for a generic spider that implements a small rules engine that you can use to write your crawlers on top of it.\n",
    "\n",
    "Also, a common pattern is to build an item with data from more than one page, using a trick to [pass additional data to the callbacks](#Passing-additional-data-to-callback-functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d457fd-147e-4115-bbc2-bb2b52127752",
   "metadata": {},
   "source": [
    "# 1.3 Using spider arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1381d791-26f4-408d-a798-caa6c5727581",
   "metadata": {},
   "source": [
    "You can provide command line arguments to your spiders by using the `-a` option when running them:\n",
    "\n",
    "```sh\n",
    "scrapy crawl quotes -O quotes-humor.json -a tag=humor\n",
    "```\n",
    "\n",
    "These arguments are passed to the Spider’s `__init__` method and become spider attributes by default.\n",
    "\n",
    "In this example, the value provided for the `tag` argument will be available via `self.tag`. You can use this to make your spider fetch only quotes with a specific tag, building the URL based on the argument:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        url = \"https://quotes.toscrape.com/\"\n",
    "        tag = getattr(self, \"tag\", None)\n",
    "        if tag is not None:\n",
    "            url += \"tag/\" + tag\n",
    "\n",
    "        yield scrapy.Request(url, self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css(\"div.quote\"):\n",
    "            yield {\n",
    "                \"text\": quote.css(\"span,text::text\").get(),\n",
    "                \"author\": qutote.css(\"small.author::text\").get(),\n",
    "            }\n",
    "\n",
    "        next_page = response.css(\"li.next a::attr(href)\").get()\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, self.parse)\n",
    "```\n",
    "\n",
    "If you pass the `tag=humor` argument to this spider, you’ll notice that it will only visit URLs from the humor tag, such as `https://quotes.toscrape.com/tag/humor`.\n",
    "\n",
    "You can learn more about [handling spider arguments here](#Spider-arguments)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f185efe-b616-43a1-a5e6-b353428a8bb9",
   "metadata": {},
   "source": [
    "# <b>2. Basic concepts</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcf98ed-4b96-4d47-bf9a-5541f63e6e1d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2.1 Command line tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af389cd8-1346-4b66-a4f7-493eb9e002b0",
   "metadata": {},
   "source": [
    "Scrapy is controlled through the scrapy command-line tool, to be referred here as the **“Scrapy tool”** to differentiate it from the sub-commands, which we just call **“commands”** or **“Scrapy commands”**.\n",
    "\n",
    "The Scrapy tool provides several commands, for multiple purposes, and each one accepts a different set of arguments and options.\n",
    "\n",
    "(The scrapy deploy command has been removed in 1.0 in favor of the standalone `scrapyd-deploy`. See [Deploying your project](#https://scrapyd.readthedocs.io/en/latest/deploy.html).)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ba4f14-0e35-4937-a359-047d7f31b446",
   "metadata": {},
   "source": [
    "## Configuration settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47412c3-d7dd-42fb-a252-01a02c749cbe",
   "metadata": {},
   "source": [
    "Scrapy will look for configuration parameters in ini-style `scrapy.cfg` files in standard locations:\n",
    "- `/etc/scrapy.cfg` or `c:\\scrapy\\scrapy.cfg` (system-wide),\n",
    "- `~/.config/scrapy.cfg` (`$XDG_CONFIG_HOME`) and `~/.scrapy.cfg` (`$HOME`) for global (user-wide) settings, and\n",
    "- `scrapy.cfg` inside a Scrapy project’s root (see next section).\n",
    "\n",
    "Settings from these files are merged in the listed order of preference: \n",
    "- user-defined values have higher priority than system-wide defaults and \n",
    "- project-wide settings will override all others, when defined.\n",
    "\n",
    "Scrapy also understands, and can be configured through, a number of environment variables. Currently these are:\n",
    "- SCRAPY_SETTINGS_MODULE (see [Designating the settings](#Designating-the-settings))\n",
    "- SCRAPY_PROJECT (see [Sharing the root directory between projects](#Sharing-the-root-directory-between-projects))\n",
    "- SCRAPY_PYTHON_SHELL (see [Scrapy shell](#2.3-Scrapy-shell))\n",
    "\n",
    "_ChatGPT:_  \n",
    "Ini-style, short for \"Initialization style,\" refers to a simple text-based file format used for configuration or initialization files in computing. It is named after the \".ini\" file extension commonly associated with these types of files in Windows environments.\n",
    "\n",
    "Ini-style files consist of sections, each containing key-value pairs, typically used to represent configuration settings for applications or systems. The structure is straightforward, with sections enclosed in square brackets \"`[ ]`\" and key-value pairs separated by an equals sign \"`=`\" or a colon \"`:`\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6e9a81-6f5f-43bd-92b0-ed446b46eb08",
   "metadata": {},
   "source": [
    "### Default structure of Scrapy projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68e269f-b48c-48d8-8762-f77af389761a",
   "metadata": {},
   "source": [
    "Before delving into the command-line tool and its sub-commands, let’s first understand the directory structure of a Scrapy project.\n",
    "\n",
    "Though it can be modified, all Scrapy projects have the same file structure by default, similar to this:\n",
    "\n",
    "```\n",
    "scrapy.cfg\n",
    "myproject/\n",
    "    __init__.py\n",
    "    items.py\n",
    "    middlewares.py\n",
    "    pipelines.py\n",
    "    settings.py\n",
    "    spiders/\n",
    "        __init__.py\n",
    "        spider1.py\n",
    "        spider2.py\n",
    "        ...\n",
    "```\n",
    "\n",
    "The directory where the `scrapy.cfg` file resides is known as the **project root directory**. That file contains the name of the python module that defines the project settings. Here is an example:\n",
    "\n",
    "```\n",
    "[settings]\n",
    "default = myproject.settings\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7001a7c7-757b-47bc-922c-1dec0ac324ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T13:24:10.551128Z",
     "iopub.status.busy": "2024-02-08T13:24:10.550692Z",
     "iopub.status.idle": "2024-02-08T13:24:10.656234Z",
     "shell.execute_reply": "2024-02-08T13:24:10.655199Z",
     "shell.execute_reply.started": "2024-02-08T13:24:10.551091Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/commi/Yandex.Disk/it_learning/08_web_scraping/02_scrapy\n"
     ]
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa09ec7b-fb2b-4281-9797-6881b097d5a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T13:25:15.012909Z",
     "iopub.status.busy": "2024-02-08T13:25:15.012624Z",
     "iopub.status.idle": "2024-02-08T13:25:15.126321Z",
     "shell.execute_reply": "2024-02-08T13:25:15.125041Z",
     "shell.execute_reply.started": "2024-02-08T13:25:15.012886Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mdata/tutorial\u001b[0m\n",
      "├── scrapy.cfg\n",
      "└── \u001b[01;34mtutorial\u001b[0m\n",
      "    ├── __init__.py\n",
      "    ├── items.py\n",
      "    ├── middlewares.py\n",
      "    ├── pipelines.py\n",
      "    ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "    │   ├── __init__.cpython-311.pyc\n",
      "    │   └── settings.cpython-311.pyc\n",
      "    ├── settings.py\n",
      "    └── \u001b[01;34mspiders\u001b[0m\n",
      "        ├── __init__.py\n",
      "        ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "        │   ├── __init__.cpython-311.pyc\n",
      "        │   ├── quotes_spider.cpython-311.pyc\n",
      "        │   └── tmp.cpython-311.pyc\n",
      "        ├── quotes_spider.py\n",
      "        └── tmp.py\n",
      "\n",
      "5 directories, 14 files\n"
     ]
    }
   ],
   "source": [
    "tree data/tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d9cc80-c93d-4b37-a4ee-43a2eb1a91a9",
   "metadata": {},
   "source": [
    "### Sharing the root directory between projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa40661-12a0-46a1-8f32-ae53547311ad",
   "metadata": {},
   "source": [
    "A project root directory, the one that contains the `scrapy.cfg`, may be shared by multiple Scrapy projects, each with its own settings module.\n",
    "\n",
    "In that case, you must define one or more aliases for those settings modules under `[settings]` in your `scrapy.cfg` file:\n",
    "\n",
    "```\n",
    "[settings]\n",
    "default = myproject1.settings\n",
    "project1 = myproject1.settings\n",
    "project2 = myproject2.settings\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64896358-a49f-4c6f-a891-356dc17f0562",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T13:26:09.487762Z",
     "iopub.status.busy": "2024-02-08T13:26:09.487365Z",
     "iopub.status.idle": "2024-02-08T13:26:09.596102Z",
     "shell.execute_reply": "2024-02-08T13:26:09.595094Z",
     "shell.execute_reply.started": "2024-02-08T13:26:09.487731Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Automatically created by: scrapy startproject\n",
      "#\n",
      "# For more information about the [deploy] section see:\n",
      "# https://scrapyd.readthedocs.io/en/latest/deploy.html\n",
      "\n",
      "[settings]\n",
      "default = tutorial.settings\n",
      "\n",
      "[deploy]\n",
      "#url = http://localhost:6800/\n",
      "project = tutorial\n"
     ]
    }
   ],
   "source": [
    "cat data/tutorial/scrapy.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e23f15e-2618-4b56-ada8-8228802be2ed",
   "metadata": {},
   "source": [
    "By default, the scrapy command-line tool will use the default settings. Use the `SCRAPY_PROJECT` environment variable to specify a different project for scrapy to use:\n",
    "\n",
    "```sh\n",
    "$ scrapy settings --get BOT_NAME\n",
    "Project 1 Bot\n",
    "$ export SCRAPY_PROJECT=project2\n",
    "$ scrapy settings --get BOT_NAME\n",
    "Project 2 Bot\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e59a853-45ee-45ec-9582-dd76c9ab0315",
   "metadata": {},
   "source": [
    "## Using the `scrapy` tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e3ddb8-0d24-433a-9710-c2fb35857f20",
   "metadata": {},
   "source": [
    "### General info and help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2d48da-5088-4f18-bf4d-770b5a58ecd0",
   "metadata": {},
   "source": [
    "You can start by running the Scrapy tool with no arguments and it will print some usage help and the available commands:\n",
    "\n",
    "```sh\n",
    "Scrapy X.Y - no active project\n",
    "\n",
    "Usage:\n",
    "  scrapy <command> [options] [args]\n",
    "\n",
    "Available commands:\n",
    "  crawl         Run a spider\n",
    "  fetch         Fetch a URL using the Scrapy downloader\n",
    "[...]\n",
    "```\n",
    "\n",
    "The first line will print the currently active project if you’re inside a Scrapy project. In this example it was run from outside a project. If run from inside a project it would have printed something like this:\n",
    "\n",
    "```sh\n",
    "Scrapy X.Y - project: myproject\n",
    "\n",
    "Usage:\n",
    "  scrapy <command> [options] [args]\n",
    "\n",
    "[...]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4357dd-ef05-48e8-8b4e-40c9c6346c26",
   "metadata": {},
   "source": [
    "### Creating projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b4069e-43d8-4c03-b075-b06666e5cd58",
   "metadata": {},
   "source": [
    "The first thing you typically do with the `scrapy` tool is create your Scrapy project:\n",
    "\n",
    "```sh\n",
    "scrapy startproject myproject [project_dir]\n",
    "```\n",
    "\n",
    "That will create a Scrapy project under the `project_dir` directory. If `project_dir` wasn’t specified, `project_dir` will be the same as `myproject`.\n",
    "\n",
    "Next, you go inside the new project directory:\n",
    "\n",
    "``` sh\n",
    "cd project_dir\n",
    "```\n",
    "\n",
    "And you’re ready to use the `scrapy` command to manage and control your project from there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a05beb3-9d7c-454e-b9b8-bbcf0df0a5ac",
   "metadata": {},
   "source": [
    "### Controlling projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864a1ee2-9024-4945-a01a-aa388e45a24c",
   "metadata": {},
   "source": [
    "You use the `scrapy` tool _from inside_ your projects to control and manage them.\n",
    "\n",
    "For example, to create a new spider:\n",
    "\n",
    "```\n",
    "scrapy genspider mydomain mydomain.com\n",
    "```\n",
    "\n",
    "Some Scrapy commands (like `crawl`) must be run from inside a Scrapy project. See the [commands reference](#Available-tool-commands) below for more information on which commands must be run from inside projects, and which not.\n",
    "\n",
    "Also keep in mind that some commands may have slightly different behaviours when running them from inside projects. For example, the `fetch` command will use spider-overridden behaviours (such as the `user_agent` attribute to override the user-agent) if the url being fetched is associated with some specific spider. This is intentional, as the `fetch` command is meant to be used to check how spiders are downloading pages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55e7475-de82-47d1-8944-63d700c1fa72",
   "metadata": {},
   "source": [
    "## Available tool commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75c6413-6b2a-40d6-98d8-7385c7346512",
   "metadata": {},
   "source": [
    "This section contains a list of the available built-in commands with a description and some usage examples. Remember, you can always get more info about each command by running:\n",
    "\n",
    "```sh\n",
    "scrapy <command> -h\n",
    "```\n",
    "\n",
    "And you can see all available commands with:\n",
    "\n",
    "```sh\n",
    "scrapy -h\n",
    "```\n",
    "\n",
    "There are two kinds of commands, \n",
    "- those that only work from inside a `Scrapy` project (**Project-specific commands**) and \n",
    "- those that also work without an active Scrapy project (**Global commands**), \n",
    "\n",
    "though they may behave slightly different when running from inside a project (as they would use the project overridden settings)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2465e2e4-2e8a-482a-b9ac-21225ab2a8c3",
   "metadata": {},
   "source": [
    "### Global commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce85567c-e09f-4a3b-a0a4-a69e9cf8904b",
   "metadata": {},
   "source": [
    "- `startproject`\n",
    "- `genspider`\n",
    "- `settings`\n",
    "- `runspider`\n",
    "- `shell`\n",
    "- `fetch`\n",
    "- `view`\n",
    "- `version`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1242a3-0a5c-4f4d-b89d-e77c163c76f5",
   "metadata": {},
   "source": [
    "#### `startproject`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5556c4-dd0f-4266-9b94-b91888db76f9",
   "metadata": {},
   "source": [
    "Syntax: `scrapy startproject <project_name> [project_dir]`\n",
    "\n",
    "Requires project: no\n",
    "\n",
    "Creates a new Scrapy project named `project_name`, under the `project_dir` directory. If `project_dir` wasn’t specified, `project_dir` will be the same as `project_name`.\n",
    "\n",
    "Usage example:\n",
    "```sh\n",
    "$ scrapy startproject myproject\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb24cc9-46a7-4f54-abf9-44bdf6ed5629",
   "metadata": {},
   "source": [
    "#### `genspider`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea80d55-9768-4625-83a8-d9c4f49efe3b",
   "metadata": {},
   "source": [
    "Syntax: `scrapy genspider [-t template] <name> <domain or URL>`\n",
    "\n",
    "Requires project: no\n",
    "\n",
    "New in version 2.6.0: The ability to pass a URL instead of a domain.\n",
    "\n",
    "Create a new spider in the current folder or in the current project’s spiders folder, if called from inside a project. The `<name>` parameter is set as the spider’s name, while `<domain or URL>` is used to generate the `allowed_domains` and `start_urls` spider’s attributes.\n",
    "\n",
    "Usage example:\n",
    "\n",
    "```sh\n",
    "$ scrapy genspider -l\n",
    "```\n",
    "```\n",
    "Available templates:\n",
    "  basic\n",
    "  crawl\n",
    "  csvfeed\n",
    "  xmlfeed\n",
    "```\n",
    "```sh\n",
    "$ scrapy genspider example example.com\n",
    "```\n",
    "```\n",
    "Created spider 'example' using template 'basic'\n",
    "```\n",
    "```sh\n",
    "$ scrapy genspider -t crawl scrapyorg scrapy.org\n",
    "```\n",
    "```\n",
    "Created spider 'scrapyorg' using template 'crawl'\n",
    "```\n",
    "\n",
    "This is just a convenience shortcut command for creating spiders based on pre-defined templates, but certainly not the only way to create spiders. \n",
    "\n",
    "> You can just create the spider source code files yourself, instead of using this command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68c17c5-9995-4470-97d8-d20102db32de",
   "metadata": {},
   "source": [
    "#### `fetch`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee76f657-e31d-418a-b954-8098a043f17a",
   "metadata": {},
   "source": [
    "Syntax: `scrapy fetch <url>`\n",
    "\n",
    "Requires project: no\n",
    "\n",
    "Downloads the given URL using the Scrapy downloader and writes the contents to standard output.\n",
    "\n",
    "The interesting thing about this command is that it fetches the page how the spider would download it. For example, if the spider has a `USER_AGENT` attribute which overrides the User Agent, it will use that one.\n",
    "\n",
    "So this command can be used to “see” how your spider would fetch a certain page.\n",
    "\n",
    "If used outside a project, no particular per-spider behaviour would be applied and it will just use the default Scrapy downloader settings.\n",
    "\n",
    "Supported options:\n",
    "- `--spider SPIDER`: bypass spider autodetection and force use of specific spider\n",
    "- `--headers`: print the response’s HTTP headers instead of the response’s body\n",
    "- `--no-redirect`: do not follow HTTP 3xx redirects (default is to follow them)\n",
    "\n",
    "Usage examples:\n",
    "\n",
    "```sh\n",
    "$ scrapy fetch --nolog http://www.example.com/some/page.html\n",
    "[ ... html content here ... ]\n",
    "\n",
    "$ scrapy fetch --nolog --headers http://www.example.com/\n",
    "{'Accept-Ranges': ['bytes'],\n",
    " 'Age': ['1263   '],\n",
    " 'Connection': ['close     '],\n",
    " 'Content-Length': ['596'],\n",
    " 'Content-Type': ['text/html; charset=UTF-8'],\n",
    " 'Date': ['Wed, 18 Aug 2010 23:59:46 GMT'],\n",
    " 'Etag': ['\"573c1-254-48c9c87349680\"'],\n",
    " 'Last-Modified': ['Fri, 30 Jul 2010 15:30:18 GMT'],\n",
    " 'Server': ['Apache/2.2.3 (CentOS)']}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1db59a4-1160-4dae-ba13-29399aa2701f",
   "metadata": {},
   "source": [
    "#### `shell`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3c60d7-2e40-4cf7-b7b1-71abbb6e788a",
   "metadata": {},
   "source": [
    "Syntax: `scrapy shell [url]`\n",
    "\n",
    "Requires project: no\n",
    "\n",
    "Starts the Scrapy shell for the given URL (if given) or empty if no URL is given. Also supports UNIX-style local file paths, either relative with `./` or `../` prefixes or absolute file paths. See [Scrapy shell](#2.6-Scrapy-shell) for more info.\n",
    "\n",
    "Supported options:\n",
    "- `--spider=SPIDER`: bypass spider autodetection and force use of specific spider\n",
    "- `-c code`: evaluate the code in the shell, print the result and exit\n",
    "- `--no-redirect`: do not follow HTTP 3xx redirects (default is to follow them); this only affects the URL you may pass as argument on the command line; once you are inside the shell, fetch(url) will still follow HTTP redirects by default.\n",
    "\n",
    "Usage example:\n",
    "\n",
    "```sh\n",
    "$ scrapy shell http://www.example.com/some/page.html\n",
    "```\n",
    "```\n",
    "[ ... scrapy shell starts ... ]\n",
    "```\n",
    "```sh\n",
    "$ scrapy shell --nolog http://www.example.com/ -c '(response.status, response.url)'\n",
    "```\n",
    "```\n",
    "(200, 'http://www.example.com/')\n",
    "```\n",
    "```sh\n",
    "# shell follows HTTP redirects by default\n",
    "$ scrapy shell --nolog http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F -c '(response.status, response.url)'\n",
    "```\n",
    "```\n",
    "(200, 'http://example.com/')\n",
    "```\n",
    "```sh\n",
    "# you can disable this with --no-redirect\n",
    "# (only for the URL passed as command line argument)\n",
    "$ scrapy shell --no-redirect --nolog http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F -c '(response.status, response.url)'\n",
    "```\n",
    "```\n",
    "(302, 'http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9e15f1-7f4f-4c7a-aaab-2c348b3ff58b",
   "metadata": {},
   "source": [
    "#### `view`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5319b6b-d66c-407b-b94b-a5fdf178e9fa",
   "metadata": {},
   "source": [
    "Syntax: `scrapy view <url>`\n",
    "\n",
    "Requires project: no\n",
    "\n",
    "Opens the given URL in a browser, as your Scrapy spider would “see” it. Sometimes spiders see pages differently from regular users, so this can be used to check what the spider “sees” and confirm it’s what you expect.\n",
    "\n",
    "Supported options:\n",
    "- `--spider SPIDER`: bypass spider autodetection and force use of specific spider\n",
    "- `--no-redirect`: do not follow HTTP 3xx redirects (default is to follow them)\n",
    "\n",
    "Usage example:\n",
    "```sh\n",
    "$ scrapy view http://www.example.com/some/page.html\n",
    "[ ... browser starts ... ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3455b1d4-9dfe-4ae4-b064-04bdfa0310bd",
   "metadata": {},
   "source": [
    "#### `settings`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d11ea3e-69f0-4ec0-b129-2662e703a239",
   "metadata": {},
   "source": [
    "Syntax: `scrapy settings [options]`\n",
    "\n",
    "Requires project: no\n",
    "\n",
    "Get the value of a Scrapy setting.\n",
    "\n",
    "If used inside a project it’ll show the project setting value, otherwise it’ll show the default Scrapy value for that setting.\n",
    "\n",
    "Example usage:\n",
    "```sh\n",
    "$ scrapy settings --get BOT_NAME\n",
    "scrapybot\n",
    "$ scrapy settings --get DOWNLOAD_DELAY\n",
    "0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d7a7e9-4f69-4246-9f5a-870c36bd8bb7",
   "metadata": {},
   "source": [
    "#### `runspider`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69893570-95cc-4db5-9590-e75f44c9b285",
   "metadata": {},
   "source": [
    "Syntax: `scrapy runspider <spider_file.py>`\n",
    "\n",
    "Requires project: no\n",
    "\n",
    "Run a spider self-contained in a Python file, without having to create a project.\n",
    "\n",
    "Example usage:\n",
    "```sh\n",
    "$ scrapy runspider myspider.py\n",
    "[ ... spider starts crawling ... ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad821fe-a7a6-414e-bd73-1360c6a7ee1a",
   "metadata": {},
   "source": [
    "#### `version`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cccbee-4ff7-484d-8476-946fbb25f811",
   "metadata": {},
   "source": [
    "Syntax: `scrapy version [-v]`\n",
    "\n",
    "Requires project: no\n",
    "\n",
    "Prints the Scrapy version. If used with `-v` it also prints Python, Twisted and Platform info, which is useful for bug reports."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83cbd90-a31e-4cef-8844-78df1a206d20",
   "metadata": {},
   "source": [
    "#### `bench`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c804180-568c-4db6-8512-1a7e41a1d7a2",
   "metadata": {},
   "source": [
    "Syntax: `scrapy bench`\n",
    "\n",
    "Requires project: no\n",
    "\n",
    "Run a quick benchmark test. [Benchmarking](#4.12-Benchmarking)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187bbb2b-c1e0-4d18-87a2-8b48c2c060cc",
   "metadata": {},
   "source": [
    "### Project-only commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3815f3-e657-42fb-8ea8-4a37491682d3",
   "metadata": {},
   "source": [
    "- `crawl`\n",
    "- `check`\n",
    "- `list`\n",
    "- `edit`\n",
    "- `parse`\n",
    "- `bench`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e1e36d-3ac3-416a-9d2b-a7d674c3ec92",
   "metadata": {},
   "source": [
    "#### `crawl`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f72c64f-4d54-4cd2-80c4-a9c4f0b7c90a",
   "metadata": {},
   "source": [
    "Syntax: `scrapy crawl <spider>`\n",
    "\n",
    "Requires project: yes\n",
    "\n",
    "Start crawling using a spider.\n",
    "\n",
    "Supported options:\n",
    "- `-h`, `--help`: show a help message and exit\n",
    "- `-a NAME=VALUE`: set a spider argument (may be repeated)\n",
    "- `--output FILE` or `-o FILE`: append scraped items to the end of `FILE` (use - for stdout), to define format set a colon at the end of the output URI (i.e. `-o FILE:FORMAT`)\n",
    "- `--overwrite-output FILE` or `-O FILE`: dump scraped items into `FILE`, overwriting any existing file, to define format set a colon at the end of the output URI (i.e. `-O FILE:FORMAT`)\n",
    "- `--output-format FORMAT` or `-t FORMAT`: deprecated way to define format to use for dumping items, does not work in combination with `-O`\n",
    "\n",
    "Usage examples:\n",
    "```sh\n",
    "$ scrapy crawl myspider\n",
    "```\n",
    "```\n",
    "[ ... myspider starts crawling ... ]\n",
    "```\n",
    "```sh\n",
    "$ scrapy crawl -o myfile:csv myspider\n",
    "```\n",
    "```\n",
    "[ ... myspider starts crawling and appends the result to the file myfile in csv format ... ]\n",
    "```\n",
    "```sh\n",
    "$ scrapy crawl -O myfile:json myspider\n",
    "```\n",
    "```\n",
    "[ ... myspider starts crawling and saves the result in myfile in json format overwriting the original content... ]\n",
    "```\n",
    "```sh\n",
    "$ scrapy crawl -o myfile -t csv myspider\n",
    "```\n",
    "```\n",
    "[ ... myspider starts crawling and appends the result to the file myfile in csv format ... ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af19fab-8920-4671-8a4d-603ba7540dc7",
   "metadata": {},
   "source": [
    "#### check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6813288-ea3d-40a7-bbc3-00b5b7c61002",
   "metadata": {},
   "source": [
    "Syntax: `scrapy check [options] <spider>`\n",
    "\n",
    "Requires project: yes\n",
    "\n",
    "Check spider contracts.\n",
    "\n",
    "Options:\n",
    "\n",
    "```\n",
    "  -h, --help            show this help message and exit\n",
    "  -l, --list            only list contracts, without checking them\n",
    "  -v, --verbose         print contract tests for all spiders\n",
    "\n",
    "Global Options\n",
    "--------------\n",
    "  --logfile FILE        log file. if omitted stderr will be used\n",
    "  -L LEVEL, --loglevel LEVEL\n",
    "                        log level (default: DEBUG)\n",
    "  --nolog               disable logging completely\n",
    "  --profile FILE        write python cProfile stats to FILE\n",
    "  --pidfile FILE        write process ID to FILE\n",
    "  -s NAME=VALUE, --set NAME=VALUE\n",
    "                        set/override setting (may be repeated)\n",
    "  --pdb                 enable pdb on failure\n",
    "```\n",
    "\n",
    "Usage examples:\n",
    "\n",
    "```sh\n",
    "$ scrapy check -l\n",
    "first_spider\n",
    "  * parse\n",
    "  * parse_item\n",
    "second_spider\n",
    "  * parse\n",
    "  * parse_item\n",
    "\n",
    "$ scrapy check\n",
    "[FAILED] first_spider:parse_item\n",
    ">>> 'RetailPricex' field is missing\n",
    "\n",
    "[FAILED] first_spider:parse\n",
    ">>> Returned 92 requests, expected 0..4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c34bdad-5244-4dcf-ba70-ca37ff894e6c",
   "metadata": {},
   "source": [
    "##### Spider conracts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f80a975-dada-4ba0-9d33-888de08573d2",
   "metadata": {},
   "source": [
    "_ChatGPT:_  \n",
    "In Scrapy, `\"spider contracts\"` refer to a feature designed to enforce certain rules or constraints on the output of your spiders (the crawlers you create using Scrapy). These contracts are defined using the `scrapy.contracts` module and are meant to ensure that your spiders are behaving as expected and producing the desired output.\n",
    "\n",
    "Spider contracts can be particularly useful for ensuring data quality and consistency, especially when you're dealing with large-scale web scraping projects where data structure or content might vary across different pages or domains.\n",
    "\n",
    "Here's a brief overview of how spider contracts work in Scrapy:\n",
    "\n",
    "- **Defining Contracts:** You define contracts using Python classes that subclass `scrapy.contracts.Contract`. Within these classes, you define methods to verify specific aspects of the spider output, such as the presence of certain fields or the structure of items.\n",
    "\n",
    "- **Implementing Verification Logic:** Within the contract class methods, you write the logic to verify whether the spider output meets the defined criteria. This logic typically involves inspecting the scraped data and raising exceptions if the criteria are not met.\n",
    "\n",
    "- **Enforcing Contracts:** Once you've defined your contracts, you can enable them for specific spiders by adding the contracts attribute to your spider classes and specifying which contracts to apply.\n",
    "\n",
    "- **Running Contract Checks:** When you run your spiders, Scrapy will automatically apply the specified contracts to the spider output and perform the verification checks. If any of the checks fail, Scrapy will raise an exception, indicating that the spider output does not conform to the defined contracts.\n",
    "\n",
    "By using spider contracts, you can ensure that your spiders are producing reliable and consistent output, which can be essential for downstream processing and analysis of the scraped data.\n",
    "\n",
    "Here's a simple example of a spider contract class that verifies the presence of certain fields in the spider output:\n",
    "\n",
    "```python\n",
    "from scrapy.contracts import Contract\n",
    "\n",
    "\n",
    "class RequiredFieldsContract(Contract):\n",
    "    \"\"\"Contract to verify the presence of required fields in spider output\"\"\"\n",
    "\n",
    "    name = 'required_fields'\n",
    "\n",
    "    def verify(self, output):\n",
    "        required_fields = ['title', 'url', 'content']\n",
    "        for item in output:\n",
    "            for field in required_fields:\n",
    "                if field not in item:\n",
    "                    raise ContractFail(f\"Required field '{field}' missing in item: {item}\")\n",
    "```\n",
    "\n",
    "And here's how you would enable this contract for a specific spider:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = 'myspider'\n",
    "    start_urls = ['http://example.com']\n",
    "\n",
    "    contracts = [RequiredFieldsContract]\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Your parsing logic here\n",
    "        pass\n",
    "```\n",
    "\n",
    "With this setup, Scrapy will verify that each item produced by MySpider contains the required fields specified in the `RequiredFieldsContract`. If any item fails this verification, Scrapy will raise a `ContractFail` exception."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b61150-9b0b-407d-b9d8-94fd1bda2456",
   "metadata": {},
   "source": [
    "#### `list`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc7c1f6-1f6a-4f00-b8d2-e1c04f180554",
   "metadata": {},
   "source": [
    "Syntax: `scrapy list`\n",
    "\n",
    "Requires project: yes\n",
    "\n",
    "List all available spiders in the current project. The output is one spider per line.\n",
    "\n",
    "Usage example:\n",
    "\n",
    "```sh\n",
    "$ scrapy list\n",
    "spider1\n",
    "spider2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddac57f-c471-4df5-9099-e6c17c1d074c",
   "metadata": {},
   "source": [
    "#### `edit`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3a49f7-c3b0-4be7-9cc2-f2fae06ad004",
   "metadata": {},
   "source": [
    "Syntax: `scrapy edit <spider>`\n",
    "\n",
    "Requires project: yes\n",
    "\n",
    "Edit the given spider using the editor defined in the `EDITOR` environment variable or (if unset) the `EDITOR` setting.\n",
    "\n",
    "This command is provided only as a convenience shortcut for the most common case, the developer is of course free to choose any tool or IDE to write and debug spiders.\n",
    "\n",
    "Usage example:\n",
    "```sh\n",
    "$ scrapy edit spider1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafd92c4-d913-4326-b241-eb4b7d1e46df",
   "metadata": {},
   "source": [
    "#### `parse`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0685754-6672-46e3-98bb-9768aaec1fbe",
   "metadata": {},
   "source": [
    "Syntax: `scrapy parse <url> [options]`\n",
    "\n",
    "Requires project: yes\n",
    "\n",
    "Fetches the given URL and parses it with the spider that handles it, using the method passed with the `--callback` option, or parse if not given.\n",
    "\n",
    "Supported options:\n",
    "- `--spider=SPIDER`: bypass spider autodetection and force use of specific spider\n",
    "- `--a NAME=VALUE`: set spider argument (may be repeated)\n",
    "- `--callback` or `-c`: spider method to use as callback for parsing the response\n",
    "- `--meta` or `-m`: additional request meta that will be passed to the callback request. This must be a valid json string. Example: `–meta=’{“foo” : “bar”}`’\n",
    "- `--cbkwargs`: additional keyword arguments that will be passed to the callback. This must be a valid json string. Example: `–cbkwargs=’{“foo” : “bar”}`’\n",
    "- `--pipelines`: process items through pipelines\n",
    "- `--rules` or `-r`: use `CrawlSpider` rules to discover the callback (i.e. spider method) to use for parsing the response\n",
    "- `--noitems`: don’t show scraped items\n",
    "- `--nolinks`: don’t show extracted links\n",
    "- `--nocolour`: avoid using pygments to colorize the output\n",
    "- `--depth` or `-d`: depth level for which the requests should be followed recursively (default: 1)\n",
    "- `--verbose` or `-v`: display information for each depth level\n",
    "- `--output` or `-o`: dump scraped items to a file\n",
    "\n",
    "New in version 2.3.\n",
    "\n",
    "Usage example:\n",
    "```sh\n",
    "$ scrapy parse http://www.example.com/ -c parse_item\n",
    "[ ... scrapy log lines crawling example.com spider ... ]\n",
    "\n",
    ">>> STATUS DEPTH LEVEL 1 <<<\n",
    "# Scraped Items  ------------------------------------------------------------\n",
    "[{'name': 'Example item',\n",
    " 'category': 'Furniture',\n",
    " 'length': '12 cm'}]\n",
    "\n",
    "# Requests  -----------------------------------------------------------------\n",
    "[]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39354f2-ed1f-4197-8cc1-2110a94b17da",
   "metadata": {},
   "source": [
    "## Custom project commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de89e9d9-4476-4739-bc20-a9d2fa44f66b",
   "metadata": {},
   "source": [
    "You can also add your custom project commands by using the `COMMANDS_MODULE` setting. See the Scrapy commands in [scrapy/commands](https://github.com/scrapy/scrapy/tree/master/scrapy/commands) for examples on how to implement your commands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc05d3e8-effa-46d8-aa1a-e70a56b092a2",
   "metadata": {},
   "source": [
    "### `COMMANDS_MODULE`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8376e57c-a9fb-4455-9aeb-92a2e8f0bd80",
   "metadata": {},
   "source": [
    "Default: `''` (empty string)\n",
    "\n",
    "A module to use for looking up custom Scrapy commands. This is used to add custom commands for your Scrapy project.\n",
    "\n",
    "Example:\n",
    "```sh\n",
    "COMMANDS_MODULE = \"mybot.commands\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc21c0e-28e7-486e-97b0-1ffdd34fa5e7",
   "metadata": {},
   "source": [
    "### Register commands via `setup.py` entry points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4e606d-945a-4fed-8f16-87198095e466",
   "metadata": {},
   "source": [
    "You can also add Scrapy commands from an external library by adding a `scrapy.commands` section in the entry points of the library `setup.py` file.\n",
    "\n",
    "The following example adds `my_command` command:\n",
    "\n",
    "```python\n",
    "from setuptools import setup, find_packages\n",
    "\n",
    "setup(\n",
    "    name=\"scrapy-mymodule\",\n",
    "    entry_points={\n",
    "        \"scrapy.commands\": [\n",
    "            \"my_command=my_scrapy_module.commands:MyCommand\",\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eb0cc9-17fc-4dce-bd26-e5e0521a3e29",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2.2 Spiders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23da3b48-32dc-4d2d-a293-97a6c9ed9a6b",
   "metadata": {},
   "source": [
    "**Spiders** are classes which define how a certain site (or a group of sites) will be scraped, including \n",
    "- how to perform the crawl (i.e. follow links) and \n",
    "- how to extract structured data from their pages (i.e. scraping items). \n",
    "\n",
    "In other words, Spiders are the place where you define the custom behaviour for crawling and parsing pages for a particular site (or, in some cases, a group of sites).\n",
    "\n",
    "For spiders, the scraping cycle goes through something like this:\n",
    "\n",
    "1. You start by generating the initial **Requests** to crawl the first URLs, and specify a **callback function** to be called with the `response` downloaded from those requests.\n",
    "\n",
    "    The first requests to perform are obtained by calling the `start_requests()` method which (by default) generates `Request` for the URLs specified in the `start_urls` and the `parse` method as callback function for the Requests.\n",
    "\n",
    "1. In the callback function, you parse the **response** (web page) and return \n",
    "- [item objects](#2.4-Items), \n",
    "- `Request` objects, or \n",
    "- an iterable of these objects. \n",
    "\n",
    "    Those Requests will also contain a callback (maybe the same) and will then be downloaded by Scrapy and then their response handled by the specified callback.\n",
    "\n",
    "3. In callback functions, you parse the page contents, typically using [Selectors](#2.3-Selectors) (but you can also use `BeautifulSoup`, `lxml` or whatever mechanism you prefer) and generate items with the parsed data.\n",
    "\n",
    "4. Finally, the items returned from the spider will be typically \n",
    "- persisted to a database (in some [Item Pipeline](#2.7-Item-Pipeline)) or \n",
    "- written to a file using [Feed exports](#2.8-Feed-exports).\n",
    "\n",
    "Even though this cycle applies (more or less) to any kind of spider, there are different kinds of default spiders bundled into Scrapy for different purposes. We will talk about those types here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc61fdf-4c86-470c-a0a1-1522cab0a38f",
   "metadata": {},
   "source": [
    "## `scrapy.Spider`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff575310-89e1-4257-addb-f5adf0784bdf",
   "metadata": {},
   "source": [
    "```python\n",
    "class scrapy.spiders.Spider\n",
    "class scrapy.Spider\n",
    "```\n",
    "\n",
    "This is the simplest spider, and the one from which every other spider must inherit (including spiders that come bundled with Scrapy, as well as spiders that you write yourself). It doesn’t provide any special functionality. It just provides a default `start_requests()` implementation which sends requests from the `start_urls` spider attribute and calls the spider’s method `parse` for each of the resulting responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4745f42a-c903-4e03-85e8-ba1d6c3bb485",
   "metadata": {},
   "source": [
    "### `name`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92720f61-00f0-4f41-a7fa-0a5920f436bf",
   "metadata": {},
   "source": [
    "A string which defines the name for this spider. \n",
    "\n",
    "The spider `name` is how the spider is located (and instantiated) by Scrapy, so **it must be unique**. However, nothing prevents you from instantiating more than one instance of the same spider. This is the most important spider attribute and it’s required.\n",
    "\n",
    "If the spider scrapes a single domain, a common practice is to name the spider after the domain, with or without the TLD. So, for example, a spider that crawls `mywebsite.com` would often be called `mywebsite`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa2ce8d-9341-464e-a9e8-8ccdede9469a",
   "metadata": {},
   "source": [
    "### `allowed_domains`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa92a733-1013-41f4-bf8c-5ddbdcde39c8",
   "metadata": {},
   "source": [
    "An optional list of strings containing domains that this spider is allowed to crawl. Requests for URLs not belonging to the domain names specified in this list (or their subdomains) won’t be followed if `OffsiteMiddleware` is enabled.\n",
    "\n",
    "Let’s say your target url is `https://www.example.com/1.html`, then add '`example.com`' to the list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96206b29-ff4d-42fd-b294-82cded02b6c9",
   "metadata": {},
   "source": [
    "### `start_urls`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7fc88b-0ab9-46b1-b443-81edf5fc1915",
   "metadata": {},
   "source": [
    "A list of URLs where the spider will begin to crawl from, when no particular URLs are specified. So, the first pages downloaded will be those listed here. The subsequent `Request` will be generated successively from data contained in the start URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ab07e2-1f6b-46bc-ab55-029dddd16834",
   "metadata": {},
   "source": [
    "### `custom_settings`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21e5c20-ed27-4bf7-a705-dc005aba57b5",
   "metadata": {},
   "source": [
    "A dictionary of settings that will be overridden from the project wide configuration when running this spider. It must be defined as a class attribute since the settings are updated before instantiation.\n",
    "\n",
    "For a list of available built-in settings see: [Built-in settings reference](#Built-in-settings-reference)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e79ecce-f740-4b21-bc50-9b9dfb16d004",
   "metadata": {},
   "source": [
    "### `crawler`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15b4cf3-296d-4810-b3a8-a4c4e9d2d9d1",
   "metadata": {},
   "source": [
    "This attribute is set by the `from_crawler()` class method after initializing the class, and links to the `Crawler` object to which this spider instance is bound.\n",
    "\n",
    "Crawlers encapsulate a lot of components in the project for their single entry access (such as extensions, middlewares, signals managers, etc). See [Crawler API](#Crawler-API) to know more about them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410ee02d-2cfb-488d-9713-30771f24b59d",
   "metadata": {},
   "source": [
    "### `settings`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7754e708-f842-45c0-8a4c-63b580516507",
   "metadata": {},
   "source": [
    "Configuration for running this spider. This is a `Settings` instance, see the [Settings](#2.11-Settings) topic for a detailed introduction on this subject."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d907670e-9820-4bd9-8bbb-f317ad16b07f",
   "metadata": {},
   "source": [
    "### `logger`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0212c630-32c0-40e2-96b9-922e4f3026f9",
   "metadata": {},
   "source": [
    "Python logger created with the Spider’s name. You can use it to send log messages through it as described on [Logging from Spiders](#Logging-from-Spiders)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91ffd3e-9476-4491-9ccc-b4978b497cd4",
   "metadata": {},
   "source": [
    "### `state`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1264b83c-fc26-489f-af6d-5dadcace5162",
   "metadata": {},
   "source": [
    "A `dict` you can use to persist some spider state between batches. See [Keeping persistent state between batches](#Keeping-persistent-state-between-batches) to know more about it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efab0609-e32f-4618-8720-bf80a27a8e0e",
   "metadata": {},
   "source": [
    "### `from_crawler(crawler, *args, **kwargs)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdc3f04-d689-459c-8119-1ad2b4be92fe",
   "metadata": {},
   "source": [
    "This is the class method used by Scrapy to create your spiders.\n",
    "\n",
    "You probably won’t need to override this directly because the default implementation acts as a proxy to the `__init__()` method, calling it with the given arguments `args` and named arguments `kwargs`.\n",
    "\n",
    "Nonetheless, this method sets the crawler and settings attributes in the new instance so they can be accessed later inside the spider’s code.\n",
    "\n",
    "_Changed in version 2.11:_ The settings in `crawler.settings` can now be modified in this method, which is handy if you want to modify them based on arguments. As a consequence, these settings aren’t the final values as they can be modified later by e.g. [add-ons](#5.2-Add-ons). For the same reason, most of the `Crawler` attributes aren’t initialized at this point.\n",
    "\n",
    "The final settings and the initialized `Crawler` attributes are available in the `start_requests()` method, handlers of the `engine_started` signal and later.\n",
    "\n",
    "**Parameters**\n",
    "- `crawler` (Crawler instance) – crawler to which the spider will be bound\n",
    "- `args` (list) – arguments passed to the `__init__()` method\n",
    "- `kwargs` (dict) – keyword arguments passed to the `__init__()` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d480951-a736-4d04-856c-048b646378f8",
   "metadata": {},
   "source": [
    "### `classmethod update_settings(settings)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a8be40-9650-431d-8140-2911138a174f",
   "metadata": {},
   "source": [
    "The `update_settings()` method is used to modify the spider’s settings and is called during initialization of a spider instance.\n",
    "\n",
    "It takes a `Settings` object as a parameter and can add or update the spider’s configuration values. This method is a class method, meaning that it is called on the Spider class and allows all instances of the spider to share the same configuration.\n",
    "\n",
    "While per-spider settings can be set in `custom_settings`, using `update_settings()` allows you to dynamically \n",
    "- add, \n",
    "- remove or \n",
    "- change settings \n",
    "\n",
    "based on other \n",
    "- settings, \n",
    "- spider attributes or \n",
    "- other factors \n",
    "\n",
    "and use setting priorities other than 'spider'. Also, it’s easy to extend `update_settings()` in a subclass by overriding it, while doing the same with `custom_settings` can be hard.\n",
    "\n",
    "For example, suppose a spider needs to modify `FEEDS`:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = \"myspider\"\n",
    "    custom_feed = {\n",
    "        \"/home/user/documents/items.json\": {\n",
    "            \"format\": \"json\",\n",
    "            \"indent\": 4,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def update_settings(cls, settings):\n",
    "        super().update_settings(settings)\n",
    "        settings.setdefault(\"FEEDS\", {}).update(cls.custom_feed)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aea074-29d9-458e-90eb-a41ecccca030",
   "metadata": {},
   "source": [
    "### `parse(response)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca3b02b-a092-4354-b43b-bc6e598bdca6",
   "metadata": {},
   "source": [
    "This is the default callback used by Scrapy to process downloaded `response`s, when their requests don’t specify a callback.\n",
    "\n",
    "The `parse` method is in charge of processing the `response` and returning scraped data and/or more URLs to follow. Other `Request`s callbacks have the same requirements as the Spider class.\n",
    "\n",
    "This method, as well as any other `Request` callback, must return \n",
    "- a `Request` object, \n",
    "- an [`item` object](#2.4-Items), \n",
    "- an iterable of `Request` objects and/or \n",
    "- [item objects](#2.4-Items), or \n",
    "- `None`.\n",
    "\n",
    "**Parameters**\n",
    "- `response` (Response) – the response to parse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c626767c-3fba-4f9c-b42f-d89327d382da",
   "metadata": {},
   "source": [
    "### `log(message[, level, component])`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bf1671-20a7-4c54-acd0-cb1d8cb5177b",
   "metadata": {},
   "source": [
    "Wrapper that sends a log message through the Spider’s logger, kept for backward compatibility. For more information see [Logging from Spiders](#Logging-from-Spiders)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf3c034-f640-4c9b-a663-bab745d377a8",
   "metadata": {},
   "source": [
    "### `closed(reason)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ad622a-1ae5-4e3d-80b9-283eadd7079c",
   "metadata": {},
   "source": [
    "Called when the spider closes. This method provides a shortcut to `signals.connect()` for the `spider_closed` signal.\n",
    "\n",
    "Let’s see an example:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = \"example.com\"\n",
    "    allowed_domains = [\"example.com\"]\n",
    "    start_urls = [\n",
    "        \"http://www.example.com/1.html\",\n",
    "        \"http://www.example.com/2.html\",\n",
    "        \"http://www.example.com/3.html\",\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        self.logger.info(\"A response from %s just arrived!\", response.url)\n",
    "```\n",
    "\n",
    "Return multiple Requests and items from a single callback:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = \"example.com\"\n",
    "    allowed_domains = [\"example.com\"]\n",
    "    start_urls = [\n",
    "        \"http://www.example.com/1.html\",\n",
    "        \"http://www.example.com/2.html\",\n",
    "        \"http://www.example.com/3.html\",\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for h3 in response.xpath(\"//h3\").getall():\n",
    "            yield {\"title\": h3}\n",
    "\n",
    "        for href in response.xpath(\"//a/@href\").getall():\n",
    "            yield scrapy.Request(response.urljoin(href), self.parse)\n",
    "```\n",
    "\n",
    "Instead of `start_urls` you can use `start_requests()` directly; to give data more structure you can use `Item` objects:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "from myproject.items import MyItem\n",
    "\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = \"example.com\"\n",
    "    allowed_domains = [\"example.com\"]\n",
    "\n",
    "    def start_requests(self):\n",
    "        yield scrapy.Request(\"http://www.example.com/1.html\", self.parse)\n",
    "        yield scrapy.Request(\"http://www.example.com/2.html\", self.parse)\n",
    "        yield scrapy.Request(\"http://www.example.com/3.html\", self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        for h3 in response.xpath(\"//h3\").getall():\n",
    "            yield MyItem(title=h3)\n",
    "\n",
    "        for href in response.xpath(\"//a/@href\").getall():\n",
    "            yield scrapy.Request(response.urljoin(href), self.parse)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7491c1a-a1ff-45c5-8a35-67d9b42768f7",
   "metadata": {},
   "source": [
    "## Spider arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df023785-1f13-428f-9aef-933b0321fac0",
   "metadata": {},
   "source": [
    "Spiders can receive arguments that modify their behaviour. Some common uses for spider arguments are to define the start URLs or to restrict the crawl to certain sections of the site, but they can be used to configure any functionality of the spider.\n",
    "\n",
    "Spider arguments are passed through the `crawl` command using the `-a` option. For example:\n",
    "\n",
    "```sh\n",
    "scrapy crawl myspider -a category=electronics\n",
    "```\n",
    "\n",
    "Spiders can access arguments in their `__init__` methods:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = \"myspider\"\n",
    "\n",
    "    def __init__(self, category=None, *args, **kwargs):\n",
    "        super(MySpider, self).__init__(*args, **kwargs)\n",
    "        self.start_urls = [f\"http://www.example.com/categories/{category}\"]\n",
    "        # ...\n",
    "```\n",
    "\n",
    "The default `__init__` method will take any spider arguments and copy them to the spider as attributes. The above example can also be written as follows:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = \"myspider\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        yield scrapy.Request(f\"http://www.example.com/categories/{self.category}\")\n",
    "```\n",
    "\n",
    "If you are running [Scrapy from a script](#Run-Scrapy-from-a-script), you can specify spider arguments when calling `CrawlerProcess.crawl` or `CrawlerRunner.crawl`:\n",
    "\n",
    "```python\n",
    "process = CrawlerProcess()\n",
    "process.crawl(MySpider, category=\"electronics\")\n",
    "```\n",
    "\n",
    "Keep in mind that spider arguments are only strings. The spider will not do any parsing on its own. If you were to set the `start_urls` attribute from the command line, you would have to parse it on your own into a list using something like `ast.literal_eval()` or `json.loads()` and then set it as an attribute. Otherwise, you would cause iteration over a `start_urls` string (a very common python pitfall) resulting in each character being seen as a separate url.\n",
    "\n",
    "A valid use case is to set the http auth credentials used by `HttpAuthMiddleware` or the user agent used by `UserAgentMiddleware`:\n",
    "\n",
    "```sh\n",
    "scrapy crawl myspider -a http_user=myuser -a http_pass=mypassword -a user_agent=mybot\n",
    "```\n",
    "\n",
    "Spider arguments can also be passed through the Scrapyd schedule.json API. See [Scrapyd documentation](https://scrapyd.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad5eb7d-0502-471b-8aa5-35aa361aa5a3",
   "metadata": {},
   "source": [
    "## Generic Spiders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7c0409-6f64-46ea-ad99-3f6fc56ec0d8",
   "metadata": {},
   "source": [
    "Scrapy comes with some useful generic spiders that you can use to subclass your spiders from. Their aim is to provide convenient functionality for a few common scraping cases, like following all links on a site based on certain rules, crawling from [Sitemaps](https://www.sitemaps.org/index.html), or parsing an XML/CSV feed.\n",
    "\n",
    "For the examples used in the following spiders, we’ll assume you have a project with a `TestItem` declared in a `myproject.items` module:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class TestItem(scrapy.Item):\n",
    "    id = scrapy.Field()\n",
    "    name = scrapy.Field()\n",
    "    description = scrapy.Field()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c43396-6357-470a-badf-e51271c22a39",
   "metadata": {},
   "source": [
    "### CrawlSpider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7690fc-e0c9-4c08-917d-460991431ccd",
   "metadata": {},
   "source": [
    "`class scrapy.spiders.CrawlSpider` [source](https://docs.scrapy.org/en/latest/_modules/scrapy/spiders/crawl.html#CrawlSpider)\n",
    "\n",
    "This is the most commonly used spider for crawling regular websites, as it provides a convenient mechanism for following links by defining a set of rules. It may not be the best suited for your particular web sites or project, but it’s generic enough for several cases, so you can start from it and override it as needed for more custom functionality, or just implement your own spider.\n",
    "\n",
    "Apart from the attributes inherited from `Spider` (that you must specify), this class supports a new attribute:\n",
    "\n",
    "- `rules`\n",
    "\n",
    "Which is a list of one (or more) `Rule` objects. Each `Rule` defines a certain behaviour for crawling the site. Rules objects are described below. If multiple rules match the same link, the first one will be used, according to the order they’re defined in this attribute.\n",
    "\n",
    "This spider also exposes an overridable method:\n",
    "\n",
    "- `parse_start_url(response, **kwargs)` [source](https://docs.scrapy.org/en/latest/_modules/scrapy/spiders/crawl.html#CrawlSpider.parse_start_url)\n",
    "\n",
    "This method is called for each `response` produced for the URLs in the spider’s start_urls attribute. It allows to parse the initial responses and must return either \n",
    "- an `item` object, \n",
    "- a `Request` object, or \n",
    "- an `iterable` containing any of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dc0826-cdd8-48cb-a29f-216808516cc6",
   "metadata": {},
   "source": [
    "#### Crawling rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9c6c38-9176-494c-bf7a-f94f2b339c6e",
   "metadata": {},
   "source": [
    "```python\n",
    "class scrapy.spiders.Rule(link_extractor=None, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None, errback=None)` \n",
    "```\n",
    "[source](https://docs.scrapy.org/en/latest/_modules/scrapy/spiders/crawl.html#Rule)\n",
    "\n",
    "- `link_extractor` is a [Link Extractor](#2.10-Link-Extractors) object which defines how links will be extracted from each crawled page. Each produced link will be used to generate a `Request` object, which will contain the link’s text in its meta dictionary (under the `link_text` key). If omitted, a default link extractor created with no arguments will be used, resulting in all links being extracted.\n",
    "\n",
    "- `callback` is a callable or a string (in which case a method from the spider object with that name will be used) to be called for each link extracted with the specified link extractor. This callback receives a `Response` as its first argument and must return either a single instance or an iterable of [item objects](#2.4-Items) and/or `Request` objects (or any subclass of them). As mentioned above, the received `Response` object will contain the text of the link that produced the `Request` in its meta dictionary (under the `link_text` key)\n",
    "\n",
    "- `cb_kwargs` is a dict containing the keyword arguments to be passed to the callback function.\n",
    "\n",
    "- `follow` is a boolean which specifies if links should be followed from each `response` extracted with this rule. If callback is `None` `follow` defaults to `True`, otherwise it defaults to `False`.\n",
    "\n",
    "- `process_links` is a callable, or a string (in which case a method from the spider object with that name will be used) which will be called for each list of links extracted from each response using the specified `link_extractor`. This is mainly used for filtering purposes.\n",
    "\n",
    "- `process_request` is a callable (or a string, in which case a method from the spider object with that name will be used) which will be called for every `Request` extracted by this rule. This callable should take said request as first argument and the `Response` from which the request originated as second argument. It must return a `Request` object or `None` (to filter out the request).\n",
    "\n",
    "- `errback` is a callable or a string (in which case a method from the spider object with that name will be used) to be called if any exception is raised while processing a request generated by the rule. It receives a `Twisted Failure` instance as first parameter.\n",
    "\n",
    "> <b>Warning</b>: Because of its internal implementation, you must explicitly set callbacks for new requests when writing CrawlSpider-based spiders; unexpected behaviour can occur otherwise.\n",
    "\n",
    "_New in version 2.0: The `errback` parameter._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb12be9-3453-4fa9-9177-1253092329d2",
   "metadata": {},
   "source": [
    "#### CrawlSpider example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8b5c7a-be8e-4899-be00-40556d9a118d",
   "metadata": {},
   "source": [
    "Let’s now take a look at an example `CrawlSpider` with rules:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "\n",
    "\n",
    "class MySpider(CrawlSpider):\n",
    "    name = \"example.com\"\n",
    "    allowed_domains = [\"example.com\"]\n",
    "    start_urls = [\"http://www.example.com\"]\n",
    "\n",
    "    rules = (\n",
    "        # Extract links matching 'category.php' (but not matching 'subsection.php')\n",
    "        # and follow links from them (since no callback means follow=True by default).\n",
    "        Rule(LinkExtractor(allow=(r\"category\\.php\",), deny=(r\"subsection\\.php\",))),\n",
    "        # Extract links matching 'item.php' and parse them with the spider's method parse_item\n",
    "        Rule(LinkExtractor(allow=(r\"item\\.php\",)), callback=\"parse_item\"),\n",
    "    )\n",
    "\n",
    "    def parse_item(self, response):\n",
    "        self.logger.info(\"Hi, this is an item page! %s\", response.url)\n",
    "        item = scrapy.Item()\n",
    "        item[\"id\"] = response.xpath('//td[@id=\"item_id\"]/text()').re(r\"ID: (\\d+)\")\n",
    "        item[\"name\"] = response.xpath('//td[@id=\"item_name\"]/text()').get()\n",
    "        item[\"description\"] = response.xpath(\n",
    "            '//td[@id=\"item_description\"]/text()'\n",
    "        ).get()\n",
    "        item[\"link_text\"] = response.meta[\"link_text\"]\n",
    "        url = response.xpath('//td[@id=\"additional_data\"]/@href').get()\n",
    "        return response.follow(\n",
    "            url, self.parse_additional_page, cb_kwargs=dict(item=item)\n",
    "        )\n",
    "\n",
    "    def parse_additional_page(self, response, item):\n",
    "        item[\"additional_data\"] = response.xpath(\n",
    "            '//p[@id=\"additional_data\"]/text()'\n",
    "        ).get()\n",
    "        return item\n",
    "```\n",
    "\n",
    "This spider would start crawling `example.com`’s home page, collecting category links, and item links, parsing the latter with the `parse_item` method. For each item response, some data will be extracted from the HTML using XPath, and an `Item` will be filled with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba05a7a-7db4-4a02-a329-f3f49ad5cdb0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d977100-d354-4111-b0ba-3c47b01dc83b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e19e123-9f7c-410a-933a-5b801a56ff97",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c79cc364-bab4-4ec7-a406-9fb39123b321",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fdd19ae-e6c6-44e8-8635-114b42a6c59d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6e8b05b-8ef2-4551-b3af-525802c5efd6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a7c41ee-29d6-41dc-b4f6-e11ecc64c8c7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "205b828a-f19c-4691-8c62-fc677a0810c9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c846f7c-3076-4db2-a87d-e424a6cdbfce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2189794a-fbbd-4ded-ad53-6ed1013cb4ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49b0b42d-6347-4a43-aab8-20bb01772048",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18ceefc5-e0af-4ec1-84d5-9584c00f29a2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d6c292e-3009-44bb-90bb-0b5dc96d3a7c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92f7690b-00ad-442f-a3e8-15f6a30b6e77",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79114932-10e2-43ed-940b-caa414176c50",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "185810a8-9745-4192-a975-d2a2ca48c395",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37b4f647-24be-4523-b9bd-23c3c42338ea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4c3f633-4055-4804-ae79-f8a9444b6eef",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83170bb3-b1b3-4fa7-8d0e-fcd84eab63a7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7a60bd0-ad5d-46c6-944e-14d7695864ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f6dcf4a-f99e-49f2-a3ac-049295411656",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe3e123c-2d5e-4d8e-826c-eb7f443738ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a54236f-61d7-4c88-bd69-acb69d1ad4e7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34a2c405-8dc2-45ee-abe1-23ea61d59eb4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c83020f-fc3e-4427-a177-b2df7142013d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7031e3ad-d653-46c9-a687-ac0aad49f74c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6441ac21-65bf-4e7f-9485-bc165e86f97c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c1951ba-9574-437c-9e7c-e0bcd5c1fd0f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27348b79-4b71-4335-bd11-7c53540bf7c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31ee05b0-0c8f-4e6f-8976-49cd2c634a5e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2984623-9c75-4959-a1a1-d178a46e8613",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fc17133-e134-4f4d-b1bc-c8fa3a1cfef1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bf12c77-842b-44aa-800c-18883be943c9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f652e9d9-a6f6-4650-9dc4-c571f587cf6b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc836c1b-9bee-4650-b047-0d18f712b678",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2176a897-1c04-490b-841e-ecebc15eb3cb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ecaa713-653c-4cf7-a6c3-969752d7b1cb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44db82ca-3722-4043-a68a-88d4fa9d30b4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52231378-329c-43a4-8701-fa6137937fde",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60c13c34-91e1-4cd5-a1e9-8ae0aaaa5a54",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e5dc560-4c6d-4e4b-8ebc-1fc575c34659",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "663f94ff-d95c-4053-a8e9-79e867447bbc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bed6e9b-829a-450b-8a36-4aa84906c352",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a23d0878-9763-4b9e-a877-bfdf3bc27d37",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fab8a2b6-fa94-4b24-9c15-f79c06af98bc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42910fa5-47c9-472a-8e0f-dc10a41e4fbd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba37ea4d-a6d0-4698-a277-eeb5c1b012f7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7def67f-f742-431b-ac65-b3ea702e6954",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2bbe11d-9beb-4388-9680-a7c09ef0b2df",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "370fbc1d-37f7-4bf1-ae2b-a484d036a935",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "756c4442-aae3-4181-9445-92355a1282ce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "963fb61a-9fce-4afe-ab7c-c53fb634b853",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2174d96-3070-4a33-a0ea-20e754bf193f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e843363b-eb9c-4253-a47d-b7ad41d2e1ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f22b85b9-2501-4017-b488-d08eb25743f5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74c35179-0fdd-4341-a0a0-0ed82abfd706",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ef4ec3e-18d5-47d8-aea9-3d0610a46435",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db8cc4a6-b05f-44d4-8177-2e67c1aaa102",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99390406-04a2-40a8-90a7-9a3b4e1cd11e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffff4872-52a2-4860-bafd-a062478f3ffa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74a97104-6490-41c9-a0fe-fb8703ef9f71",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0df0a8e5-e6dd-4b6f-9b00-3e4ab13669e9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "077ba53a-e7fb-45b3-97e2-4beae36d7f9e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb848ec9-39d0-42ac-91c4-7b84f37bf252",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3181b307-2f04-496d-a677-c3b96eb5fb60",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eacc428c-31c2-4103-9c17-a04a6a0f1685",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64bd4f4e-438e-47dd-b64c-96e6d94bd196",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "813c96d3-162f-49e5-aecf-38ddd9103128",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8634c1aa-a276-44bd-a6c9-3f40363f9cd0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b6ce8f1-50a8-45e3-8212-0bbc585aaaf2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf199003-b181-4e04-84c3-7e3d7e982f78",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b63e285-5871-4b4c-88b0-839ef9e656bd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c84a1e2e-f440-4a2c-a4e6-0f5d8736ba16",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7bb0f0ba-8443-4793-99e3-0c0d63ac9b5d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aef02e1f-3dc3-41c2-803a-9932d82ee2f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3dbea2aa-6ae9-4ad6-bbe1-5eb0377842a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ef75585-9444-45a2-981b-ea6eb0ac1783",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ba3cefa-e189-4289-8ad2-b0f19bc04755",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f39c2953-e301-4263-9ff4-3853d1d5792d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8e3e31c-431b-4b98-86e2-098d40f3d688",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "700c5ae8-ffb1-4ad0-8291-2ea0c4121364",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3af5fbe0-ca65-4916-911e-c37447109de7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fccd541b-c0a2-42f1-ac25-706608d546d2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecb79f98-fb05-4d0e-9d2d-a6d99bb81814",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5207d4c0-f716-4c41-a63b-c00971935d59",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cb82df8-fe35-4c9b-b98d-4fb12873374e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1877b67a-6b89-4b79-b328-08c48b022de8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8256ddb6-dac5-4108-9894-181ae50d42c5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f3f2ae5-98e6-40dc-b601-e63dd6113382",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e84ac4e9-7f5f-4b33-b604-49aa3d621e28",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "472efb7b-8cbd-463e-bd08-ec33bee6eb1c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7cbeb0e-acf7-4910-bd45-4cc032f7ebd7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc1d34d0-b7e1-4574-828a-d223509a7bb2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a24a664-e8e1-4340-baaa-7d69793bf577",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6718497-b1df-4418-b908-2a19a454bd03",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d672216-6f4c-4d33-8f9d-54e982df129c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09585f29-6d09-4a7b-a1cb-950cb6cb8b40",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b21878e8-7559-4f86-bce2-1a52e07842c2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc9291a8-65ea-4f7b-a65e-5825ebdec945",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a80fa174-b7cc-4239-b555-798a2e16bd3c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd1689b0-2dac-4e39-a78b-2102a852513b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40edd53a-d1d9-4116-92c0-8ba9f3bc841f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ecc5dc1-43e2-4e2d-8740-7160a8163323",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58ac8e3b-9988-432f-ad7f-0977036b9d8e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "837750af-85a2-4ffa-a2c6-c62b1d1b64d7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6015d606-ce34-4e61-8356-3510e7c7d9b6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8640ef78-4b36-4cd2-908d-fcdd2d34662e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3a80c95-bd96-456e-9b43-03fa87873628",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "467cea9d-8aa7-4559-ab74-54bc6d00579f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddc93cf2-696d-4539-89a6-caea293d452c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07cc3a4c-1518-4065-adff-285f8d03c39d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd47149e-ac49-48c9-992b-56cbc1d9272f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "703e4ba1-651e-4f00-99a2-149e7d6f5cb8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2082f614-df62-49b2-9cc2-4579d7e53bf5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9285a32-3415-40b9-8bcc-d5cad5df0106",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8d52766-f930-4c94-8f76-9ff95d79316f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80bf1236-d949-4e58-aa67-1e64679d4d1f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11f5bb3b-e413-4c5b-96b4-6a3e358fe209",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "443d4275-6b2d-478f-a46f-4bb058e4d02a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "632916b5-e088-46d5-90f2-256683b10555",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f2e2bc8-6065-407f-84a3-737e485eec91",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72102130-3c38-498f-b557-1f9ddd237e4f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c396704-0a72-458d-a053-d250c1789065",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "282fedd4-15f6-4dd6-8981-219ddd7c7699",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "251fc71a-48e6-40a2-9e85-6919f3f8383f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f9884e0-bf7f-4b84-84e9-d7b770c4bcaa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1481a87f-402e-4a60-ab24-4c102fe73124",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d25b01c-5051-4075-8705-4fca893f8aa5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64a13653-8f21-4b50-8525-dfde3428d151",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8dea94e1-ccdf-4ddc-bca5-07ce0eef88aa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "822881be-d4be-4f2a-88b2-b108ac30c3e6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90028eae-181d-4549-913b-dec120bd491d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cc3c57a-887b-422c-bb55-8efc57ebe40f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92f23c25-862a-4414-94a1-5d2f37bdadcf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b49ddc06-1291-4361-94b4-fc5a8c0e7664",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1976517-6bcd-4e8c-ae9a-1eab4a2e944a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "588f33f1-5628-47bb-91f8-3a3193cc085b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac71d957-d161-4881-ac2e-ea9b06ab0b11",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb933203-edda-48ca-8ca5-7873c6f5c247",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf53ce88-8b56-4fba-9cfa-78aad57b9775",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "130fc6cf-34b2-4c0a-a297-67e17a9076ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83d30708-5834-46f4-bbe3-b2a5a6788833",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2.3 Selectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad2efea-b857-4781-bf32-6605780aae57",
   "metadata": {},
   "source": [
    "## Selecting element attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf2385d-9394-4362-b6c2-7e0480e91e88",
   "metadata": {},
   "source": [
    "# 2.4 Items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f1426d-a4ed-4c61-99e0-b370875b6d02",
   "metadata": {},
   "source": [
    "The main goal in scraping is to extract structured data from unstructured sources, typically, web pages. Spiders may return the extracted data as **items**, Python objects that define **key-value pairs**.\n",
    "\n",
    "Scrapy supports multiple types of items. When you create an item, you may use whichever type of item you want. When you write code that receives an item, your code should work for any item type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0721c01-10b3-47ea-afb2-c8a9989732ee",
   "metadata": {},
   "source": [
    "## Item Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a28f229-6272-41cc-9c85-458cdd775d9e",
   "metadata": {},
   "source": [
    "Scrapy supports the following types of items, via the [itemadapter library](https://github.com/scrapy/itemadapter): \n",
    "- dictionaries, \n",
    "- `Item` objects, \n",
    "- `dataclass` objects, and \n",
    "- `attrs` objects.\n",
    "\n",
    "_ChatGPT:_  \n",
    "Let's discuss the best practices for choosing among these options:\n",
    "\n",
    "- **Dictionaries**: They are the most basic and widely used option. They are suitable for simple scraping tasks where the data structure is straightforward and doesn't require much validation or additional functionality. They are also more lightweight compared to defining custom classes.\n",
    "\n",
    "- **Item objects**: These are recommended for more complex scraping tasks where you need to enforce a specific data structure, perform validation, or implement custom methods for your items. If you prefer a more object-oriented approach and want to leverage features like inheritance and encapsulation, defining Item classes is a good choice.\n",
    "\n",
    "- **Dataclass objects** and **Attrs objects**: These options provide similar benefits, offering a more concise syntax for defining classes with attributes compared to traditional classes. They are suitable when you want a more structured approach compared to dictionaries but prefer a lightweight alternative to defining full-fledged Item classes. Choosing between dataclasses and attrs largely depends on your familiarity with each approach and any specific features offered by either library that you find beneficial.\n",
    "\n",
    "In summary, the best practice for choosing the type of items in Scrapy depends on the complexity of your scraping task, your preference for syntax and features, and your familiarity with the available options. For simpler tasks, dictionaries may suffice, while for more complex tasks, defining custom Item classes, dataclasses, or attrs classes may be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1472f3-b6bc-4c56-83e9-e3f31805138a",
   "metadata": {},
   "source": [
    "### Dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011146ba-009c-4daf-9c10-3b5e0814476b",
   "metadata": {},
   "source": [
    "As an item type, `dict` is convenient and familiar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4e4da2-c98b-4bd9-91d2-236a1fa9877b",
   "metadata": {},
   "source": [
    "### `Item` objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69e308d-7225-4046-9c26-f56dfb51f76f",
   "metadata": {},
   "source": [
    "`Item` provides a `dict`-like API plus additional features that make it the most feature-complete item type:\n",
    "\n",
    "```python\n",
    "class scrapy.item.Item([arg])\n",
    "class scrapy.Item([arg])\n",
    "```\n",
    "\n",
    "- `Item` objects replicate the standard `dict` API, including its `__init__` method.\n",
    "- `Item` allows defining field names, so that:\n",
    "    - `KeyError` is raised when using undefined field names (i.e. prevents typos going unnoticed)\n",
    "    - [Item exporters](#5.8-Item-Exporters) can export all fields by default even if the first scraped object does not have values for all of them\n",
    "- `Item` also allows defining field metadata, which can be used to customize [serialization](#Serialization-of-item-fields).\n",
    "- `trackref` tracks `Item` objects to help find memory leaks (see [Debugging memory leaks](#Debugging-memory-leaks-with-trackref) with trackref).\n",
    "- Item objects also provide the following additional API members:\n",
    "    - `Item.copy()`\n",
    "    - `Item.deepcopy()`: Return a `deepcopy()` of this item.\n",
    "    - `fields`: A dictionary containing all declared fields for this `Item`, not only those populated. The keys are the field names and the values are the `Field` objects used in the Item declaration.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "from scrapy.item import Item, Field\n",
    "\n",
    "\n",
    "class CustomItem(Item):\n",
    "    one_field = Field()\n",
    "    another_field = Field()\n",
    "```\n",
    "\n",
    "_ChatGPT:_  \n",
    "In the context of Scrapy, `Field()` objects are used within the `scrapy.Item` subclass definitions. `Field()` is not a datatype in the traditional sense. It's a class provided by Scrapy for defining the structure of items in your spider. Internally, Scrapy uses `Field()` instances to manage and validate the data that is scraped and stored.\n",
    "\n",
    "When you define an `Item` subclass in Scrapy, you're essentially creating a structured container to hold the data you want to scrape from web pages. Each piece of data you want to extract from a web page corresponds to a field in your Item subclass.\n",
    "\n",
    "The flexibility of `Field()` objects allows you to handle a wide variety of data types encountered during web scraping. When defining your Scrapy Item subclasses, you should choose the most appropriate data type for each field based on the nature of the data you're scraping.\n",
    "\n",
    "Here's a basic example of how you might define a Scrapy Item subclass with fields of different data types:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class ProductItem(scrapy.Item):\n",
    "    name = scrapy.Field()             # String\n",
    "    price = scrapy.Field()            # Float\n",
    "    in_stock = scrapy.Field()         # Boolean\n",
    "    tags = scrapy.Field()             # List\n",
    "    attributes = scrapy.Field()       # Dictionary\n",
    "    release_date = scrapy.Field()     # Date\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a40729-7e3a-49e6-9813-0a14d0c7cda3",
   "metadata": {},
   "source": [
    "### `Dataclass` objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7c8e25-1ea4-4b30-a196-747c8fd3b939",
   "metadata": {},
   "source": [
    "_New in version 2.2._  \n",
    "[`dataclass()`](https://docs.python.org/3/library/dataclasses.html#dataclasses.dataclass) allows defining item classes with field names, so that [item exporters](#5.8-Item-Exporters) can export all fields by default even if the first scraped object does not have values for all of them.\n",
    "\n",
    "Additionally, `dataclass` items also allow to:\n",
    "- define the type and default value of each defined field.\n",
    "- define custom field metadata through `dataclasses.field()`, which can be used to [customize serialization](#Serialization-of-item-fields).\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CustomItem:\n",
    "    one_field: str\n",
    "    another_field: int\n",
    "```\n",
    "\n",
    "> _Note:_ Field types are not enforced at run time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5e5964-5755-48a9-a585-6d32b0721556",
   "metadata": {},
   "source": [
    "### `attr.s` objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c80299-6dd3-45f3-95f9-b5560f80a28d",
   "metadata": {},
   "source": [
    "_New in version 2.2._  \n",
    "[`attr.s()`](https://www.attrs.org/en/stable/api-attr.html#attr.s) allows defining item classes with field names, so that [item exporters](#5.8-Item-Exporters) can export all fields by default even if the first scraped object does not have values for all of them.\n",
    "\n",
    "Additionally, attr.s items also allow to:\n",
    "- define the type and default value of each defined field.\n",
    "- define custom field [metadata](https://www.attrs.org/en/stable/examples.html#metadata), which can be used to [customize serialization](#Serialization-of-item-fields).\n",
    "\n",
    "In order to use this type, the [attrs package](https://www.attrs.org/en/stable/index.html) needs to be installed.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "import attr\n",
    "\n",
    "\n",
    "@attr.s\n",
    "class CustomItem:\n",
    "    one_field = attr.ib()\n",
    "    another_field = attr.ib()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022b4fff-b5a1-43c7-9035-5bab9627118a",
   "metadata": {},
   "source": [
    "## Working with `Item` objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbe41bd-85db-4931-877b-bfc3302ecb0f",
   "metadata": {},
   "source": [
    "### Declaring `Item` subclasses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2675dc9a-9a82-4009-840a-bf35cea1d87c",
   "metadata": {},
   "source": [
    "`Item` subclasses are declared using a simple `class` definition syntax and `Field` objects. Here is an example:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class Product(scrapy.Item):\n",
    "    name = scrapy.Field()\n",
    "    price = scrapy.Field()\n",
    "    stock = scrapy.Field()\n",
    "    tags = scrapy.Field()\n",
    "    last_updated = scrapy.Field(serializer=str)\n",
    "```\n",
    "\n",
    "> _Note:_ Those familiar with Django will notice that Scrapy `Items` are declared similar to [Django Models](https://docs.djangoproject.com/en/dev/topics/db/models/), except that Scrapy `Items` are much simpler as there is no concept of different field types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a9c6bd-939b-4198-aa90-0429561e6117",
   "metadata": {},
   "source": [
    "### Declaring `fields`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c97ede-cdbb-47ad-8d90-89e8d67c5e38",
   "metadata": {},
   "source": [
    "`Field` objects are used to specify metadata for each field. For example, the serializer function for the `last_updated` field illustrated in the example above.\n",
    "\n",
    "You can specify any kind of metadata for each field. There is no restriction on the values accepted by `Field` objects. For this same reason, there is no reference list of all available metadata keys. Each key defined in `Field` objects could be used by a different component, and only those components know about it. You can also define and use any other `Field` key in your project too, for your own needs. \n",
    "\n",
    "> The main goal of `Field` objects is to provide a way to define all `field` metadata in one place. \n",
    "\n",
    "Typically, those components whose behaviour depends on each field use certain field keys to configure that behaviour. You must refer to their documentation to see which metadata keys are used by each component.\n",
    "\n",
    "It’s important to note that the `Field` objects used to declare the `item` do not stay assigned as class attributes. Instead, they can be accessed through the `Item.fields` attribute.\n",
    "\n",
    "```python\n",
    "class scrapy.item.Field([arg])\n",
    "class scrapy.Field([arg])\n",
    "```\n",
    "\n",
    "The `Field` class is just an alias to the built-in `dict` class and doesn’t provide any extra functionality or attributes. In other words, `Field` objects are plain-old Python dicts. A separate class is used to support the `item` declaration syntax based on class attributes.\n",
    "\n",
    "> _Note:_ `Field` metadata can also be declared for `dataclass` and `attrs` items. Please refer to the documentation for [`dataclasses.field`](https://docs.python.org/3/library/dataclasses.html#dataclasses.field) and [`attr.ib`](https://www.attrs.org/en/stable/api.html#attr.ib) for additional information.\n",
    "\n",
    "_ChatGPT:_  \n",
    "In Scrapy, the components the authors are referring to are likely the various parts of the framework that utilize the metadata associated with Field objects.\n",
    "\n",
    "Here's a breakdown:\n",
    "\n",
    "- **Field objects**: In Scrapy, Field objects are used to define the fields (or attributes) of items that will be scraped from web pages. These fields can represent various types of data such as strings, integers, dates, etc. Each field can have associated metadata, which provides additional information or instructions about how the field should be processed or handled.\n",
    "\n",
    "- **Components**: Scrapy is composed of several components that work together to facilitate the scraping process. These components include spiders, pipelines, middlewares, and extensions, among others. Each of these components may utilize the metadata associated with Field objects in different ways to customize the scraping behavior or perform additional processing on the scraped data.\n",
    "\n",
    "- **Metadata keys**: Metadata keys are the specific attributes or properties that can be associated with Field objects to provide additional information. Examples of metadata keys include xpath, css, serializer, output_processor, etc. These keys can be used to specify how the field should be extracted from the web page, processed, or serialized.\n",
    "\n",
    "- **Customization**: The authors are emphasizing that Scrapy allows for flexibility and customization by allowing users to define and use any metadata keys they need for their specific project requirements. This means that users can define their own metadata keys and use them within their spiders, pipelines, or other components to customize the scraping process according to their needs.\n",
    "\n",
    "Overall, the authors are highlighting the flexibility and extensibility of Scrapy, allowing users to define and use metadata keys as needed across various components of their scraping projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148025fd-b4f8-44bb-861f-feb6f926d482",
   "metadata": {},
   "source": [
    "Here are some examples of common tasks performed with items, using the `Product` item declared above. You will notice the API is very similar to the dict API.\n",
    "\n",
    "```python\n",
    "class Product(scrapy.Item):\n",
    "    name = scrapy.Field()\n",
    "    price = scrapy.Field()\n",
    "    stock = scrapy.Field()\n",
    "    tags = scrapy.Field()\n",
    "    last_updated = scrapy.Field(serializer=str)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01ad9b2-283b-45e1-a735-d512441d4b7f",
   "metadata": {},
   "source": [
    "### Creating `item`s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9acbea1-51ea-4b0e-a2cf-20617cde027f",
   "metadata": {},
   "source": [
    "```python\n",
    "product = Product(name=\"Desktop PC\", price=1000)\n",
    "print(product)\n",
    "```\n",
    "```\n",
    "Product(name='Desktop PC', price=1000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bd56fd-50ee-4806-bd85-6d1fce32cebc",
   "metadata": {},
   "source": [
    "### Getting `field` values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20932cf-2660-4928-ac3a-443348c8f930",
   "metadata": {},
   "source": [
    "```python\n",
    "product[\"name\"]\n",
    "Desktop PC\n",
    "\n",
    "product.get(\"name\")\n",
    "Desktop PC\n",
    "\n",
    "product[\"price\"]\n",
    "1000\n",
    "\n",
    "product[\"last_updated\"]\n",
    "Traceback (most recent call last):\n",
    "    ...\n",
    "KeyError: 'last_updated'\n",
    "\n",
    "product.get(\"last_updated\", \"not set\")\n",
    "not set\n",
    "\n",
    "product[\"lala\"]  # getting unknown field\n",
    "Traceback (most recent call last):\n",
    "    ...\n",
    "KeyError: 'lala'\n",
    "\n",
    "product.get(\"lala\", \"unknown field\")\n",
    "'unknown field'\n",
    "\n",
    "\"name\" in product  # is name field populated?\n",
    "True\n",
    "\n",
    "\"last_updated\" in product  # is last_updated populated?\n",
    "False\n",
    "\n",
    "\"last_updated\" in product.fields  # is last_updated a declared field?\n",
    "True\n",
    "\n",
    "\"lala\" in product.fields  # is lala a declared field?\n",
    "False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a473abd-58e2-4dbd-b78d-9d832829a8ca",
   "metadata": {},
   "source": [
    "### Setting `field` values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ff7960-9c12-49d4-8e5d-30c990a40aab",
   "metadata": {},
   "source": [
    "```python\n",
    "product[\"last_updated\"] = \"today\"\n",
    "product[\"last_updated\"]\n",
    "today\n",
    "\n",
    "product[\"lala\"] = \"test\"  # setting unknown field\n",
    "Traceback (most recent call last):\n",
    "    ...\n",
    "KeyError: 'Product does not support field: lala'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e15adda-c29f-42bf-a44a-e0cdfc4204b6",
   "metadata": {},
   "source": [
    "### Accessing all populated values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975a840e-9efa-4678-a1e4-571773ae9ec5",
   "metadata": {},
   "source": [
    "To access all populated values, just use the typical dict API:\n",
    "\n",
    "```python\n",
    "product.keys()\n",
    "['price', 'name']\n",
    "\n",
    "product.items()\n",
    "[('price', 1000), ('name', 'Desktop PC')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c0b5f4-90a9-47d1-9324-91e820c43a64",
   "metadata": {},
   "source": [
    "### Copying `item`s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b8e4bb-b9dd-42c6-835a-c7044a309501",
   "metadata": {},
   "source": [
    "To copy an item, you must first decide whether you want a **shallow copy** (by reference) or a **deep copy** (by value).\n",
    "\n",
    "If your item contains mutable values like lists or dictionaries, a _shallow copy will keep references_ to the same mutable values across all different copies.\n",
    "\n",
    "For example, if you have an item with a list of tags, and you create a shallow copy of that item, both the original item and the copy have the same list of tags. Adding a tag to the list of one of the items will add the tag to the other item as well.\n",
    "\n",
    "If that is not the desired behavior, use a deep copy instead.\n",
    "\n",
    "See [`copy`](https://docs.python.org/3/library/copy.html#module-copy) for more information.\n",
    "\n",
    "To create a shallow copy of an item, you can either call `copy()` on an existing item (`product2 = product.copy()`) or instantiate your `item` class from an existing item (`product2 = Product(product)`).\n",
    "\n",
    "To create a deep copy, call `deepcopy()` instead (`product2 = product.deepcopy()`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d581c0f5-cf3b-4733-8935-2ac5d4eee3ae",
   "metadata": {},
   "source": [
    "### Other common tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75494e52-ce3f-4604-a501-3f63d4804805",
   "metadata": {},
   "source": [
    "Creating dicts from items:\n",
    "\n",
    "```python\n",
    "dict(product)  # create a dict from all populated values\n",
    "{'price': 1000, 'name': 'Desktop PC'}\n",
    "```\n",
    "\n",
    "Creating items from dicts:\n",
    "```python\n",
    "Product({\"name\": \"Laptop PC\", \"price\": 1500})\n",
    "Product(price=1500, name='Laptop PC')\n",
    "\n",
    "Product({\"name\": \"Laptop PC\", \"lala\": 1500})  # warning: unknown field in dict\n",
    "Traceback (most recent call last):\n",
    "    ...\n",
    "KeyError: 'Product does not support field: lala'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6fdb99-d011-4eec-b42c-340dbd6c8bb2",
   "metadata": {},
   "source": [
    "### Extending `Item` subclasses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6c1705-65fa-4fd4-8e1d-b6f94dd15d66",
   "metadata": {},
   "source": [
    "You can extend `Item`s (to add more fields or to change some metadata for some fields) by declaring a subclass of your original Item.\n",
    "\n",
    "For example:\n",
    "```python\n",
    "class DiscountedProduct(Product):\n",
    "    discount_percent = scrapy.Field(serializer=str)\n",
    "    discount_expiration_date = scrapy.Field()\n",
    "```\n",
    "\n",
    "You can also extend field metadata by using the previous field metadata and appending more values, or changing existing values, like this:\n",
    "```python\n",
    "class SpecificProduct(Product):\n",
    "    name = scrapy.Field(Product.fields[\"name\"], serializer=my_serializer)\n",
    "```\n",
    "\n",
    "That adds (or replaces) the serializer metadata key for the `name` field, keeping all the previously existing metadata values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72eb04c-c511-4310-ac10-3d099118e021",
   "metadata": {},
   "source": [
    "## Supporting All `Item` Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5202bf46-e09f-4255-8ced-38b0f6d52c2e",
   "metadata": {},
   "source": [
    "In code that receives an `item`, such as methods of [item pipelines](#2.7-Item-Pipeline) or [spider middlewares](#5.4-Spider-Middleware), it is a good practice to use the `ItemAdapter` class and the `is_item()` function to write code that works for any supported item type:\n",
    "\n",
    "- `class itemadapter.ItemAdapter(item: Any)` [source](https://docs.scrapy.org/en/latest/_modules/itemadapter/adapter.html#ItemAdapter): \n",
    "    - Wrapper class to interact with data container objects. It provides a common interface to extract and set data without having to take the object’s type into account.\n",
    "- `itemadapter.is_item(obj: Any)→ bool` [source](https://docs.scrapy.org/en/latest/_modules/itemadapter/utils.html#is_item): \n",
    "    - Return `True` if the given object belongs to one of the supported types, `False` otherwise. Alias for `ItemAdapter.is_item`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9fca74-c362-4813-b1e4-acc866013d13",
   "metadata": {},
   "source": [
    "- `class scrapy.item.ItemMeta(class_name, bases, attrs)` [source](https://docs.scrapy.org/en/latest/_modules/scrapy/item.html#ItemMeta): \n",
    "    - [Metaclass](https://realpython.com/python-metaclasses) of `Item` that handles field definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72b46f0-d92e-4954-b501-34d0e7f3d77a",
   "metadata": {},
   "source": [
    "# 2.5 Item Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63493c25-683c-40dd-a5b7-0709bb5550ab",
   "metadata": {},
   "source": [
    "**Item Loaders** provide a convenient mechanism for populating scraped items. Even though items can be populated directly, Item Loaders provide a much more convenient API for populating them from a scraping process, by automating some common tasks like parsing the raw extracted data before assigning it.\n",
    "\n",
    "In other words, \n",
    "\n",
    "> items provide the _container_ of scraped data, while Item Loaders provide the mechanism for _populating_ that container.\n",
    "\n",
    "Item Loaders are designed to provide a flexible, efficient and easy mechanism for extending and overriding different `field` parsing rules, either by spider, or by source format (`HTML`, `XML`, etc) without becoming a nightmare to maintain.\n",
    "\n",
    "> _Note:_ Item Loaders are an extension of the [itemloaders library](https://itemloaders.readthedocs.io/en/latest/) that make it easier to work with Scrapy by adding support for [responses](#2.9-Requests-and-Responses)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090e3c87-d58c-4b42-a775-e9c3d4ea270c",
   "metadata": {},
   "source": [
    "## Using Item Loaders to populate items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0adb11-5b3b-43b1-9007-e967fd0beb47",
   "metadata": {},
   "source": [
    "To use an Item Loader, you must first instantiate it. You can either instantiate it with an `item` object or without one, in which case an `item` object is automatically created in the Item Loader `__init__` method using the `item` class specified in the `ItemLoader.default_item_class` attribute.\n",
    "\n",
    "Then, you start collecting values into the Item Loader, typically using [Selectors](#2.3-Selectors). You can add more than one value to the same item `field`; the Item Loader will know how to “join” those values later using a proper processing function.\n",
    "\n",
    "> _Note:_ Collected data is internally stored as `list`s, allowing to add several values to the same `field`. If an item argument is passed when creating a loader, each of the item’s values will be stored as-is if it’s already an iterable, or wrapped with a list if it’s a single value.\n",
    "\n",
    "Here is a typical Item Loader usage in a Spider, using the `Product` item declared in the Items chapter:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class Product(scrapy.Item):\n",
    "    name = scrapy.Field()\n",
    "    price = scrapy.Field()\n",
    "    stock = scrapy.Field()\n",
    "    tags = scrapy.Field()\n",
    "    last_updated = scrapy.Field(serializer=str)\n",
    "```\n",
    "\n",
    "```python\n",
    "from scrapy.loader import ItemLoader\n",
    "from myproject.items import Product\n",
    "\n",
    "\n",
    "def parse(self, response):\n",
    "    l = ItemLoader(item=Product(), response=response)\n",
    "    l.add_xpath(\"name\", '//div[@class=\"product_name\"]')\n",
    "    l.add_xpath(\"name\", '//div[@class=\"product_title\"]')\n",
    "    l.add_xpath(\"price\", '//p[@id=\"price\"]')\n",
    "    l.add_css(\"stock\", \"p#stock\")\n",
    "    l.add_value(\"last_updated\", \"today\")  # you can also use literal values\n",
    "    return l.load_item()\n",
    "```\n",
    "\n",
    "By quickly looking at that code, we can see the `name` field is being extracted from two different XPath locations in the page:\n",
    "\n",
    "```\n",
    "//div[@class=\"product_name\"]\n",
    "\n",
    "//div[@class=\"product_title\"]\n",
    "```\n",
    "\n",
    "In other words, data is being collected by extracting it from two XPath locations, using the `add_xpath()` method. This is the data that will be assigned to the `name` field later.\n",
    "\n",
    "Afterwards, similar calls are used for `price` and `stock` fields (the latter using a CSS selector with the `add_css()` method), and finally the `last_update` field is populated directly with a literal value (`today`) using a different method: `add_value()`.\n",
    "\n",
    "Finally, when all data is collected, the `ItemLoader.load_item()` method is called which actually returns the `item` populated with the data previously extracted and collected with the `add_xpath()`, `add_css()`, and `add_value()` calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2de0ab-b359-46c0-8db4-734538db2a46",
   "metadata": {},
   "source": [
    "## Working with `dataclass` items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e47f4d4-3bf3-4407-82f3-f209838386f7",
   "metadata": {},
   "source": [
    "By default, `dataclass` items require all fields to be passed when created. This could be an issue when using `dataclass` items with item loaders: \n",
    "- unless a pre-populated item is passed to the loader, fields will be populated incrementally using the loader’s `add_xpath()`, `add_css()` and `add_value()` methods.\n",
    "\n",
    "One approach to overcome this is to define items using the `field()` function, with a default argument:\n",
    "\n",
    "```python\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class InventoryItem:\n",
    "    name: Optional[str] = field(default=None)\n",
    "    price: Optional[float] = field(default=None)\n",
    "    stock: Optional[int] = field(default=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2220ce24-0325-47b8-96cb-b8798d7348a1",
   "metadata": {},
   "source": [
    "## Input and Output processors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49470cb-b2bf-4057-a6e7-d4137709b106",
   "metadata": {},
   "source": [
    "An Item Loader contains one **input processor** and one **output processor** for each (`item`) `field`. \n",
    "\n",
    "The input processor processes the extracted data as soon as it’s received (through the `add_xpath()`, `add_css()` or `add_value()` methods) and the result of the input processor is collected and kept inside the `ItemLoader`. \n",
    "\n",
    "After collecting all data, the `ItemLoader.load_item()` method is called to populate and get the populated `item` object. That’s when the output processor is called with the data previously collected (and processed using the input processor). The result of the output processor is the final value that gets assigned to the `item`.\n",
    "\n",
    "Let’s see an example to illustrate how the input and output processors are called for a particular `field` (the same applies for any other field):\n",
    "\n",
    "```python\n",
    "l = ItemLoader(Product(), some_selector)\n",
    "l.add_xpath(\"name\", xpath1)  # (1)\n",
    "l.add_xpath(\"name\", xpath2)  # (2)\n",
    "l.add_css(\"name\", css)  # (3)\n",
    "l.add_value(\"name\", \"test\")  # (4)\n",
    "return l.load_item()  # (5)\n",
    "```\n",
    "\n",
    "So what happens is:\n",
    "1. Data from `xpath1` is extracted, and passed through the input processor of the `name` field. The result of the input processor is collected and kept in the Item Loader (but not yet assigned to the item).\n",
    "1. Data from `xpath2` is extracted, and passed through the same input processor used in (`1`). The result of the input processor is appended to the data collected in (`1`) (if any).\n",
    "1. This case is similar to the previous ones, except that the data is extracted from the css `CSS` selector, and passed through the same input processor used in (`1`) and (`2`). The result of the input processor is appended to the data collected in (`1`) and (`2`) (if any).\n",
    "1. This case is also similar to the previous ones, except that the value to be collected is assigned directly, instead of being extracted from a `XPath` expression or a `CSS` selector. However, the value is still passed through the input processors. In this case, since the value is not iterable it is converted to an iterable of a single element before passing it to the input processor, because input processor always receive iterables.\n",
    "1. The data collected in steps (`1`), (`2`), (`3`) and (`4`) is passed through the output processor of the `name` field. The result of the output processor is the value assigned to the `name` field in the `item`.\n",
    "\n",
    "It’s worth noticing that processors are just callable objects, which are called with the data to be parsed, and return a parsed value. So you can use any function as input or output processor. The only requirement is that they must accept one (and only one) positional argument, which will be an iterable.\n",
    "\n",
    "_Changed in version 2.0: Processors no longer need to be methods._\n",
    "\n",
    "> _Note:_ Both input and output processors must receive an iterable as their first argument. The output of those functions can be anything. The result of input processors will be appended to an internal list (in the `Loader`) containing the collected values (for that `field`). The result of the output processors is the value that will be finally assigned to the `item`.\n",
    "\n",
    "The other thing you need to keep in mind is that the values returned by input processors are collected internally (in lists) and then passed to output processors to populate the fields.\n",
    "\n",
    "Last, but not least, itemloaders come with some [commonly used processors](https://itemloaders.readthedocs.io/en/latest/built-in-processors.html#built-in-processors) built-in for convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013f7594-f771-46a9-8ba3-1e1fd36a6115",
   "metadata": {},
   "source": [
    "### Available [built-in processors](https://itemloaders.readthedocs.io/en/latest/built-in-processors.html#built-in-processors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa887070-6599-4636-a45a-e059e5e128c4",
   "metadata": {},
   "source": [
    "Even though you can use any callable function as input and output processors, itemloaders provides some commonly used processors, which are described below.\n",
    "\n",
    "Some of them, like the `MapCompose` (which is typically used as input processor) compose the output of several functions executed in order, to produce the final parsed value.\n",
    "\n",
    "This module provides some commonly used processors for Item Loaders.\n",
    "\n",
    "See documentation in `docs/topics/loaders.rst`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1081d97a-13ab-4b67-b850-b7e3addff7d0",
   "metadata": {},
   "source": [
    "#### `Compose`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296548b8-56af-449c-a452-6c092f1adb30",
   "metadata": {},
   "source": [
    "`class itemloaders.processors.Compose(*functions, **default_loader_context)` [source](https://itemloaders.readthedocs.io/en/latest/_modules/itemloaders/processors.html#Compose)\n",
    "\n",
    "A processor which is constructed from the composition of the given functions. This means that each input value of this processor is passed to the first function, and the result of that function is passed to the second function, and so on, until the last function returns the output value of this processor.\n",
    "\n",
    "By default, stop process on `None` value. This behaviour can be changed by passing keyword argument `stop_on_none=False`.\n",
    "\n",
    "Example:\n",
    "\n",
    "```ipython\n",
    ">>> from itemloaders.processors import Compose\n",
    ">>> proc = Compose(lambda v: v[0], str.upper)\n",
    ">>> proc(['hello', 'world'])\n",
    "'HELLO'\n",
    "```\n",
    "\n",
    "Each function can optionally receive a [`loader_context`](https://itemloaders.readthedocs.io/en/latest/loaders-context.html#loaders-context) parameter. For those which do, this processor will pass the currently active `Loader` context through that parameter.\n",
    "\n",
    "The keyword arguments passed in the `__init__` method are used as the default Loader context values passed to each function call. However, the final `Loader` context values passed to functions are overridden with the currently active `Loader` context accessible through the `ItemLoader.context` attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07da42ee-ab34-4a1e-a82b-7b1ba491db7f",
   "metadata": {},
   "source": [
    "#### `Identity`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef978993-c814-415d-b080-c9d1e3a3bc83",
   "metadata": {},
   "source": [
    "`class itemloaders.processors.Identity` [source](https://itemloaders.readthedocs.io/en/latest/_modules/itemloaders/processors.html#Identity)\n",
    "\n",
    "The simplest processor, which doesn’t do anything. It returns the original values unchanged. It doesn’t receive any `__init__` method arguments, nor does it accept Loader contexts.\n",
    "\n",
    "Example:\n",
    "\n",
    "```ipython\n",
    ">>> from itemloaders.processors import Identity\n",
    ">>> proc = Identity()\n",
    ">>> proc(['one', 'two', 'three'])\n",
    "['one', 'two', 'three']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbf3557-8fc3-4bdf-9efe-44be3d331757",
   "metadata": {},
   "source": [
    "#### `Join`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0e1e93-0fb6-4daa-8e1a-63617420db65",
   "metadata": {},
   "source": [
    "`class itemloaders.processors.Join(separator=' ')` [source](https://itemloaders.readthedocs.io/en/latest/_modules/itemloaders/processors.html#Join)\n",
    "\n",
    "Returns the values joined with the separator given in the `__init__` method, which defaults to `' '`. It doesn’t accept **Loader contexts**.\n",
    "\n",
    "When using the default separator, this processor is equivalent to the function: `' '.join`.\n",
    "\n",
    "Examples:\n",
    "\n",
    "```ipython\n",
    ">>> from itemloaders.processors import Join\n",
    ">>> proc = Join()\n",
    ">>> proc(['one', 'two', 'three'])\n",
    "'one two three'\n",
    ">>> proc = Join('<br>')\n",
    ">>> proc(['one', 'two', 'three'])\n",
    "'one<br>two<br>three'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb782ce-6c50-43da-a9a6-d592a21f46fc",
   "metadata": {},
   "source": [
    "#### `MapCompose`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6662a3-3f28-4d0d-91ee-ff4e8a6b62e1",
   "metadata": {},
   "source": [
    "`class itemloaders.processors.MapCompose(*functions, **default_loader_context)` [source](https://itemloaders.readthedocs.io/en/latest/_modules/itemloaders/processors.html#MapCompose)\n",
    "\n",
    "A processor which is constructed from the composition of the given functions, similar to the `Compose` processor. The difference with this processor is the way internal results are passed among functions, which is as follows:\n",
    "\n",
    "The input value of this processor is iterated and the first function is applied to each element. The results of these function calls (one for each element) are concatenated to construct a new iterable, which is then used to apply the second function, and so on, until the last function is applied to each value of the list of values collected so far. The output values of the last function are concatenated together to produce the output of this processor.\n",
    "\n",
    "Each particular function can return a value or a list of values, which is flattened with the list of values returned by the same function applied to the other input values. The functions can also return `None` in which case the output of that function is ignored for further processing over the chain.\n",
    "\n",
    "This processor provides a convenient way to compose functions that only work with single values (instead of iterables). For this reason the `MapCompose` processor is typically used as input processor, since data is often extracted using the `extract()` method of [parsel selectors](https://parsel.readthedocs.io/en/latest/parsel.html#parsel.selector.Selector.extract), which returns a list of unicode strings.\n",
    "\n",
    "The example below should clarify how it works:\n",
    "\n",
    "```ipython\n",
    ">>> def filter_world(x):\n",
    "...     return None if x == 'world' else x\n",
    "...\n",
    ">>> from itemloaders.processors import MapCompose\n",
    ">>> proc = MapCompose(filter_world, str.upper)\n",
    ">>> proc(['hello', 'world', 'this', 'is', 'something'])\n",
    "['HELLO', 'THIS', 'IS', 'SOMETHING']\n",
    "```\n",
    "\n",
    "As with the `Compose` processor, functions can receive Loader contexts, and `__init__` method keyword arguments are used as default context values. See `Compose` processor for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917a0f59-95c6-4e95-9dee-c4798199b217",
   "metadata": {},
   "source": [
    "#### `SelectJmes`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a1c928-20b9-495f-af4f-808f5b7af398",
   "metadata": {},
   "source": [
    "`class itemloaders.processors.SelectJmes(json_path)` [source](https://itemloaders.readthedocs.io/en/latest/_modules/itemloaders/processors.html#SelectJmes)\n",
    "\n",
    "Query the input string for the `jmespath` (given at instantiation), and return the answer Requires : [jmespath](https://github.com/jmespath/jmespath) \n",
    "\n",
    "> _Note:_ `SelectJmes` accepts only one input element at a time.\n",
    "\n",
    "Example:\n",
    "\n",
    "```ipython\n",
    ">>> from itemloaders.processors import SelectJmes, Compose, MapCompose\n",
    ">>> proc = SelectJmes(\"foo\") #for direct use on lists and dictionaries\n",
    ">>> proc({'foo': 'bar'})\n",
    "'bar'\n",
    ">>> proc({'foo': {'bar': 'baz'}})\n",
    "{'bar': 'baz'}\n",
    "```\n",
    "\n",
    "Working with Json:\n",
    "\n",
    "```ipython\n",
    ">>> import json\n",
    ">>> proc_single_json_str = Compose(json.loads, SelectJmes(\"foo\"))\n",
    ">>> proc_single_json_str('{\"foo\": \"bar\"}')\n",
    "'bar'\n",
    ">>> proc_json_list = Compose(json.loads, MapCompose(SelectJmes('foo')))\n",
    ">>> proc_json_list('[{\"foo\":\"bar\"}, {\"baz\":\"tar\"}]')\n",
    "['bar']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a48e076-bb64-4b60-91e1-2e6fa79ee308",
   "metadata": {},
   "source": [
    "#### `TakeFirst`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570a8757-bb83-4d17-b9b3-87bc98b6fe20",
   "metadata": {},
   "source": [
    "`class itemloaders.processors.TakeFirst` [source](https://itemloaders.readthedocs.io/en/latest/_modules/itemloaders/processors.html#TakeFirst)\n",
    "\n",
    "Returns the first non-null/non-empty value from the values received, so it’s typically used as an output processor to single-valued fields. It doesn’t receive any `__init__` method arguments, nor does it accept Loader contexts.\n",
    "\n",
    "Example:\n",
    "\n",
    "```ipython\n",
    ">>> from itemloaders.processors import TakeFirst\n",
    ">>> proc = TakeFirst()\n",
    ">>> proc(['', 'one', 'two', 'three'])\n",
    "'one'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f97863-5f5f-438b-93eb-365cb2af873f",
   "metadata": {},
   "source": [
    "## Declaring Item Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79a107f-9b2d-4e34-bb54-58fc3fea810f",
   "metadata": {},
   "source": [
    "Item Loaders are declared using a class definition syntax. Here is an example:\n",
    "\n",
    "```python\n",
    "from itemloaders.processors import TakeFirst, MapCompose, Join\n",
    "from scrapy.loader import ItemLoader\n",
    "\n",
    "\n",
    "class ProductLoader(ItemLoader):\n",
    "    default_output_processor = TakeFirst()\n",
    "\n",
    "    name_in = MapCompose(str.title)\n",
    "    name_out = Join()\n",
    "\n",
    "    price_in = MapCompose(str.strip)\n",
    "\n",
    "    # ...\n",
    "```\n",
    "\n",
    "As you can see, input processors are declared using the `_in` suffix while output processors are declared using the `_out` suffix. And you can also declare a default input/output processors using the `ItemLoader.default_input_processor` and `ItemLoader.default_output_processor` attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdffbccd-7c42-4892-a6c0-a77b9abbba84",
   "metadata": {},
   "source": [
    "## Declaring Input and Output Processors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fb02ec-3fda-40d9-9960-414a862e001d",
   "metadata": {},
   "source": [
    "As seen in the previous section, input and output processors can be declared in the Item Loader definition, and it’s very common to declare input processors this way. However, there is one more place where you can specify the input and output processors to use: in the `Item` `Field` metadata. Here is an example:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "from itemloaders.processors import Join, MapCompose, TakeFirst\n",
    "from w3lib.html import remove_tags\n",
    "\n",
    "\n",
    "def filter_price(value):\n",
    "    if value.isdigit():\n",
    "        return value\n",
    "\n",
    "\n",
    "class Product(scrapy.Item):\n",
    "    name = scrapy.Field(\n",
    "        input_processor=MapCompose(remove_tags),\n",
    "        output_processor=Join(),\n",
    "    )\n",
    "    price = scrapy.Field(\n",
    "        input_processor=MapCompose(remove_tags, filter_price),\n",
    "        output_processor=TakeFirst(),\n",
    "    )\n",
    "```\n",
    "\n",
    "```ipython\n",
    ">>> from scrapy.loader import ItemLoader\n",
    ">>> il = ItemLoader(item=Product())\n",
    ">>> il.add_value(\"name\", [\"Welcome to my\", \"<strong>website</strong>\"])\n",
    ">>> il.add_value(\"price\", [\"&euro;\", \"<span>1000</span>\"])\n",
    ">>> il.load_item()\n",
    "{'name': 'Welcome to my website', 'price': '1000'}\n",
    "```\n",
    "\n",
    "The precedence order, for both input and output processors, is as follows:\n",
    "1. Item Loader field-specific attributes: `field_in` and `field_out` (most precedence)\n",
    "1. Field metadata (`input_processor` and `output_processor` key)\n",
    "1. Item Loader defaults: `ItemLoader.default_input_processor()` and `ItemLoader.default_output_processor()` (least precedence)\n",
    "\n",
    "See also: [Reusing and extending Item Loaders](#Reusing-and-extending-Item-Loaders)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "864be6a5-9057-4516-9132-7b305138eaa1",
   "metadata": {},
   "source": [
    "## Item Loader Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39688f3-ce59-4cbe-87e9-3ee3e4e05940",
   "metadata": {},
   "source": [
    "The **Item Loader Context** is a `dict` of arbitrary key/values which is shared among all input and output processors in the **Item Loader**. It can be passed when declaring, instantiating or using Item Loader. They are used to modify the behaviour of the input/output processors.\n",
    "\n",
    "For example, suppose you have a function `parse_length` which receives a text value and extracts a length from it:\n",
    "\n",
    "```python\n",
    "def parse_length(text, loader_context):\n",
    "    unit = loader_context.get(\"unit\", \"m\")\n",
    "    # ... length parsing code goes here ...\n",
    "    return parsed_length\n",
    "```\n",
    "\n",
    "By accepting a `loader_context` argument the function is explicitly telling the Item Loader that it’s able to receive an Item Loader context, so the Item Loader passes the currently active context when calling it, and the processor function (`parse_length` in this case) can thus use them.\n",
    "\n",
    "There are several ways to modify Item Loader context values:\n",
    "\n",
    "- By modifying the currently active Item Loader context (`context` attribute):\n",
    "\n",
    "```python\n",
    "loader = ItemLoader(product)\n",
    "loader.context[\"unit\"] = \"cm\"\n",
    "```\n",
    "\n",
    "- On Item Loader instantiation (the keyword arguments of Item Loader `__init__` method are stored in the Item Loader context):\n",
    "\n",
    "```python\n",
    "loader = ItemLoader(product, unit=\"cm\")\n",
    "```\n",
    "\n",
    "- On Item Loader declaration, for those input/output processors that support instantiating them with an Item Loader context. `MapCompose` is one of them:\n",
    "\n",
    "```python\n",
    "class ProductLoader(ItemLoader):\n",
    "    length_out = MapCompose(parse_length, unit=\"cm\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9486e40-1001-41e1-8acb-a8740cdc1f9d",
   "metadata": {},
   "source": [
    "## `ItemLoader` objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86848bc6-3e27-4968-8334-8b43793860cd",
   "metadata": {},
   "source": [
    "```python\n",
    "class scrapy.loader.ItemLoader(item=None, selector=None, response=None, parent=None, **context)\n",
    "``` \n",
    "[source](https://docs.scrapy.org/en/latest/_modules/scrapy/loader.html#ItemLoader)\n",
    "\n",
    "A user-friendly abstraction to populate an item with data by applying field processors to scraped data. When instantiated with a `selector` or a `response` it supports data extraction from web pages using selectors.\n",
    "\n",
    "**Parameters**\n",
    "- `item (scrapy.item.Item)` – The item instance to populate using subsequent calls to \n",
    "    - `add_xpath()`, \n",
    "    - `add_css()`, or \n",
    "    - `add_value()`.\n",
    "- `selector (Selector object)` – The selector to extract data from, when using the \n",
    "    - `add_xpath()`, \n",
    "    - `add_css()`, \n",
    "    - `replace_xpath()`, or \n",
    "    - `replace_css()` method.\n",
    "- `response (Response object)` – The response used to construct the selector using the `default_selector_class`, unless the selector argument is given, in which case this argument is ignored.\n",
    "\n",
    "If no `item` is given, one is instantiated automatically using the class in `default_item_class`.\n",
    "\n",
    "The `item`, `selector`, `response` and remaining keyword arguments are assigned to the Loader context (accessible through the `context` attribute)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d9fb4d-9b4d-4e50-9e6a-746dd8a6fa21",
   "metadata": {},
   "source": [
    "### `item`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ee38c1-c350-4f21-91a8-ea4aca77dc83",
   "metadata": {},
   "source": [
    "The item object being parsed by this Item Loader. This is mostly used as a property so, when attempting to override this value, you may want to check out `default_item_class` first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b94ffe-ab06-419d-95e0-bef157ad3206",
   "metadata": {},
   "source": [
    "### `context`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a03689-c072-4894-967b-0bb5748700a6",
   "metadata": {},
   "source": [
    "The currently active `Context` of this Item Loader."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07272676-f059-4335-8e25-43d5e79c5542",
   "metadata": {},
   "source": [
    "### `default_item_class`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03682e71-007b-4ece-812a-a047f4e8a761",
   "metadata": {},
   "source": [
    "An `item` class (or factory), used to instantiate items when not given in the `__init__` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f12210-4c0f-4234-81ed-36edb3a45d0a",
   "metadata": {},
   "source": [
    "### `default_input_processor`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac48c6f-f3e3-4258-b64a-4bdb3ffb63d9",
   "metadata": {},
   "source": [
    "The default input processor to use for those fields which don’t specify one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6066e7d0-fe82-4b98-98e9-4ddae186f34a",
   "metadata": {},
   "source": [
    "### `default_output_processor`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d3219e-fbc4-47f0-8413-5b7136e85268",
   "metadata": {},
   "source": [
    "The default output processor to use for those fields which don’t specify one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce0e9ef-12c7-4f26-a6e3-7978d6bad491",
   "metadata": {},
   "source": [
    "### `default_selector_class`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74252ca-4979-4bd5-ba00-08577396bc63",
   "metadata": {},
   "source": [
    "The class used to construct the selector of this `ItemLoader`, if only a `response` is given in the `__init__` method. If a `selector` is given in the `__init__` method this attribute is ignored. This attribute is sometimes overridden in subclasses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca64d92-0b3b-4ead-8c80-3ee58e7ea126",
   "metadata": {},
   "source": [
    "### `selector`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c52831-dca6-40cf-94b6-40a4bb8c80d6",
   "metadata": {},
   "source": [
    "The `Selector` object to extract data from. It’s either the `selector` given in the `__init__` method or one created from the `response` given in the `__init__` method using the `default_selector_class`. This attribute is meant to be read-only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc978eb-e61a-4037-86fb-1d254b19bdbb",
   "metadata": {},
   "source": [
    "### `add_css()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4294fda9-0394-436c-8e87-b45b5f6cba89",
   "metadata": {},
   "source": [
    "`add_css(field_name, css, *processors, re=None, **kw)` [source](https://docs.scrapy.org/en/latest/_modules/itemloaders.html#ItemLoader.add_css)\n",
    "\n",
    "Similar to `ItemLoader.add_value()` but receives a CSS `selector` instead of a value, which is used to extract a list of unicode strings from the `selector` associated with this `ItemLoader`.\n",
    "\n",
    "See `get_css()` for `kwargs`.\n",
    "\n",
    "**Parameters**:\n",
    "- `css (str)` – the CSS selector to extract data from\n",
    "\n",
    "Examples:\n",
    "\n",
    "```python\n",
    "# HTML snippet: <p class=\"product-name\">Color TV</p>\n",
    "loader.add_css('name', 'p.product-name')\n",
    "# HTML snippet: <p id=\"price\">the price is $1200</p>\n",
    "loader.add_css('price', 'p#price', re='the price is (.*)')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a369a98b-f9e3-4331-a5dc-fb870769f262",
   "metadata": {},
   "source": [
    "### `add_jmes()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83fffc7-51d5-45a2-b4e7-e48ca7f19121",
   "metadata": {},
   "source": [
    "`add_jmes(field_name, jmes, *processors, re=None, **kw)` [source](https://docs.scrapy.org/en/latest/_modules/itemloaders.html#ItemLoader.add_jmes)\n",
    "\n",
    "Similar to `ItemLoader.add_value()` but receives a JMESPath `selector` instead of a value, which is used to extract a list of unicode strings from the selector associated with this `ItemLoader`.\n",
    "\n",
    "See `get_jmes()` for `kwargs`.\n",
    "\n",
    "**Parameters**\n",
    "- `jmes (str)` – the JMESPath selector to extract data from\n",
    "\n",
    "Examples:\n",
    "\n",
    "```python\n",
    "# HTML snippet: {\"name\": \"Color TV\"}\n",
    "loader.add_jmes('name')\n",
    "# HTML snippet: {\"price\": the price is $1200\"}\n",
    "loader.add_jmes('price', TakeFirst(), re='the price is (.*)')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f428e6e5-9074-45bc-853d-97fb3c83d0a4",
   "metadata": {},
   "source": [
    "### `add_value()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2020a4e-f53f-4e4c-a9e2-10ebd4b80e35",
   "metadata": {},
   "source": [
    "`add_value(field_name, value, *processors, re=None, **kw)` [source](https://docs.scrapy.org/en/latest/_modules/itemloaders.html#ItemLoader.add_value)\n",
    "\n",
    "Process and then add the given value for the given field.\n",
    "\n",
    "The value is first passed through `get_value()` by giving the processors and kwargs, and then passed through the field input processor and its result appended to the data collected for that field. If the field already contains collected data, the new data is added.\n",
    "\n",
    "The given `field_name` can be `None`, in which case values for multiple fields may be added. And the processed value should be a `dict` with `field_name` mapped to values.\n",
    "\n",
    "Examples:\n",
    "\n",
    "```python\n",
    "loader.add_value('name', 'Color TV')\n",
    "loader.add_value('colours', ['white', 'blue'])\n",
    "loader.add_value('length', '100')\n",
    "loader.add_value('name', 'name: foo', TakeFirst(), re='name: (.+)')\n",
    "loader.add_value(None, {'name': 'foo', 'sex': 'male'})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f97d631-e754-4df7-b0f5-534f1fe331ad",
   "metadata": {},
   "source": [
    "### `add_xpath()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99843dd2-b72a-464a-b152-ef26cd37a501",
   "metadata": {},
   "source": [
    "`add_xpath(field_name, xpath, *processors, re=None, **kw)` [source](https://docs.scrapy.org/en/latest/_modules/itemloaders.html#ItemLoader.add_xpath)\n",
    "\n",
    "Similar to `ItemLoader.add_value()` but receives an XPath instead of a value, which is used to extract a list of strings from the selector associated with this `ItemLoader`.\n",
    "\n",
    "See `get_xpath()` for `kwargs`.\n",
    "\n",
    "**Parameters**\n",
    "- `xpath (str)` – the XPath to extract data from\n",
    "\n",
    "Examples:\n",
    "\n",
    "```python\n",
    "# HTML snippet: <p class=\"product-name\">Color TV</p>\n",
    "loader.add_xpath('name', '//p[@class=\"product-name\"]')\n",
    "# HTML snippet: <p id=\"price\">the price is $1200</p>\n",
    "loader.add_xpath('price', '//p[@id=\"price\"]', re='the price is (.*)')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed34cb54-f6a9-4732-bc3e-132470824f88",
   "metadata": {},
   "source": [
    "### `get_collected_values()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cdeaaf-0228-4d31-97a0-9641339e214f",
   "metadata": {},
   "source": [
    "`get_collected_values(field_name)` \n",
    "\n",
    "[source](https://docs.scrapy.org/en/latest/_modules/itemloaders.html#ItemLoader.get_collected_values)\n",
    "\n",
    "Return the collected values for the given field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3f468d-8c88-49f6-bc65-1b2b2b650116",
   "metadata": {},
   "source": [
    "### `get_css()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbc6263-3f0f-4c90-b7bd-6730a0bbe2e0",
   "metadata": {},
   "source": [
    "`get_css(css, *processors, re=None, **kw)` [source](https://docs.scrapy.org/en/latest/_modules/itemloaders.html#ItemLoader.get_collected_values)\n",
    "\n",
    "Similar to `ItemLoader.get_value()` but receives a CSS selector instead of a value, which is used to extract a list of unicode strings from the selector associated with this ItemLoader.\n",
    "\n",
    "**Parameters**\n",
    "- `css (str)` – the CSS selector to extract data from\n",
    "- `re (str or Pattern)` – a regular expression to use for extracting data from the selected CSS region\n",
    "\n",
    "Examples:\n",
    "\n",
    "```python\n",
    "# HTML snippet: <p class=\"product-name\">Color TV</p>\n",
    "loader.get_css('p.product-name')\n",
    "# HTML snippet: <p id=\"price\">the price is $1200</p>\n",
    "loader.get_css('p#price', TakeFirst(), re='the price is (.*)')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ac4c98-f1a7-4b1b-ae73-39f2bdb0944f",
   "metadata": {},
   "source": [
    "### `get_jmes()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb5468a-b97a-49b6-b937-031e01e072bf",
   "metadata": {},
   "source": [
    "`get_jmes(jmes, *processors, re=None, **kw)` [source](https://docs.scrapy.org/en/latest/_modules/itemloaders.html#ItemLoader.get_jmes)\n",
    "\n",
    "Similar to `ItemLoader.get_value()` but receives a JMESPath selector instead of a value, which is used to extract a list of unicode strings from the selector associated with this ItemLoader.\n",
    "\n",
    "**Parameters**\n",
    "- `jmes (str)` – the JMESPath selector to extract data from\n",
    "- `re (str or Pattern)` – a regular expression to use for extracting data from the selected JMESPath\n",
    "\n",
    "Examples:\n",
    "\n",
    "```python\n",
    "# HTML snippet: {\"name\": \"Color TV\"}\n",
    "loader.get_jmes('name')\n",
    "# HTML snippet: {\"price\": the price is $1200\"}\n",
    "loader.get_jmes('price', TakeFirst(), re='the price is (.*)')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e138f0c7-a130-444d-9484-e5accc7c6554",
   "metadata": {},
   "source": [
    "### `get_output_value()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1798dea7-986c-480f-bf35-dfdb4ce0a194",
   "metadata": {},
   "source": [
    "`get_output_value(field_name)` [source](https://docs.scrapy.org/en/latest/_modules/itemloaders.html#ItemLoader.get_output_value)\n",
    "\n",
    "Return the collected values parsed using the output processor, for the given `field`. This method doesn’t populate or modify the `item` at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469604a4-a23f-43f3-966a-e0cf97c865eb",
   "metadata": {},
   "source": [
    "### `get_value()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3138b71a-263a-461a-b6d6-aed9c3a63594",
   "metadata": {},
   "source": [
    "`get_value(value, *processors, re=None, **kw)` [source](https://docs.scrapy.org/en/latest/_modules/itemloaders.html#ItemLoader.get_value)\n",
    "\n",
    "Process the given value by the given processors and keyword arguments.\n",
    "\n",
    "Available keyword arguments:\n",
    "\n",
    "**Parameters**\n",
    "- `re (str or Pattern)` – a regular expression to use for extracting data from the given value using `extract_regex()` method, applied before processors\n",
    "\n",
    "Examples:\n",
    "\n",
    "```ipython\n",
    ">>> from itemloaders import ItemLoader\n",
    ">>> from itemloaders.processors import TakeFirst\n",
    ">>> loader = ItemLoader()\n",
    ">>> loader.get_value('name: foo', TakeFirst(), str.upper, re='name: (.+)')\n",
    "'FOO'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124cba71-8386-4796-a43b-f2e0ec6871ab",
   "metadata": {},
   "source": [
    "### `get_xpath()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a141eb-48bf-49f7-9f3c-73525acfad73",
   "metadata": {},
   "source": [
    "`get_xpath(xpath, *processors, re=None, **kw)` [source](https://docs.scrapy.org/en/latest/_modules/itemloaders.html#ItemLoader.get_xpath)\n",
    "\n",
    "Similar to `ItemLoader.get_value()` but receives an XPath instead of a value, which is used to extract a list of unicode strings from the selector associated with this ItemLoader.\n",
    "\n",
    "**Parameters**\n",
    "- `xpath (str)` – the XPath to extract data from\n",
    "- `re (str or Pattern)` – a regular expression to use for extracting data from the selected XPath region\n",
    "\n",
    "Examples:\n",
    "\n",
    "```python\n",
    "# HTML snippet: <p class=\"product-name\">Color TV</p>\n",
    "loader.get_xpath('//p[@class=\"product-name\"]')\n",
    "# HTML snippet: <p id=\"price\">the price is $1200</p>\n",
    "loader.get_xpath('//p[@id=\"price\"]', TakeFirst(), re='the price is (.*)')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08af38b2-6ccd-404c-87de-b9811f620bc0",
   "metadata": {},
   "source": [
    "### `load_item()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c92e020-0cc6-4e6e-a547-8e866ef36763",
   "metadata": {},
   "source": [
    "[source](https://docs.scrapy.org/en/latest/_modules/itemloaders.html#ItemLoader.load_item)\n",
    "\n",
    "Populate the item with the data collected so far, and return it. The data collected is first passed through the output processors to get the final value to assign to each item field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279845f4-304e-4d1f-8c90-433c8b0ffb12",
   "metadata": {},
   "source": [
    "### `nested_css()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aef7fc4-1675-45c3-8a13-e4d3fabdc64c",
   "metadata": {},
   "source": [
    "`nested_css(css, **context)` [source](https://docs.scrapy.org/en/latest/_modules/itemloaders.html#ItemLoader.nested_css)\n",
    "\n",
    "Create a nested loader with a css selector. The supplied selector is applied relative to selector associated with this `ItemLoader`. The nested loader shares the `item` with the parent `ItemLoader` so calls to `add_xpath()`, `add_value()`, `replace_value()`, etc. will behave as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98624a45-e8f6-40f0-85e4-aaa32aa834c9",
   "metadata": {},
   "source": [
    "### `nested_xpath()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd51ed-922a-4165-9759-1865e519ef46",
   "metadata": {},
   "source": [
    "`nested_xpath(xpath, **context)` [source](https://docs.scrapy.org/en/latest/_modules/itemloaders.html#ItemLoader.nested_xpath)\n",
    "\n",
    "Create a nested loader with an xpath selector. The supplied selector is applied relative to selector associated with this `ItemLoader`. The nested loader shares the item with the parent `ItemLoader` so calls to `add_xpath()`, `add_value()`, `replace_value()`, etc. will behave as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8603f8e-23f0-4784-b749-786fd0aef066",
   "metadata": {},
   "source": [
    "### `replace_css()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e0cd6a-2a2b-41a5-80fb-a254d52044c0",
   "metadata": {},
   "source": [
    "`replace_css(field_name, css, *processors, re=None, **kw)` [source](https://docs.scrapy.org/en/latest/_modules/itemloaders.html#ItemLoader.replace_css)\n",
    "\n",
    "Similar to `add_css()` but replaces collected data instead of adding it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27f6d4b-869e-465f-bee0-4ca58e091ebe",
   "metadata": {},
   "source": [
    "### `replace_jmes()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f4407c-7711-4869-a039-0e74438e3eab",
   "metadata": {},
   "source": [
    "`replace_jmes(field_name, jmes, *processors, re=None, **kw)` [source](https://docs.scrapy.org/en/latest/_modules/itemloaders.html#ItemLoader.replace_jmes)\n",
    "\n",
    "Similar to `add_jmes()` but replaces collected data instead of adding it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e35bd97-ea06-4fcb-b87b-3e7de0453156",
   "metadata": {},
   "source": [
    "### `replace_value()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284ada81-3be4-463e-8c3e-1ba291427795",
   "metadata": {},
   "source": [
    "`replace_value(field_name, value, *processors, re=None, **kw)` [source](https://docs.scrapy.org/en/latest/_modules/itemloaders.html#ItemLoader.replace_value)\n",
    "\n",
    "Similar to `add_value()` but replaces the collected data with the new value instead of adding it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b8888b-c0f5-4b2b-8bbe-1c522228445e",
   "metadata": {},
   "source": [
    "### `replace_xpath()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88be63a8-1744-4101-8e27-c1d49ea25eaf",
   "metadata": {},
   "source": [
    "`replace_xpath(field_name, xpath, *processors, re=None, **kw)` [source](https://docs.scrapy.org/en/latest/_modules/itemloaders.html#ItemLoader.replace_xpath)\n",
    "\n",
    "Similar to `add_xpath()` but replaces collected data instead of adding it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7312cf6-eb3a-46c0-929a-ea21e6ac5215",
   "metadata": {},
   "source": [
    "## Nested Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1c46ea-c721-4697-8868-464089e56ad2",
   "metadata": {},
   "source": [
    "When parsing related values from a subsection of a document, it can be useful to create nested loaders. Imagine you’re extracting details from a footer of a page that looks something like:\n",
    "\n",
    "Example:\n",
    "\n",
    "```html\n",
    "<footer>\n",
    "    <a class=\"social\" href=\"https://facebook.com/whatever\">Like Us</a>\n",
    "    <a class=\"social\" href=\"https://twitter.com/whatever\">Follow Us</a>\n",
    "    <a class=\"email\" href=\"mailto:whatever@example.com\">Email Us</a>\n",
    "</footer>\n",
    "```\n",
    "\n",
    "Without nested loaders, you need to specify the full `xpath` (or `css`) for each value that you wish to extract.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "loader = ItemLoader(item=Item())\n",
    "# load stuff not in the footer\n",
    "loader.add_xpath(\"social\", '//footer/a[@class = \"social\"]/@href')\n",
    "loader.add_xpath(\"email\", '//footer/a[@class = \"email\"]/@href')\n",
    "loader.load_item()\n",
    "```\n",
    "\n",
    "Instead, you can create a nested loader with the footer selector and add values relative to the footer. The functionality is the same but you avoid repeating the footer selector.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "loader = ItemLoader(item=Item())\n",
    "# load stuff not in the footer\n",
    "footer_loader = loader.nested_xpath(\"//footer\")\n",
    "footer_loader.add_xpath(\"social\", 'a[@class = \"social\"]/@href')\n",
    "footer_loader.add_xpath(\"email\", 'a[@class = \"email\"]/@href')\n",
    "# no need to call footer_loader.load_item()\n",
    "loader.load_item()\n",
    "```\n",
    "\n",
    "You can nest loaders arbitrarily and they work with either `xpath` or `css` selectors. As a general guideline, use nested loaders when they make your code simpler but do not go overboard with nesting or your parser can become difficult to read."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4338d61-4f9f-452a-9a34-35d321cf5709",
   "metadata": {},
   "source": [
    "## Reusing and extending Item Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bea4c2-8dad-4725-8d9e-2ae9ecb38a87",
   "metadata": {},
   "source": [
    "As your project grows bigger and acquires more and more spiders, maintenance becomes a fundamental problem, especially when you have to deal with many different parsing rules for each spider, having a lot of exceptions, but also wanting to reuse the common processors.\n",
    "\n",
    "Item Loaders are designed to ease the maintenance burden of parsing rules, without losing flexibility and, at the same time, providing a convenient mechanism for extending and overriding them. For this reason Item Loaders support traditional Python class inheritance for dealing with differences of specific spiders (or groups of spiders).\n",
    "\n",
    "Suppose, for example, that some particular site encloses their product names in three dashes (e.g. `---Plasma TV---`) and you don’t want to end up scraping those dashes in the final product names.\n",
    "\n",
    "Here’s how you can remove those dashes by reusing and extending the default Product Item Loader (`ProductLoader`):\n",
    "\n",
    "```python\n",
    "from itemloaders.processors import MapCompose\n",
    "from myproject.ItemLoaders import ProductLoader\n",
    "\n",
    "\n",
    "def strip_dashes(x):\n",
    "    return x.strip(\"-\")\n",
    "\n",
    "\n",
    "class SiteSpecificLoader(ProductLoader):\n",
    "    name_in = MapCompose(strip_dashes, ProductLoader.name_in)\n",
    "```\n",
    "\n",
    "Another case where extending Item Loaders can be very helpful is when you have multiple source formats, for example `XML` and `HTML`. In the `XML` version you may want to remove CDATA occurrences. Here’s an example of how to do it:\n",
    "\n",
    "```python\n",
    "from itemloaders.processors import MapCompose\n",
    "from myproject.ItemLoaders import ProductLoader\n",
    "from myproject.utils.xml import remove_cdata\n",
    "\n",
    "\n",
    "class XmlProductLoader(ProductLoader):\n",
    "    name_in = MapCompose(remove_cdata, ProductLoader.name_in)\n",
    "```\n",
    "\n",
    "And that’s how you typically extend input processors.\n",
    "\n",
    "As for output processors, it is more common to declare them in the field metadata, as they usually depend only on the field and not on each specific site parsing rule (as input processors do). See also: [Declaring Input and Output Processors](#Declaring-Input-and-Output-Processors).\n",
    "\n",
    "There are many other possible ways to extend, inherit and override your Item Loaders, and different Item Loaders hierarchies may fit better for different projects. Scrapy only provides the mechanism; it doesn’t impose any specific organization of your Loaders collection - that’s up to you and your project’s needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7db174-e525-4b13-9b9d-7294bcc8eed8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8290a0c6-53d8-4953-9e82-495a37f96173",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da6075c6-9de7-4c55-afe2-8a095b11aae2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9730128-4af0-4fd2-a31e-9305ab0ae3fc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8391a32-c9a1-4df9-a35c-2eacdfc1f590",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a18eff78-5610-4d2d-919c-877e1671dcaf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9d1dc08-29a0-45f5-8386-3098b6176413",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7c8c9fc-25f9-4ff5-be5f-e5ca26ef509c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55e3cc22-a98f-478b-a53a-71fb345d9485",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94fad561-b385-43ee-9597-6c997520da8d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afd5fab6-ed96-4caf-8e29-7d7663575b3a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "503520f7-6526-409f-a7c6-6c3dd6d569d2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3f9c7bc-cb1f-49f2-acce-b29f7c1ffdfe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3086646-03fe-4e0c-9b50-d6031980b294",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9394293-8e29-4988-92e3-9cc8058b08f0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13e99fe6-c03f-4b81-b9cd-45dffd8923ca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6812470-421b-4ed7-b984-d23c8f32aa30",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "283df80b-fe00-4614-aebd-34885cd103fd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d223a308-1131-4e1c-980b-78c99464cfbe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4b3abc7-3ebd-4a49-a0e8-42456097e01f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cddeadd2-3304-4c0a-9aa8-b3e88dfe02d5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94d54ab5-4bbb-4ff7-904a-d25a3c91f139",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77dfcda7-aeb2-464f-940f-689280d86ecf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1c9e1fb-448a-4b4b-b6e9-51e3f28122eb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30841a41-cf74-48a4-b1e4-8e244e34137a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dea7ab04-4dca-4aa8-bdd5-1e3b5a049bdf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cfac7df-f40f-434c-a9e4-32fadd56d978",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36467b9b-e658-486c-99ec-c432fc14b6b3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fec34939-efea-4865-8884-a5194348f670",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c26f0115-acc9-4ec2-92bd-7aac54d3d509",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "558e9b64-38fb-4776-b016-46665ac26984",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e750fe5e-dee7-41b5-92e5-2a8400e085f5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80194f21-4bf5-4211-9986-2e8b69382177",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbbb268c-ff08-4e90-b88d-ffd38057dece",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb25b002-0dce-4e26-bd05-76731fd2d916",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9c6a89e-59fa-4709-a163-0040681b2ebd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c29c9ce-de81-48cd-9a58-431c1a0b3a00",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7345da98-fbe9-4961-aff8-062715eb8d85",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32813066-683a-47e9-bd9f-81eb918da68f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff22b57e-780e-436d-8bd4-30d4b9a2f554",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9d3a216-b12f-46a3-9a7b-7150f15dd0df",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d28f385-c8a3-42e3-b40d-e56e77c39a54",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5920f59-07e4-459e-8fd4-fd7bf7675bdc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10e14f26-2f61-41fd-9dae-b73ef5356390",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e9a3aa8-d3ab-4c89-9754-1c058636e4e8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dab08edd-bc4e-4c47-94a1-ceae3d91e1d1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa2342f3-6cbc-486b-b67d-1e26a409a189",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6e551b6-3cce-423c-9c68-ceb485638a1f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5ab1ee8-2aaf-4536-a3fc-5ec01ced4985",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6935e77f-158e-4c27-a099-8a2d5e8d0a55",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e842c81a-b420-4efc-9ec3-ee350ec0f1d3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca2f5bfb-c517-4347-ad1f-2c30e213b2b5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88fdcd49-536e-4602-a4db-2dd1caa952dd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6e6ff5f-0ecf-4075-96b1-a2e06404ee3d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54b6a806-a12b-4502-9bc6-1e8e7c68696b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e33e59fe-b2cc-4aaa-98fb-c9f63c8cef9d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b25dfb7a-35fa-49d3-b9b4-1402784e9bdc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26144a36-cb27-40d1-8495-dec0991e88bf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a406e6a-016d-4caf-b4a6-ab504ffe3450",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7a27c40-5cb0-4e08-ae85-cbc9ac3682de",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "141f515e-e5b4-474a-90a4-7f88486f9ad1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dcf2dd0-22d1-4d95-968b-fc978aba9090",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db07fc89-118b-438e-90e9-51416761d3bf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb07469a-79ff-42bf-a37d-87db3bfb032d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7adb7e15-e865-4330-adb7-0357005d8916",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6409098a-8663-4cb5-876d-b41222d74116",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc102b71-eb67-4e32-96bf-e675dc50410c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b0f3bf3-33ed-4e26-9e55-3f4285944dfa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "133006c0-9219-4c31-83d2-bf67c8c02188",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e7ec979-9529-4da2-9e4a-a5111126d10c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc2088de-611a-483d-bb02-e62114f0f47a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c16caca-ebba-4ace-aede-e20cf443556f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dfcfc7c-48a8-4012-8b33-a9818a0293e7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e588fee-0a7e-4f07-82e5-5feb9f5df6ba",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c67923e-4514-4171-a26b-57dd76b1b669",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02444d83-469e-46e1-ba48-3f55d4be240f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e7f25a9-19fb-4faf-8f32-cd753ecc09be",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35242cdc-0a72-4f6b-b6f4-33ea94690b48",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92a06334-cba7-4bb1-bbbd-7386827cfe3b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd6ad5fb-2a21-4ce7-872f-5664522c131f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cddde7b5-ca98-4030-b80a-7422061535db",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f599a92f-3389-44ba-8cb2-085bff5da116",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a7de247-b110-4754-b084-dc58d92cc4b2",
   "metadata": {},
   "source": [
    "# 2.6 Scrapy shell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b13653d-cb7b-49f0-a03c-ef2793afdb41",
   "metadata": {},
   "source": [
    "# 2.7 Item Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75551f7a-22a1-4fe1-9efa-e06e9f59fe86",
   "metadata": {},
   "source": [
    "# 2.8 Feed exports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc64ab36-c300-4809-a7b6-f881b010d0b7",
   "metadata": {},
   "source": [
    "# 2.9 Requests and Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6fba9f-0132-48ff-96b5-85c77107a00b",
   "metadata": {},
   "source": [
    "## Passing additional data to callback functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8f5c08-3b95-4be2-baee-3e0c0cb870be",
   "metadata": {},
   "source": [
    "# 2.10 Link Extractors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e8fa49-516a-4223-b46e-29daf54320c8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fe246e0-224f-4421-89b5-4cf89ff45756",
   "metadata": {},
   "source": [
    "# 2.11 Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909bceb4-a7e3-46c6-9e44-509d67bb8f96",
   "metadata": {},
   "source": [
    "## Designating the settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4061dd4c-c4f4-4a50-b94b-847f86fe479d",
   "metadata": {},
   "source": [
    "## Built-in settings reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cbb154-6ac6-4f26-9f54-766c8aee6b91",
   "metadata": {},
   "source": [
    "# <b>3. Built-in Services</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07071642-bcdf-4074-9ac5-683c2c174547",
   "metadata": {},
   "source": [
    "# 3.1 Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9547c2-32b1-4ef1-a376-58939698db56",
   "metadata": {},
   "source": [
    "## Logging from Spiders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5acafe-f8ea-475a-be3b-8cddb02400a6",
   "metadata": {},
   "source": [
    "# <b>4. Solving specific problems</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d81b28-f488-4e20-8e1d-762bbe7b6c32",
   "metadata": {},
   "source": [
    "# 4.4 Common practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acda595-139e-44f4-a3bc-7fa7c0e39bdf",
   "metadata": {},
   "source": [
    "## Run Scrapy from a script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaf1cb2-dd97-4e2f-b598-8806b347bae4",
   "metadata": {},
   "source": [
    "# 4.8 Debugging memory leaks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d0cbae-ad52-43ad-a7ec-4209b0182089",
   "metadata": {},
   "source": [
    "## Debugging memory leaks with `trackref`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08087fea-2315-437e-a6e1-68153d8c8982",
   "metadata": {},
   "source": [
    "# 4.12 Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afaa1fd-a706-4e34-85bf-fe6d65ff8dbc",
   "metadata": {},
   "source": [
    "# 4.13 Jobs: pausing and resuming crawls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8d9545-3f29-45c6-8a48-68aced2ada84",
   "metadata": {},
   "source": [
    "## Keeping persistent state between batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942d63ed-16e6-4ef7-ae12-73b5ea5dacc9",
   "metadata": {},
   "source": [
    "# <b>5. Extending Scrapy</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70898892-f4eb-4bcf-b4ca-fbde54e3bfdb",
   "metadata": {},
   "source": [
    "# 5.2 Add-ons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031251d6-91b1-431d-85c7-77399188ad04",
   "metadata": {},
   "source": [
    "# 5.4 Spider Middleware"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3a8f84-ccd6-4e6b-aff3-f7a1494eab73",
   "metadata": {},
   "source": [
    "# 5.8 Item Exporters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfcc4ae-f1d5-47d2-9bb5-79b232d870c2",
   "metadata": {},
   "source": [
    "## Serialization of item fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61d7184-97d7-42b7-9de6-0ed960ddc994",
   "metadata": {},
   "source": [
    "# 5.10 Core API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567a1eee-4dfd-4916-b023-a5711160ddc4",
   "metadata": {},
   "source": [
    "## Crawler API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfd090d-5027-4557-8865-67fc52ba39fd",
   "metadata": {},
   "source": [
    "# 6. All the rest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa5b299-6db3-4817-9b8b-2181bf669e30",
   "metadata": {},
   "source": [
    "# 7. Source code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7d5878-3fc0-43ee-b9ec-7beee99cac23",
   "metadata": {},
   "source": [
    "## `scrapy.spiders.crawl`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fe65ea-be06-4164-a3a5-8869eb302c86",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"\n",
    "This modules implements the CrawlSpider which is the recommended spider to use\n",
    "for scraping typical web sites that requires crawling pages.\n",
    "\n",
    "See documentation in docs/topics/spiders.rst\n",
    "\"\"\"\n",
    "\n",
    "import copy\n",
    "from typing import AsyncIterable, Awaitable, Sequence\n",
    "\n",
    "from scrapy.http import HtmlResponse, Request, Response\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.spiders import Spider\n",
    "from scrapy.utils.asyncgen import collect_asyncgen\n",
    "from scrapy.utils.spider import iterate_spider_output\n",
    "\n",
    "\n",
    "def _identity(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def _identity_process_request(request, response):\n",
    "    return request\n",
    "\n",
    "\n",
    "def _get_method(method, spider):\n",
    "    if callable(method):\n",
    "        return method\n",
    "    if isinstance(method, str):\n",
    "        return getattr(spider, method, None)\n",
    "\n",
    "\n",
    "_default_link_extractor = LinkExtractor()\n",
    "\n",
    "\n",
    "[docs]class Rule:\n",
    "    def __init__(\n",
    "        self,\n",
    "        link_extractor=None,\n",
    "        callback=None,\n",
    "        cb_kwargs=None,\n",
    "        follow=None,\n",
    "        process_links=None,\n",
    "        process_request=None,\n",
    "        errback=None,\n",
    "    ):\n",
    "        self.link_extractor = link_extractor or _default_link_extractor\n",
    "        self.callback = callback\n",
    "        self.errback = errback\n",
    "        self.cb_kwargs = cb_kwargs or {}\n",
    "        self.process_links = process_links or _identity\n",
    "        self.process_request = process_request or _identity_process_request\n",
    "        self.follow = follow if follow is not None else not callback\n",
    "\n",
    "    def _compile(self, spider):\n",
    "        self.callback = _get_method(self.callback, spider)\n",
    "        self.errback = _get_method(self.errback, spider)\n",
    "        self.process_links = _get_method(self.process_links, spider)\n",
    "        self.process_request = _get_method(self.process_request, spider)\n",
    "\n",
    "\n",
    "\n",
    "[docs]class CrawlSpider(Spider):\n",
    "    rules: Sequence[Rule] = ()\n",
    "\n",
    "    def __init__(self, *a, **kw):\n",
    "        super().__init__(*a, **kw)\n",
    "        self._compile_rules()\n",
    "\n",
    "    def _parse(self, response, **kwargs):\n",
    "        return self._parse_response(\n",
    "            response=response,\n",
    "            callback=self.parse_start_url,\n",
    "            cb_kwargs=kwargs,\n",
    "            follow=True,\n",
    "        )\n",
    "\n",
    "[docs]    def parse_start_url(self, response, **kwargs):\n",
    "        return []\n",
    "\n",
    "\n",
    "    def process_results(self, response: Response, results: list):\n",
    "        return results\n",
    "\n",
    "    def _build_request(self, rule_index, link):\n",
    "        return Request(\n",
    "            url=link.url,\n",
    "            callback=self._callback,\n",
    "            errback=self._errback,\n",
    "            meta=dict(rule=rule_index, link_text=link.text),\n",
    "        )\n",
    "\n",
    "    def _requests_to_follow(self, response):\n",
    "        if not isinstance(response, HtmlResponse):\n",
    "            return\n",
    "        seen = set()\n",
    "        for rule_index, rule in enumerate(self._rules):\n",
    "            links = [\n",
    "                lnk\n",
    "                for lnk in rule.link_extractor.extract_links(response)\n",
    "                if lnk not in seen\n",
    "            ]\n",
    "            for link in rule.process_links(links):\n",
    "                seen.add(link)\n",
    "                request = self._build_request(rule_index, link)\n",
    "                yield rule.process_request(request, response)\n",
    "\n",
    "    def _callback(self, response, **cb_kwargs):\n",
    "        rule = self._rules[response.meta[\"rule\"]]\n",
    "        return self._parse_response(\n",
    "            response, rule.callback, {**rule.cb_kwargs, **cb_kwargs}, rule.follow\n",
    "        )\n",
    "\n",
    "    def _errback(self, failure):\n",
    "        rule = self._rules[failure.request.meta[\"rule\"]]\n",
    "        return self._handle_failure(failure, rule.errback)\n",
    "\n",
    "    async def _parse_response(self, response, callback, cb_kwargs, follow=True):\n",
    "        if callback:\n",
    "            cb_res = callback(response, **cb_kwargs) or ()\n",
    "            if isinstance(cb_res, AsyncIterable):\n",
    "                cb_res = await collect_asyncgen(cb_res)\n",
    "            elif isinstance(cb_res, Awaitable):\n",
    "                cb_res = await cb_res\n",
    "            cb_res = self.process_results(response, cb_res)\n",
    "            for request_or_item in iterate_spider_output(cb_res):\n",
    "                yield request_or_item\n",
    "\n",
    "        if follow and self._follow_links:\n",
    "            for request_or_item in self._requests_to_follow(response):\n",
    "                yield request_or_item\n",
    "\n",
    "    def _handle_failure(self, failure, errback):\n",
    "        if errback:\n",
    "            results = errback(failure) or ()\n",
    "            for request_or_item in iterate_spider_output(results):\n",
    "                yield request_or_item\n",
    "\n",
    "    def _compile_rules(self):\n",
    "        self._rules = []\n",
    "        for rule in self.rules:\n",
    "            self._rules.append(copy.copy(rule))\n",
    "            self._rules[-1]._compile(self)\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler, *args, **kwargs):\n",
    "        spider = super().from_crawler(crawler, *args, **kwargs)\n",
    "        spider._follow_links = crawler.settings.getbool(\n",
    "            \"CRAWLSPIDER_FOLLOW_LINKS\", True\n",
    "        )\n",
    "        return spider\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c604e3-ef01-476f-a7ec-d403e5145df5",
   "metadata": {},
   "source": [
    "## `scrapy.item`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d76bc0-e1bf-4862-bca4-bb73c677c185",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"\n",
    "Scrapy Item\n",
    "\n",
    "See documentation in docs/topics/item.rst\n",
    "\"\"\"\n",
    "\n",
    "from abc import ABCMeta\n",
    "from collections.abc import MutableMapping\n",
    "from copy import deepcopy\n",
    "from pprint import pformat\n",
    "from typing import Dict\n",
    "\n",
    "from scrapy.utils.trackref import object_ref\n",
    "\n",
    "\n",
    "class Field(dict):\n",
    "    \"\"\"Container of field metadata\"\"\"\n",
    "\n",
    "\n",
    "[docs]class ItemMeta(ABCMeta):\n",
    "    \"\"\"Metaclass_ of :class:`Item` that handles field definitions.\n",
    "\n",
    "    .. _metaclass: https://realpython.com/python-metaclasses\n",
    "    \"\"\"\n",
    "\n",
    "    def __new__(mcs, class_name, bases, attrs):\n",
    "        classcell = attrs.pop(\"__classcell__\", None)\n",
    "        new_bases = tuple(base._class for base in bases if hasattr(base, \"_class\"))\n",
    "        _class = super().__new__(mcs, \"x_\" + class_name, new_bases, attrs)\n",
    "\n",
    "        fields = getattr(_class, \"fields\", {})\n",
    "        new_attrs = {}\n",
    "        for n in dir(_class):\n",
    "            v = getattr(_class, n)\n",
    "            if isinstance(v, Field):\n",
    "                fields[n] = v\n",
    "            elif n in attrs:\n",
    "                new_attrs[n] = attrs[n]\n",
    "\n",
    "        new_attrs[\"fields\"] = fields\n",
    "        new_attrs[\"_class\"] = _class\n",
    "        if classcell is not None:\n",
    "            new_attrs[\"__classcell__\"] = classcell\n",
    "        return super().__new__(mcs, class_name, bases, new_attrs)\n",
    "\n",
    "\n",
    "\n",
    "class Item(MutableMapping, object_ref, metaclass=ItemMeta):\n",
    "    \"\"\"\n",
    "    Base class for scraped items.\n",
    "\n",
    "    In Scrapy, an object is considered an ``item`` if it is an instance of either\n",
    "    :class:`Item` or :class:`dict`, or any subclass. For example, when the output of a\n",
    "    spider callback is evaluated, only instances of :class:`Item` or\n",
    "    :class:`dict` are passed to :ref:`item pipelines <topics-item-pipeline>`.\n",
    "\n",
    "    If you need instances of a custom class to be considered items by Scrapy,\n",
    "    you must inherit from either :class:`Item` or :class:`dict`.\n",
    "\n",
    "    Items must declare :class:`Field` attributes, which are processed and stored\n",
    "    in the ``fields`` attribute. This restricts the set of allowed field names\n",
    "    and prevents typos, raising ``KeyError`` when referring to undefined fields.\n",
    "    Additionally, fields can be used to define metadata and control the way\n",
    "    data is processed internally. Please refer to the :ref:`documentation\n",
    "    about fields <topics-items-fields>` for additional information.\n",
    "\n",
    "    Unlike instances of :class:`dict`, instances of :class:`Item` may be\n",
    "    :ref:`tracked <topics-leaks-trackrefs>` to debug memory leaks.\n",
    "    \"\"\"\n",
    "\n",
    "    fields: Dict[str, Field]\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self._values = {}\n",
    "        if args or kwargs:  # avoid creating dict for most common case\n",
    "            for k, v in dict(*args, **kwargs).items():\n",
    "                self[k] = v\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self._values[key]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        if key in self.fields:\n",
    "            self._values[key] = value\n",
    "        else:\n",
    "            raise KeyError(f\"{self.__class__.__name__} does not support field: {key}\")\n",
    "\n",
    "    def __delitem__(self, key):\n",
    "        del self._values[key]\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        if name in self.fields:\n",
    "            raise AttributeError(f\"Use item[{name!r}] to get field value\")\n",
    "        raise AttributeError(name)\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        if not name.startswith(\"_\"):\n",
    "            raise AttributeError(f\"Use item[{name!r}] = {value!r} to set field value\")\n",
    "        super().__setattr__(name, value)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._values)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._values)\n",
    "\n",
    "    __hash__ = object_ref.__hash__\n",
    "\n",
    "    def keys(self):\n",
    "        return self._values.keys()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return pformat(dict(self))\n",
    "\n",
    "    def copy(self):\n",
    "        return self.__class__(self)\n",
    "\n",
    "    def deepcopy(self):\n",
    "        \"\"\"Return a :func:`~copy.deepcopy` of this item.\"\"\"\n",
    "        return deepcopy(self)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01be58f8-6678-48b5-9719-adb3d19ded02",
   "metadata": {},
   "source": [
    "## `scrapy.loader`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09268466-adaa-42ad-a82b-4731b597e09e",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"\n",
    "Item Loader\n",
    "\n",
    "See documentation in docs/topics/loaders.rst\n",
    "\"\"\"\n",
    "import itemloaders\n",
    "\n",
    "from scrapy.item import Item\n",
    "from scrapy.selector import Selector\n",
    "\n",
    "\n",
    "class ItemLoader(itemloaders.ItemLoader):\n",
    "    \"\"\"\n",
    "    A user-friendly abstraction to populate an :ref:`item <topics-items>` with data\n",
    "    by applying :ref:`field processors <topics-loaders-processors>` to scraped data.\n",
    "    When instantiated with a ``selector`` or a ``response`` it supports\n",
    "    data extraction from web pages using :ref:`selectors <topics-selectors>`.\n",
    "\n",
    "    :param item: The item instance to populate using subsequent calls to\n",
    "        :meth:`~ItemLoader.add_xpath`, :meth:`~ItemLoader.add_css`,\n",
    "        or :meth:`~ItemLoader.add_value`.\n",
    "    :type item: scrapy.item.Item\n",
    "\n",
    "    :param selector: The selector to extract data from, when using the\n",
    "        :meth:`add_xpath`, :meth:`add_css`, :meth:`replace_xpath`, or\n",
    "        :meth:`replace_css` method.\n",
    "    :type selector: :class:`~scrapy.selector.Selector` object\n",
    "\n",
    "    :param response: The response used to construct the selector using the\n",
    "        :attr:`default_selector_class`, unless the selector argument is given,\n",
    "        in which case this argument is ignored.\n",
    "    :type response: :class:`~scrapy.http.Response` object\n",
    "\n",
    "    If no item is given, one is instantiated automatically using the class in\n",
    "    :attr:`default_item_class`.\n",
    "\n",
    "    The item, selector, response and remaining keyword arguments are\n",
    "    assigned to the Loader context (accessible through the :attr:`context` attribute).\n",
    "\n",
    "    .. attribute:: item\n",
    "\n",
    "        The item object being parsed by this Item Loader.\n",
    "        This is mostly used as a property so, when attempting to override this\n",
    "        value, you may want to check out :attr:`default_item_class` first.\n",
    "\n",
    "    .. attribute:: context\n",
    "\n",
    "        The currently active :ref:`Context <loaders-context>` of this Item Loader.\n",
    "\n",
    "    .. attribute:: default_item_class\n",
    "\n",
    "        An :ref:`item <topics-items>` class (or factory), used to instantiate\n",
    "        items when not given in the ``__init__`` method.\n",
    "\n",
    "    .. attribute:: default_input_processor\n",
    "\n",
    "        The default input processor to use for those fields which don't specify\n",
    "        one.\n",
    "\n",
    "    .. attribute:: default_output_processor\n",
    "\n",
    "        The default output processor to use for those fields which don't specify\n",
    "        one.\n",
    "\n",
    "    .. attribute:: default_selector_class\n",
    "\n",
    "        The class used to construct the :attr:`selector` of this\n",
    "        :class:`ItemLoader`, if only a response is given in the ``__init__`` method.\n",
    "        If a selector is given in the ``__init__`` method this attribute is ignored.\n",
    "        This attribute is sometimes overridden in subclasses.\n",
    "\n",
    "    .. attribute:: selector\n",
    "\n",
    "        The :class:`~scrapy.selector.Selector` object to extract data from.\n",
    "        It's either the selector given in the ``__init__`` method or one created from\n",
    "        the response given in the ``__init__`` method using the\n",
    "        :attr:`default_selector_class`. This attribute is meant to be\n",
    "        read-only.\n",
    "    \"\"\"\n",
    "\n",
    "    default_item_class = Item\n",
    "    default_selector_class = Selector\n",
    "\n",
    "    def __init__(self, item=None, selector=None, response=None, parent=None, **context):\n",
    "        if selector is None and response is not None:\n",
    "            try:\n",
    "                selector = self.default_selector_class(response)\n",
    "            except AttributeError:\n",
    "                selector = None\n",
    "        context.update(response=response)\n",
    "        super().__init__(item=item, selector=selector, parent=parent, **context)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53afa41e-90fd-4a5f-b2fe-5f4d31655f6b",
   "metadata": {},
   "source": [
    "## `itemadapter.adapter`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923fd1d2-62ca-441e-befb-60c89be1c1de",
   "metadata": {},
   "source": [
    "```python\n",
    "import dataclasses\n",
    "from abc import abstractmethod, ABCMeta\n",
    "from collections import deque\n",
    "from collections.abc import KeysView, MutableMapping\n",
    "from types import MappingProxyType\n",
    "from typing import Any, Deque, Iterator, Type, Optional, List\n",
    "\n",
    "from itemadapter.utils import (\n",
    "    _get_pydantic_model_metadata,\n",
    "    _is_attrs_class,\n",
    "    _is_pydantic_model,\n",
    ")\n",
    "\n",
    "from itemadapter._imports import attr, _scrapy_item_classes\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    \"AdapterInterface\",\n",
    "    \"AttrsAdapter\",\n",
    "    \"DataclassAdapter\",\n",
    "    \"DictAdapter\",\n",
    "    \"ItemAdapter\",\n",
    "    \"PydanticAdapter\",\n",
    "    \"ScrapyItemAdapter\",\n",
    "]\n",
    "\n",
    "\n",
    "class AdapterInterface(MutableMapping, metaclass=ABCMeta):\n",
    "    \"\"\"Abstract Base Class for adapters.\n",
    "\n",
    "    An adapter that handles a specific type of item should inherit from this\n",
    "    class and implement the abstract methods defined here, plus the\n",
    "    abtract methods inherited from the MutableMapping base class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, item: Any) -> None:\n",
    "        self.item = item\n",
    "\n",
    "    @classmethod\n",
    "    @abstractmethod\n",
    "    def is_item_class(cls, item_class: type) -> bool:\n",
    "        \"\"\"Return True if the adapter can handle the given item class, False otherwise.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def is_item(cls, item: Any) -> bool:\n",
    "        \"\"\"Return True if the adapter can handle the given item, False otherwise.\"\"\"\n",
    "        return cls.is_item_class(item.__class__)\n",
    "\n",
    "    @classmethod\n",
    "    def get_field_meta_from_class(cls, item_class: type, field_name: str) -> MappingProxyType:\n",
    "        return MappingProxyType({})\n",
    "\n",
    "    @classmethod\n",
    "    def get_field_names_from_class(cls, item_class: type) -> Optional[List[str]]:\n",
    "        \"\"\"Return a list of fields defined for ``item_class``.\n",
    "        If a class doesn't support fields, None is returned.\"\"\"\n",
    "        return None\n",
    "\n",
    "    def get_field_meta(self, field_name: str) -> MappingProxyType:\n",
    "        \"\"\"Return metadata for the given field name, if available.\"\"\"\n",
    "        return self.get_field_meta_from_class(self.item.__class__, field_name)\n",
    "\n",
    "    def field_names(self) -> KeysView:\n",
    "        \"\"\"Return a dynamic view of the item's field names.\"\"\"\n",
    "        return self.keys()  # type: ignore[return-value]\n",
    "\n",
    "\n",
    "class _MixinAttrsDataclassAdapter:\n",
    "\n",
    "    _fields_dict: dict\n",
    "    item: Any\n",
    "\n",
    "    def get_field_meta(self, field_name: str) -> MappingProxyType:\n",
    "        return self._fields_dict[field_name].metadata\n",
    "\n",
    "    def field_names(self) -> KeysView:\n",
    "        return KeysView(self._fields_dict)\n",
    "\n",
    "    def __getitem__(self, field_name: str) -> Any:\n",
    "        if field_name in self._fields_dict:\n",
    "            return getattr(self.item, field_name)\n",
    "        raise KeyError(field_name)\n",
    "\n",
    "    def __setitem__(self, field_name: str, value: Any) -> None:\n",
    "        if field_name in self._fields_dict:\n",
    "            setattr(self.item, field_name, value)\n",
    "        else:\n",
    "            raise KeyError(f\"{self.item.__class__.__name__} does not support field: {field_name}\")\n",
    "\n",
    "    def __delitem__(self, field_name: str) -> None:\n",
    "        if field_name in self._fields_dict:\n",
    "            try:\n",
    "                delattr(self.item, field_name)\n",
    "            except AttributeError:\n",
    "                raise KeyError(field_name)\n",
    "        else:\n",
    "            raise KeyError(f\"{self.item.__class__.__name__} does not support field: {field_name}\")\n",
    "\n",
    "    def __iter__(self) -> Iterator:\n",
    "        return iter(attr for attr in self._fields_dict if hasattr(self.item, attr))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(list(iter(self)))\n",
    "\n",
    "\n",
    "class AttrsAdapter(_MixinAttrsDataclassAdapter, AdapterInterface):\n",
    "    def __init__(self, item: Any) -> None:\n",
    "        super().__init__(item)\n",
    "        if attr is None:\n",
    "            raise RuntimeError(\"attr module is not available\")\n",
    "        # store a reference to the item's fields to avoid O(n) lookups and O(n^2) traversals\n",
    "        self._fields_dict = attr.fields_dict(self.item.__class__)\n",
    "\n",
    "    @classmethod\n",
    "    def is_item(cls, item: Any) -> bool:\n",
    "        return _is_attrs_class(item) and not isinstance(item, type)\n",
    "\n",
    "    @classmethod\n",
    "    def is_item_class(cls, item_class: type) -> bool:\n",
    "        return _is_attrs_class(item_class)\n",
    "\n",
    "    @classmethod\n",
    "    def get_field_meta_from_class(cls, item_class: type, field_name: str) -> MappingProxyType:\n",
    "        if attr is None:\n",
    "            raise RuntimeError(\"attr module is not available\")\n",
    "        try:\n",
    "            return attr.fields_dict(item_class)[field_name].metadata  # type: ignore\n",
    "        except KeyError:\n",
    "            raise KeyError(f\"{item_class.__name__} does not support field: {field_name}\")\n",
    "\n",
    "    @classmethod\n",
    "    def get_field_names_from_class(cls, item_class: type) -> Optional[List[str]]:\n",
    "        if attr is None:\n",
    "            raise RuntimeError(\"attr module is not available\")\n",
    "        return [a.name for a in attr.fields(item_class)]\n",
    "\n",
    "\n",
    "class DataclassAdapter(_MixinAttrsDataclassAdapter, AdapterInterface):\n",
    "    def __init__(self, item: Any) -> None:\n",
    "        super().__init__(item)\n",
    "        # store a reference to the item's fields to avoid O(n) lookups and O(n^2) traversals\n",
    "        self._fields_dict = {field.name: field for field in dataclasses.fields(self.item)}\n",
    "\n",
    "    @classmethod\n",
    "    def is_item(cls, item: Any) -> bool:\n",
    "        return dataclasses.is_dataclass(item) and not isinstance(item, type)\n",
    "\n",
    "    @classmethod\n",
    "    def is_item_class(cls, item_class: type) -> bool:\n",
    "        return dataclasses.is_dataclass(item_class)\n",
    "\n",
    "    @classmethod\n",
    "    def get_field_meta_from_class(cls, item_class: type, field_name: str) -> MappingProxyType:\n",
    "        for field in dataclasses.fields(item_class):\n",
    "            if field.name == field_name:\n",
    "                return field.metadata  # type: ignore\n",
    "        raise KeyError(f\"{item_class.__name__} does not support field: {field_name}\")\n",
    "\n",
    "    @classmethod\n",
    "    def get_field_names_from_class(cls, item_class: type) -> Optional[List[str]]:\n",
    "        return [a.name for a in dataclasses.fields(item_class)]\n",
    "\n",
    "\n",
    "class PydanticAdapter(AdapterInterface):\n",
    "\n",
    "    item: Any\n",
    "\n",
    "    @classmethod\n",
    "    def is_item_class(cls, item_class: type) -> bool:\n",
    "        return _is_pydantic_model(item_class)\n",
    "\n",
    "    @classmethod\n",
    "    def get_field_meta_from_class(cls, item_class: type, field_name: str) -> MappingProxyType:\n",
    "        try:\n",
    "            return _get_pydantic_model_metadata(item_class, field_name)\n",
    "        except KeyError:\n",
    "            raise KeyError(f\"{item_class.__name__} does not support field: {field_name}\")\n",
    "\n",
    "    @classmethod\n",
    "    def get_field_names_from_class(cls, item_class: type) -> Optional[List[str]]:\n",
    "        return list(item_class.__fields__.keys())  # type: ignore[attr-defined]\n",
    "\n",
    "    def field_names(self) -> KeysView:\n",
    "        return KeysView(self.item.__fields__)\n",
    "\n",
    "    def __getitem__(self, field_name: str) -> Any:\n",
    "        if field_name in self.item.__fields__:\n",
    "            return getattr(self.item, field_name)\n",
    "        raise KeyError(field_name)\n",
    "\n",
    "    def __setitem__(self, field_name: str, value: Any) -> None:\n",
    "        if field_name in self.item.__fields__:\n",
    "            setattr(self.item, field_name, value)\n",
    "        else:\n",
    "            raise KeyError(f\"{self.item.__class__.__name__} does not support field: {field_name}\")\n",
    "\n",
    "    def __delitem__(self, field_name: str) -> None:\n",
    "        if field_name in self.item.__fields__:\n",
    "            try:\n",
    "                delattr(self.item, field_name)\n",
    "            except AttributeError:\n",
    "                raise KeyError(field_name)\n",
    "        else:\n",
    "            raise KeyError(f\"{self.item.__class__.__name__} does not support field: {field_name}\")\n",
    "\n",
    "    def __iter__(self) -> Iterator:\n",
    "        return iter(attr for attr in self.item.__fields__ if hasattr(self.item, attr))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(list(iter(self)))\n",
    "\n",
    "\n",
    "class _MixinDictScrapyItemAdapter:\n",
    "\n",
    "    _fields_dict: dict\n",
    "    item: Any\n",
    "\n",
    "    def __getitem__(self, field_name: str) -> Any:\n",
    "        return self.item[field_name]\n",
    "\n",
    "    def __setitem__(self, field_name: str, value: Any) -> None:\n",
    "        self.item[field_name] = value\n",
    "\n",
    "    def __delitem__(self, field_name: str) -> None:\n",
    "        del self.item[field_name]\n",
    "\n",
    "    def __iter__(self) -> Iterator:\n",
    "        return iter(self.item)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.item)\n",
    "\n",
    "\n",
    "class DictAdapter(_MixinDictScrapyItemAdapter, AdapterInterface):\n",
    "    @classmethod\n",
    "    def is_item(cls, item: Any) -> bool:\n",
    "        return isinstance(item, dict)\n",
    "\n",
    "    @classmethod\n",
    "    def is_item_class(cls, item_class: type) -> bool:\n",
    "        return issubclass(item_class, dict)\n",
    "\n",
    "    def field_names(self) -> KeysView:\n",
    "        return KeysView(self.item)\n",
    "\n",
    "\n",
    "class ScrapyItemAdapter(_MixinDictScrapyItemAdapter, AdapterInterface):\n",
    "    @classmethod\n",
    "    def is_item(cls, item: Any) -> bool:\n",
    "        return isinstance(item, _scrapy_item_classes)\n",
    "\n",
    "    @classmethod\n",
    "    def is_item_class(cls, item_class: type) -> bool:\n",
    "        return issubclass(item_class, _scrapy_item_classes)\n",
    "\n",
    "    @classmethod\n",
    "    def get_field_meta_from_class(cls, item_class: type, field_name: str) -> MappingProxyType:\n",
    "        return MappingProxyType(item_class.fields[field_name])  # type: ignore[attr-defined]\n",
    "\n",
    "    @classmethod\n",
    "    def get_field_names_from_class(cls, item_class: type) -> Optional[List[str]]:\n",
    "        return list(item_class.fields.keys())  # type: ignore[attr-defined]\n",
    "\n",
    "    def field_names(self) -> KeysView:\n",
    "        return KeysView(self.item.fields)\n",
    "\n",
    "\n",
    "[docs]class ItemAdapter(MutableMapping):\n",
    "    \"\"\"Wrapper class to interact with data container objects. It provides a common interface\n",
    "    to extract and set data without having to take the object's type into account.\n",
    "    \"\"\"\n",
    "\n",
    "    ADAPTER_CLASSES: Deque[Type[AdapterInterface]] = deque(\n",
    "        [\n",
    "            ScrapyItemAdapter,\n",
    "            DictAdapter,\n",
    "            DataclassAdapter,\n",
    "            AttrsAdapter,\n",
    "            PydanticAdapter,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    def __init__(self, item: Any) -> None:\n",
    "        for cls in self.ADAPTER_CLASSES:\n",
    "            if cls.is_item(item):\n",
    "                self.adapter = cls(item)\n",
    "                break\n",
    "        else:\n",
    "            raise TypeError(f\"No adapter found for objects of type: {type(item)} ({item})\")\n",
    "\n",
    "    @classmethod\n",
    "    def is_item(cls, item: Any) -> bool:\n",
    "        for adapter_class in cls.ADAPTER_CLASSES:\n",
    "            if adapter_class.is_item(item):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    @classmethod\n",
    "    def is_item_class(cls, item_class: type) -> bool:\n",
    "        for adapter_class in cls.ADAPTER_CLASSES:\n",
    "            if adapter_class.is_item_class(item_class):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    @classmethod\n",
    "    def _get_adapter_class(cls, item_class: type) -> Type[AdapterInterface]:\n",
    "        for adapter_class in cls.ADAPTER_CLASSES:\n",
    "            if adapter_class.is_item_class(item_class):\n",
    "                return adapter_class\n",
    "        raise TypeError(f\"{item_class} is not a valid item class\")\n",
    "\n",
    "    @classmethod\n",
    "    def get_field_meta_from_class(cls, item_class: type, field_name: str) -> MappingProxyType:\n",
    "        adapter_class = cls._get_adapter_class(item_class)\n",
    "        return adapter_class.get_field_meta_from_class(item_class, field_name)\n",
    "\n",
    "    @classmethod\n",
    "    def get_field_names_from_class(cls, item_class: type) -> Optional[List[str]]:\n",
    "        adapter_class = cls._get_adapter_class(item_class)\n",
    "        return adapter_class.get_field_names_from_class(item_class)\n",
    "\n",
    "    @property\n",
    "    def item(self) -> Any:\n",
    "        return self.adapter.item\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        values = \", \".join([f\"{key}={value!r}\" for key, value in self.items()])\n",
    "        return f\"<{self.__class__.__name__} for {self.item.__class__.__name__}({values})>\"\n",
    "\n",
    "    def __getitem__(self, field_name: str) -> Any:\n",
    "        return self.adapter.__getitem__(field_name)\n",
    "\n",
    "    def __setitem__(self, field_name: str, value: Any) -> None:\n",
    "        self.adapter.__setitem__(field_name, value)\n",
    "\n",
    "    def __delitem__(self, field_name: str) -> None:\n",
    "        self.adapter.__delitem__(field_name)\n",
    "\n",
    "    def __iter__(self) -> Iterator:\n",
    "        return self.adapter.__iter__()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.adapter.__len__()\n",
    "\n",
    "    def get_field_meta(self, field_name: str) -> MappingProxyType:\n",
    "        \"\"\"Return metadata for the given field name.\"\"\"\n",
    "        return self.adapter.get_field_meta(field_name)\n",
    "\n",
    "    def field_names(self) -> KeysView:\n",
    "        \"\"\"Return read-only key view with the names of all the defined fields for the item.\"\"\"\n",
    "        return self.adapter.field_names()\n",
    "\n",
    "    def asdict(self) -> dict:\n",
    "        \"\"\"Return a dict object with the contents of the adapter. This works slightly different\n",
    "        than calling `dict(adapter)`: it's applied recursively to nested items (if there are any).\n",
    "        \"\"\"\n",
    "        return {key: self._asdict(value) for key, value in self.items()}\n",
    "\n",
    "    @classmethod\n",
    "    def _asdict(cls, obj: Any) -> Any:\n",
    "        if isinstance(obj, dict):\n",
    "            return {key: cls._asdict(value) for key, value in obj.items()}\n",
    "        if isinstance(obj, (list, set, tuple)):\n",
    "            return obj.__class__(cls._asdict(x) for x in obj)\n",
    "        if isinstance(obj, cls):\n",
    "            return obj.asdict()\n",
    "        if cls.is_item(obj):\n",
    "            return cls(obj).asdict()\n",
    "        return obj\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23458589-7d4a-490f-ba23-565f3fcc9b54",
   "metadata": {},
   "source": [
    "## `itemloaders`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725d3a2b-b164-43d4-b690-b8d381c5176a",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"\n",
    "Item Loader\n",
    "\n",
    "See documentation in docs/topics/loaders.rst\n",
    "\"\"\"\n",
    "from contextlib import suppress\n",
    "\n",
    "from itemadapter import ItemAdapter\n",
    "from parsel.utils import extract_regex, flatten\n",
    "\n",
    "from itemloaders.common import wrap_loader_context\n",
    "from itemloaders.processors import Identity\n",
    "from itemloaders.utils import arg_to_iter\n",
    "\n",
    "\n",
    "def unbound_method(method):\n",
    "    \"\"\"\n",
    "    Allow to use single-argument functions as input or output processors\n",
    "    (no need to define an unused first 'self' argument)\n",
    "    \"\"\"\n",
    "    with suppress(AttributeError):\n",
    "        if \".\" not in method.__qualname__:\n",
    "            return method.__func__\n",
    "    return method\n",
    "\n",
    "\n",
    "class ItemLoader:\n",
    "    \"\"\"\n",
    "    Return a new Item Loader for populating the given item. If no item is\n",
    "    given, one is instantiated automatically using the class in\n",
    "    :attr:`default_item_class`.\n",
    "\n",
    "    When instantiated with a :param ``selector`` parameter the :class:`ItemLoader` class\n",
    "    provides convenient mechanisms for extracting data from web pages\n",
    "    using parsel_ selectors.\n",
    "\n",
    "    :param item: The item instance to populate using subsequent calls to\n",
    "        :meth:`~ItemLoader.add_xpath`, :meth:`~ItemLoader.add_css`,\n",
    "        :meth:`~ItemLoader.add_jmes` or :meth:`~ItemLoader.add_value`.\n",
    "    :type item: :class:`dict` object\n",
    "\n",
    "    :param selector: The selector to extract data from, when using the\n",
    "        :meth:`add_xpath` (resp. :meth:`add_css`, :meth:`add_jmes`) or :meth:`replace_xpath`\n",
    "        (resp. :meth:`replace_css`, :meth:`replace_jmes`) method.\n",
    "    :type selector: :class:`~parsel.selector.Selector` object\n",
    "\n",
    "    The item, selector and the remaining keyword arguments are\n",
    "    assigned to the Loader context (accessible through the :attr:`context` attribute).\n",
    "\n",
    "    .. attribute:: item\n",
    "\n",
    "        The item object being parsed by this Item Loader.\n",
    "        This is mostly used as a property so when attempting to override this\n",
    "        value, you may want to check out :attr:`default_item_class` first.\n",
    "\n",
    "    .. attribute:: context\n",
    "\n",
    "        The currently active :ref:`Context <loaders-context>` of this Item Loader.\n",
    "        Refer to <loaders-context> for more information about the Loader Context.\n",
    "\n",
    "    .. attribute:: default_item_class\n",
    "\n",
    "        An Item class (or factory), used to instantiate items when not given in\n",
    "        the ``__init__`` method.\n",
    "\n",
    "        .. warning:: Currently, this factory/class needs to be\n",
    "            callable/instantiated without any arguments.\n",
    "            If you are using ``dataclasses``, please consider the following\n",
    "            alternative::\n",
    "\n",
    "                from dataclasses import dataclass, field\n",
    "                from typing import Optional\n",
    "\n",
    "                @dataclass\n",
    "                class Product:\n",
    "                    name: Optional[str] = field(default=None)\n",
    "                    price: Optional[float] = field(default=None)\n",
    "\n",
    "    .. attribute:: default_input_processor\n",
    "\n",
    "        The default input processor to use for those fields which don't specify\n",
    "        one.\n",
    "\n",
    "    .. attribute:: default_output_processor\n",
    "\n",
    "        The default output processor to use for those fields which don't specify\n",
    "        one.\n",
    "\n",
    "    .. attribute:: selector\n",
    "\n",
    "        The :class:`~parsel.selector.Selector` object to extract data from.\n",
    "        It's the selector given in the ``__init__`` method.\n",
    "        This attribute is meant to be read-only.\n",
    "\n",
    "    .. _parsel: https://parsel.readthedocs.io/en/latest/\n",
    "    \"\"\"\n",
    "\n",
    "    default_item_class = dict\n",
    "    default_input_processor = Identity()\n",
    "    default_output_processor = Identity()\n",
    "\n",
    "    def __init__(self, item=None, selector=None, parent=None, **context):\n",
    "        self.selector = selector\n",
    "        context.update(selector=selector)\n",
    "        if item is None:\n",
    "            item = self.default_item_class()\n",
    "        self._local_item = item\n",
    "        context[\"item\"] = item\n",
    "        self.context = context\n",
    "        self.parent = parent\n",
    "        self._local_values = {}\n",
    "        # values from initial item\n",
    "        for field_name, value in ItemAdapter(item).items():\n",
    "            self._values.setdefault(field_name, [])\n",
    "            self._values[field_name] += arg_to_iter(value)\n",
    "\n",
    "    @property\n",
    "    def _values(self):\n",
    "        if self.parent is not None:\n",
    "            return self.parent._values\n",
    "        else:\n",
    "            return self._local_values\n",
    "\n",
    "    @property\n",
    "    def item(self):\n",
    "        if self.parent is not None:\n",
    "            return self.parent.item\n",
    "        else:\n",
    "            return self._local_item\n",
    "\n",
    "    def nested_xpath(self, xpath, **context):\n",
    "        \"\"\"\n",
    "        Create a nested loader with an xpath selector.\n",
    "        The supplied selector is applied relative to selector associated\n",
    "        with this :class:`ItemLoader`. The nested loader shares the item\n",
    "        with the parent :class:`ItemLoader` so calls to :meth:`add_xpath`,\n",
    "        :meth:`add_value`, :meth:`replace_value`, etc. will behave as expected.\n",
    "        \"\"\"\n",
    "        selector = self.selector.xpath(xpath)\n",
    "        context.update(selector=selector)\n",
    "        subloader = self.__class__(item=self.item, parent=self, **context)\n",
    "        return subloader\n",
    "\n",
    "\n",
    "    def nested_css(self, css, **context):\n",
    "        \"\"\"\n",
    "        Create a nested loader with a css selector.\n",
    "        The supplied selector is applied relative to selector associated\n",
    "        with this :class:`ItemLoader`. The nested loader shares the item\n",
    "        with the parent :class:`ItemLoader` so calls to :meth:`add_xpath`,\n",
    "        :meth:`add_value`, :meth:`replace_value`, etc. will behave as expected.\n",
    "        \"\"\"\n",
    "        selector = self.selector.css(css)\n",
    "        context.update(selector=selector)\n",
    "        subloader = self.__class__(item=self.item, parent=self, **context)\n",
    "        return subloader\n",
    "\n",
    "\n",
    "    def add_value(self, field_name, value, *processors, re=None, **kw):\n",
    "        \"\"\"\n",
    "        Process and then add the given ``value`` for the given field.\n",
    "\n",
    "        The value is first passed through :meth:`get_value` by giving the\n",
    "        ``processors`` and ``kwargs``, and then passed through the\n",
    "        :ref:`field input processor <processors>` and its result\n",
    "        appended to the data collected for that field. If the field already\n",
    "        contains collected data, the new data is added.\n",
    "\n",
    "        The given ``field_name`` can be ``None``, in which case values for\n",
    "        multiple fields may be added. And the processed value should be a dict\n",
    "        with field_name mapped to values.\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            loader.add_value('name', 'Color TV')\n",
    "            loader.add_value('colours', ['white', 'blue'])\n",
    "            loader.add_value('length', '100')\n",
    "            loader.add_value('name', 'name: foo', TakeFirst(), re='name: (.+)')\n",
    "            loader.add_value(None, {'name': 'foo', 'sex': 'male'})\n",
    "        \"\"\"\n",
    "        value = self.get_value(value, *processors, re=re, **kw)\n",
    "        if value is None:\n",
    "            return\n",
    "        if not field_name:\n",
    "            for k, v in value.items():\n",
    "                self._add_value(k, v)\n",
    "        else:\n",
    "            self._add_value(field_name, value)\n",
    "\n",
    "\n",
    "    def replace_value(self, field_name, value, *processors, re=None, **kw):\n",
    "        \"\"\"\n",
    "        Similar to :meth:`add_value` but replaces the collected data with the\n",
    "        new value instead of adding it.\n",
    "        \"\"\"\n",
    "        value = self.get_value(value, *processors, re=re, **kw)\n",
    "        if value is None:\n",
    "            return\n",
    "        if not field_name:\n",
    "            for k, v in value.items():\n",
    "                self._replace_value(k, v)\n",
    "        else:\n",
    "            self._replace_value(field_name, value)\n",
    "\n",
    "\n",
    "    def _add_value(self, field_name, value):\n",
    "        value = arg_to_iter(value)\n",
    "        processed_value = self._process_input_value(field_name, value)\n",
    "        if processed_value:\n",
    "            self._values.setdefault(field_name, [])\n",
    "            self._values[field_name] += arg_to_iter(processed_value)\n",
    "\n",
    "    def _replace_value(self, field_name, value):\n",
    "        self._values.pop(field_name, None)\n",
    "        self._add_value(field_name, value)\n",
    "\n",
    "    def get_value(self, value, *processors, re=None, **kw):\n",
    "        \"\"\"\n",
    "        Process the given ``value`` by the given ``processors`` and keyword\n",
    "        arguments.\n",
    "\n",
    "        Available keyword arguments:\n",
    "\n",
    "        :param re: a regular expression to use for extracting data from the\n",
    "            given value using :func:`~parsel.utils.extract_regex` method,\n",
    "            applied before processors\n",
    "        :type re: str or typing.Pattern\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        >>> from itemloaders import ItemLoader\n",
    "        >>> from itemloaders.processors import TakeFirst\n",
    "        >>> loader = ItemLoader()\n",
    "        >>> loader.get_value('name: foo', TakeFirst(), str.upper, re='name: (.+)')\n",
    "        'FOO'\n",
    "        \"\"\"\n",
    "        if re:\n",
    "            value = arg_to_iter(value)\n",
    "            value = flatten(extract_regex(re, x) for x in value)\n",
    "\n",
    "        for proc in processors:\n",
    "            if value is None:\n",
    "                break\n",
    "            _proc = proc\n",
    "            proc = wrap_loader_context(proc, self.context)\n",
    "            try:\n",
    "                value = proc(value)\n",
    "            except Exception as e:\n",
    "                raise ValueError(\n",
    "                    \"Error with processor %s value=%r error='%s: %s'\"\n",
    "                    % (_proc.__class__.__name__, value, type(e).__name__, str(e))\n",
    "                ) from e\n",
    "        return value\n",
    "\n",
    "\n",
    "    def load_item(self):\n",
    "        \"\"\"\n",
    "        Populate the item with the data collected so far, and return it. The\n",
    "        data collected is first passed through the :ref:`output processors\n",
    "        <processors>` to get the final value to assign to each item field.\n",
    "        \"\"\"\n",
    "        adapter = ItemAdapter(self.item)\n",
    "        for field_name in tuple(self._values):\n",
    "            value = self.get_output_value(field_name)\n",
    "            if value is not None:\n",
    "                adapter[field_name] = value\n",
    "\n",
    "        return adapter.item\n",
    "\n",
    "\n",
    "    def get_output_value(self, field_name):\n",
    "        \"\"\"\n",
    "        Return the collected values parsed using the output processor, for the\n",
    "        given field. This method doesn't populate or modify the item at all.\n",
    "        \"\"\"\n",
    "        proc = self.get_output_processor(field_name)\n",
    "        proc = wrap_loader_context(proc, self.context)\n",
    "        value = self._values.get(field_name, [])\n",
    "        try:\n",
    "            return proc(value)\n",
    "        except Exception as e:\n",
    "            raise ValueError(\n",
    "                \"Error with output processor: field=%r value=%r error='%s: %s'\"\n",
    "                % (field_name, value, type(e).__name__, str(e))\n",
    "            ) from e\n",
    "\n",
    "\n",
    "    def get_collected_values(self, field_name):\n",
    "        \"\"\"Return the collected values for the given field.\"\"\"\n",
    "        return self._values.get(field_name, [])\n",
    "\n",
    "\n",
    "    def get_input_processor(self, field_name):\n",
    "        proc = getattr(self, \"%s_in\" % field_name, None)\n",
    "        if not proc:\n",
    "            proc = self._get_item_field_attr(\n",
    "                field_name, \"input_processor\", self.default_input_processor\n",
    "            )\n",
    "        return unbound_method(proc)\n",
    "\n",
    "    def get_output_processor(self, field_name):\n",
    "        proc = getattr(self, \"%s_out\" % field_name, None)\n",
    "        if not proc:\n",
    "            proc = self._get_item_field_attr(\n",
    "                field_name, \"output_processor\", self.default_output_processor\n",
    "            )\n",
    "        return unbound_method(proc)\n",
    "\n",
    "    def _get_item_field_attr(self, field_name, key, default=None):\n",
    "        field_meta = ItemAdapter(self.item).get_field_meta(field_name)\n",
    "        return field_meta.get(key, default)\n",
    "\n",
    "    def _process_input_value(self, field_name, value):\n",
    "        proc = self.get_input_processor(field_name)\n",
    "        _proc = proc\n",
    "        proc = wrap_loader_context(proc, self.context)\n",
    "        try:\n",
    "            return proc(value)\n",
    "        except Exception as e:\n",
    "            raise ValueError(\n",
    "                \"Error with input processor %s: field=%r value=%r \"\n",
    "                \"error='%s: %s'\"\n",
    "                % (\n",
    "                    _proc.__class__.__name__,\n",
    "                    field_name,\n",
    "                    value,\n",
    "                    type(e).__name__,\n",
    "                    str(e),\n",
    "                )\n",
    "            ) from e\n",
    "\n",
    "    def _check_selector_method(self):\n",
    "        if self.selector is None:\n",
    "            raise RuntimeError(\n",
    "                \"To use XPath or CSS selectors, %s \"\n",
    "                \"must be instantiated with a selector\" % self.__class__.__name__\n",
    "            )\n",
    "\n",
    "    def add_xpath(self, field_name, xpath, *processors, re=None, **kw):\n",
    "        \"\"\"\n",
    "        Similar to :meth:`ItemLoader.add_value` but receives an XPath instead of a\n",
    "        value, which is used to extract a list of strings from the\n",
    "        selector associated with this :class:`ItemLoader`.\n",
    "\n",
    "        See :meth:`get_xpath` for ``kwargs``.\n",
    "\n",
    "        :param xpath: the XPath to extract data from\n",
    "        :type xpath: str\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            # HTML snippet: <p class=\"product-name\">Color TV</p>\n",
    "            loader.add_xpath('name', '//p[@class=\"product-name\"]')\n",
    "            # HTML snippet: <p id=\"price\">the price is $1200</p>\n",
    "            loader.add_xpath('price', '//p[@id=\"price\"]', re='the price is (.*)')\n",
    "\n",
    "        \"\"\"\n",
    "        values = self._get_xpathvalues(xpath, **kw)\n",
    "        self.add_value(field_name, values, *processors, re=re, **kw)\n",
    "\n",
    "\n",
    "    def replace_xpath(self, field_name, xpath, *processors, re=None, **kw):\n",
    "        \"\"\"\n",
    "        Similar to :meth:`add_xpath` but replaces collected data instead of adding it.\n",
    "        \"\"\"\n",
    "        values = self._get_xpathvalues(xpath, **kw)\n",
    "        self.replace_value(field_name, values, *processors, re=re, **kw)\n",
    "\n",
    "\n",
    "    def get_xpath(self, xpath, *processors, re=None, **kw):\n",
    "        \"\"\"\n",
    "        Similar to :meth:`ItemLoader.get_value` but receives an XPath instead of a\n",
    "        value, which is used to extract a list of unicode strings from the\n",
    "        selector associated with this :class:`ItemLoader`.\n",
    "\n",
    "        :param xpath: the XPath to extract data from\n",
    "        :type xpath: str\n",
    "\n",
    "        :param re: a regular expression to use for extracting data from the\n",
    "            selected XPath region\n",
    "        :type re: str or typing.Pattern\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            # HTML snippet: <p class=\"product-name\">Color TV</p>\n",
    "            loader.get_xpath('//p[@class=\"product-name\"]')\n",
    "            # HTML snippet: <p id=\"price\">the price is $1200</p>\n",
    "            loader.get_xpath('//p[@id=\"price\"]', TakeFirst(), re='the price is (.*)')\n",
    "\n",
    "        \"\"\"\n",
    "        values = self._get_xpathvalues(xpath, **kw)\n",
    "        return self.get_value(values, *processors, re=re, **kw)\n",
    "\n",
    "\n",
    "    def _get_xpathvalues(self, xpaths, **kw):\n",
    "        self._check_selector_method()\n",
    "        xpaths = arg_to_iter(xpaths)\n",
    "        return flatten(self.selector.xpath(xpath, **kw).getall() for xpath in xpaths)\n",
    "\n",
    "    def add_css(self, field_name, css, *processors, re=None, **kw):\n",
    "        \"\"\"\n",
    "        Similar to :meth:`ItemLoader.add_value` but receives a CSS selector\n",
    "        instead of a value, which is used to extract a list of unicode strings\n",
    "        from the selector associated with this :class:`ItemLoader`.\n",
    "\n",
    "        See :meth:`get_css` for ``kwargs``.\n",
    "\n",
    "        :param css: the CSS selector to extract data from\n",
    "        :type css: str\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            # HTML snippet: <p class=\"product-name\">Color TV</p>\n",
    "            loader.add_css('name', 'p.product-name')\n",
    "            # HTML snippet: <p id=\"price\">the price is $1200</p>\n",
    "            loader.add_css('price', 'p#price', re='the price is (.*)')\n",
    "        \"\"\"\n",
    "        values = self._get_cssvalues(css)\n",
    "        self.add_value(field_name, values, *processors, re=re, **kw)\n",
    "\n",
    "\n",
    "    def replace_css(self, field_name, css, *processors, re=None, **kw):\n",
    "        \"\"\"\n",
    "        Similar to :meth:`add_css` but replaces collected data instead of adding it.\n",
    "        \"\"\"\n",
    "        values = self._get_cssvalues(css)\n",
    "        self.replace_value(field_name, values, *processors, re=re, **kw)\n",
    "\n",
    "\n",
    "    def get_css(self, css, *processors, re=None, **kw):\n",
    "        \"\"\"\n",
    "        Similar to :meth:`ItemLoader.get_value` but receives a CSS selector\n",
    "        instead of a value, which is used to extract a list of unicode strings\n",
    "        from the selector associated with this :class:`ItemLoader`.\n",
    "\n",
    "        :param css: the CSS selector to extract data from\n",
    "        :type css: str\n",
    "\n",
    "        :param re: a regular expression to use for extracting data from the\n",
    "            selected CSS region\n",
    "        :type re: str or typing.Pattern\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            # HTML snippet: <p class=\"product-name\">Color TV</p>\n",
    "            loader.get_css('p.product-name')\n",
    "            # HTML snippet: <p id=\"price\">the price is $1200</p>\n",
    "            loader.get_css('p#price', TakeFirst(), re='the price is (.*)')\n",
    "        \"\"\"\n",
    "        values = self._get_cssvalues(css)\n",
    "        return self.get_value(values, *processors, re=re, **kw)\n",
    "\n",
    "\n",
    "    def _get_cssvalues(self, csss):\n",
    "        self._check_selector_method()\n",
    "        csss = arg_to_iter(csss)\n",
    "        return flatten(self.selector.css(css).getall() for css in csss)\n",
    "\n",
    "    def add_jmes(self, field_name, jmes, *processors, re=None, **kw):\n",
    "        \"\"\"\n",
    "        Similar to :meth:`ItemLoader.add_value` but receives a JMESPath selector\n",
    "        instead of a value, which is used to extract a list of unicode strings\n",
    "        from the selector associated with this :class:`ItemLoader`.\n",
    "\n",
    "        See :meth:`get_jmes` for ``kwargs``.\n",
    "\n",
    "        :param jmes: the JMESPath selector to extract data from\n",
    "        :type jmes: str\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            # HTML snippet: {\"name\": \"Color TV\"}\n",
    "            loader.add_jmes('name')\n",
    "            # HTML snippet: {\"price\": the price is $1200\"}\n",
    "            loader.add_jmes('price', TakeFirst(), re='the price is (.*)')\n",
    "        \"\"\"\n",
    "        values = self._get_jmesvalues(jmes)\n",
    "        self.add_value(field_name, values, *processors, re=re, **kw)\n",
    "\n",
    "\n",
    "    def replace_jmes(self, field_name, jmes, *processors, re=None, **kw):\n",
    "        \"\"\"\n",
    "        Similar to :meth:`add_jmes` but replaces collected data instead of adding it.\n",
    "        \"\"\"\n",
    "        values = self._get_jmesvalues(jmes)\n",
    "        self.replace_value(field_name, values, *processors, re=re, **kw)\n",
    "\n",
    "\n",
    "    def get_jmes(self, jmes, *processors, re=None, **kw):\n",
    "        \"\"\"\n",
    "        Similar to :meth:`ItemLoader.get_value` but receives a JMESPath selector\n",
    "        instead of a value, which is used to extract a list of unicode strings\n",
    "        from the selector associated with this :class:`ItemLoader`.\n",
    "\n",
    "        :param jmes: the JMESPath selector to extract data from\n",
    "        :type jmes: str\n",
    "\n",
    "        :param re: a regular expression to use for extracting data from the\n",
    "            selected JMESPath\n",
    "        :type re: str or typing.Pattern\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            # HTML snippet: {\"name\": \"Color TV\"}\n",
    "            loader.get_jmes('name')\n",
    "            # HTML snippet: {\"price\": the price is $1200\"}\n",
    "            loader.get_jmes('price', TakeFirst(), re='the price is (.*)')\n",
    "        \"\"\"\n",
    "        values = self._get_jmesvalues(jmes)\n",
    "        return self.get_value(values, *processors, re=re, **kw)\n",
    "\n",
    "\n",
    "    def _get_jmesvalues(self, jmess):\n",
    "        self._check_selector_method()\n",
    "        jmess = arg_to_iter(jmess)\n",
    "        if not hasattr(self.selector, \"jmespath\"):\n",
    "            raise AttributeError(\n",
    "                \"Please install parsel >= 1.8.1 to get jmespath support\"\n",
    "            )\n",
    "        return flatten(self.selector.jmespath(jmes).getall() for jmes in jmess)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2758e087-2ac4-4bfe-90bd-92122080988f",
   "metadata": {},
   "source": [
    "## `itemloaders.processors`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e6cae2-f373-4750-98ec-8e4b1ec95787",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"\n",
    "This module provides some commonly used processors for Item Loaders.\n",
    "\n",
    "See documentation in docs/topics/loaders.rst\n",
    "\"\"\"\n",
    "from collections import ChainMap\n",
    "\n",
    "from itemloaders.common import wrap_loader_context\n",
    "from itemloaders.utils import arg_to_iter\n",
    "\n",
    "\n",
    "class MapCompose:\n",
    "    \"\"\"\n",
    "    A processor which is constructed from the composition of the given\n",
    "    functions, similar to the :class:`Compose` processor. The difference with\n",
    "    this processor is the way internal results are passed among functions,\n",
    "    which is as follows:\n",
    "\n",
    "    The input value of this processor is *iterated* and the first function is\n",
    "    applied to each element. The results of these function calls (one for each element)\n",
    "    are concatenated to construct a new iterable, which is then used to apply the\n",
    "    second function, and so on, until the last function is applied to each\n",
    "    value of the list of values collected so far. The output values of the last\n",
    "    function are concatenated together to produce the output of this processor.\n",
    "\n",
    "    Each particular function can return a value or a list of values, which is\n",
    "    flattened with the list of values returned by the same function applied to\n",
    "    the other input values. The functions can also return ``None`` in which\n",
    "    case the output of that function is ignored for further processing over the\n",
    "    chain.\n",
    "\n",
    "    This processor provides a convenient way to compose functions that only\n",
    "    work with single values (instead of iterables). For this reason the\n",
    "    :class:`MapCompose` processor is typically used as input processor, since\n",
    "    data is often extracted using the\n",
    "    :meth:`~parsel.selector.Selector.extract` method of `parsel selectors`_,\n",
    "    which returns a list of unicode strings.\n",
    "\n",
    "    The example below should clarify how it works:\n",
    "\n",
    "    >>> def filter_world(x):\n",
    "    ...     return None if x == 'world' else x\n",
    "    ...\n",
    "    >>> from itemloaders.processors import MapCompose\n",
    "    >>> proc = MapCompose(filter_world, str.upper)\n",
    "    >>> proc(['hello', 'world', 'this', 'is', 'something'])\n",
    "    ['HELLO', 'THIS', 'IS', 'SOMETHING']\n",
    "\n",
    "    As with the Compose processor, functions can receive Loader contexts, and\n",
    "    ``__init__`` method keyword arguments are used as default context values.\n",
    "    See :class:`Compose` processor for more info.\n",
    "\n",
    "    .. _`parsel selectors`: https://parsel.readthedocs.io/en/latest/parsel.html#parsel.selector.Selector.extract\n",
    "    \"\"\"  # noqa\n",
    "\n",
    "    def __init__(self, *functions, **default_loader_context):\n",
    "        self.functions = functions\n",
    "        self.default_loader_context = default_loader_context\n",
    "\n",
    "    def __call__(self, value, loader_context=None):\n",
    "        values = arg_to_iter(value)\n",
    "        if loader_context:\n",
    "            context = ChainMap(loader_context, self.default_loader_context)\n",
    "        else:\n",
    "            context = self.default_loader_context\n",
    "        wrapped_funcs = [wrap_loader_context(f, context) for f in self.functions]\n",
    "        for func in wrapped_funcs:\n",
    "            next_values = []\n",
    "            for v in values:\n",
    "                try:\n",
    "                    next_values += arg_to_iter(func(v))\n",
    "                except Exception as e:\n",
    "                    raise ValueError(\n",
    "                        \"Error in MapCompose with \"\n",
    "                        \"%s value=%r error='%s: %s'\"\n",
    "                        % (str(func), value, type(e).__name__, str(e))\n",
    "                    ) from e\n",
    "            values = next_values\n",
    "        return values\n",
    "\n",
    "\n",
    "\n",
    "class Compose:\n",
    "    \"\"\"\n",
    "    A processor which is constructed from the composition of the given\n",
    "    functions. This means that each input value of this processor is passed to\n",
    "    the first function, and the result of that function is passed to the second\n",
    "    function, and so on, until the last function returns the output value of\n",
    "    this processor.\n",
    "\n",
    "    By default, stop process on ``None`` value. This behaviour can be changed by\n",
    "    passing keyword argument ``stop_on_none=False``.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    >>> from itemloaders.processors import Compose\n",
    "    >>> proc = Compose(lambda v: v[0], str.upper)\n",
    "    >>> proc(['hello', 'world'])\n",
    "    'HELLO'\n",
    "\n",
    "    Each function can optionally receive a ``loader_context`` parameter. For\n",
    "    those which do, this processor will pass the currently active :ref:`Loader\n",
    "    context <loaders-context>` through that parameter.\n",
    "\n",
    "    The keyword arguments passed in the ``__init__`` method are used as the default\n",
    "    Loader context values passed to each function call. However, the final\n",
    "    Loader context values passed to functions are overridden with the currently\n",
    "    active Loader context accessible through the :attr:`ItemLoader.context\n",
    "    <itemloaders.ItemLoader.context>` attribute.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *functions, **default_loader_context):\n",
    "        self.functions = functions\n",
    "        self.stop_on_none = default_loader_context.get(\"stop_on_none\", True)\n",
    "        self.default_loader_context = default_loader_context\n",
    "\n",
    "    def __call__(self, value, loader_context=None):\n",
    "        if loader_context:\n",
    "            context = ChainMap(loader_context, self.default_loader_context)\n",
    "        else:\n",
    "            context = self.default_loader_context\n",
    "        wrapped_funcs = [wrap_loader_context(f, context) for f in self.functions]\n",
    "        for func in wrapped_funcs:\n",
    "            if value is None and self.stop_on_none:\n",
    "                break\n",
    "            try:\n",
    "                value = func(value)\n",
    "            except Exception as e:\n",
    "                raise ValueError(\n",
    "                    \"Error in Compose with \"\n",
    "                    \"%s value=%r error='%s: %s'\"\n",
    "                    % (str(func), value, type(e).__name__, str(e))\n",
    "                ) from e\n",
    "        return value\n",
    "\n",
    "\n",
    "\n",
    "class TakeFirst:\n",
    "    \"\"\"\n",
    "    Returns the first non-null/non-empty value from the values received,\n",
    "    so it's typically used as an output processor to single-valued fields.\n",
    "    It doesn't receive any ``__init__`` method arguments, nor does it accept Loader contexts.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    >>> from itemloaders.processors import TakeFirst\n",
    "    >>> proc = TakeFirst()\n",
    "    >>> proc(['', 'one', 'two', 'three'])\n",
    "    'one'\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, values):\n",
    "        for value in values:\n",
    "            if value is not None and value != \"\":\n",
    "                return value\n",
    "\n",
    "\n",
    "\n",
    "class Identity:\n",
    "    \"\"\"\n",
    "    The simplest processor, which doesn't do anything. It returns the original\n",
    "    values unchanged. It doesn't receive any ``__init__`` method arguments, nor does it\n",
    "    accept Loader contexts.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    >>> from itemloaders.processors import Identity\n",
    "    >>> proc = Identity()\n",
    "    >>> proc(['one', 'two', 'three'])\n",
    "    ['one', 'two', 'three']\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, values):\n",
    "        return values\n",
    "\n",
    "\n",
    "\n",
    "class SelectJmes:\n",
    "    \"\"\"\n",
    "    Query the input string for the jmespath (given at instantiation), and return the answer\n",
    "    Requires : jmespath(https://github.com/jmespath/jmespath)\n",
    "    Note: SelectJmes accepts only one input element at a time.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    >>> from itemloaders.processors import SelectJmes, Compose, MapCompose\n",
    "    >>> proc = SelectJmes(\"foo\") #for direct use on lists and dictionaries\n",
    "    >>> proc({'foo': 'bar'})\n",
    "    'bar'\n",
    "    >>> proc({'foo': {'bar': 'baz'}})\n",
    "    {'bar': 'baz'}\n",
    "\n",
    "    Working with Json:\n",
    "\n",
    "    >>> import json\n",
    "    >>> proc_single_json_str = Compose(json.loads, SelectJmes(\"foo\"))\n",
    "    >>> proc_single_json_str('{\"foo\": \"bar\"}')\n",
    "    'bar'\n",
    "    >>> proc_json_list = Compose(json.loads, MapCompose(SelectJmes('foo')))\n",
    "    >>> proc_json_list('[{\"foo\":\"bar\"}, {\"baz\":\"tar\"}]')\n",
    "    ['bar']\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, json_path):\n",
    "        self.json_path = json_path\n",
    "        import jmespath\n",
    "\n",
    "        self.compiled_path = jmespath.compile(self.json_path)\n",
    "\n",
    "    def __call__(self, value):\n",
    "        \"\"\"Query value for the jmespath query and return answer\n",
    "        :param value: a data structure (dict, list) to extract from\n",
    "        :return: Element extracted according to jmespath query\n",
    "        \"\"\"\n",
    "        return self.compiled_path.search(value)\n",
    "\n",
    "\n",
    "\n",
    "class Join:\n",
    "    \"\"\"\n",
    "    Returns the values joined with the separator given in the ``__init__`` method, which\n",
    "    defaults to ``' '``. It doesn't accept Loader contexts.\n",
    "\n",
    "    When using the default separator, this processor is equivalent to the\n",
    "    function: ``' '.join``\n",
    "\n",
    "    Examples:\n",
    "\n",
    "    >>> from itemloaders.processors import Join\n",
    "    >>> proc = Join()\n",
    "    >>> proc(['one', 'two', 'three'])\n",
    "    'one two three'\n",
    "    >>> proc = Join('<br>')\n",
    "    >>> proc(['one', 'two', 'three'])\n",
    "    'one<br>two<br>three'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, separator=\" \"):\n",
    "        self.separator = separator\n",
    "\n",
    "    def __call__(self, values):\n",
    "        return self.separator.join(values)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20a108c-7fc2-40c7-879c-329010edd376",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1fba1ab-b2f5-4818-910b-0392fc1cd818",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb881491-fd84-4b29-a5fb-fee985ad96bb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e936ef46-109c-4de9-9506-c0733572634d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d124c33-d47c-47a9-a074-591c7b848819",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7972708a-47b8-452b-9392-ea12104b0c55",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "298337ff-5d60-4495-be9a-b076eabe85b8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "973eb397-17f7-4766-a31e-8b328f1660f4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c618290-1313-4223-8574-a1257864c22c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcd9cde4-b0a9-47ce-8e1a-566e584f4b3c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18cbaacd-c56b-4df4-a8d1-c03f5b5fa05f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b3d7b5c-ceb7-41e6-a129-3162c4c8d68a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a0f813b-33a9-4044-95e2-d780bf967ee0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "677b6177-7e5f-446e-bf72-96ad5ae717ca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cad7ed3c-25ac-476b-bbcd-6668807863ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20dbbaa1-1768-4adb-a85f-4d08269fa1d8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ed00abc-dbf8-42f3-899b-a0d27bb0266f",
   "metadata": {},
   "source": [
    "# <b>(FORGET) Additional</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a179bc2-3bcc-4353-8412-5bf253a7bb25",
   "metadata": {},
   "source": [
    "|bash|description|\n",
    "|-|-|\n",
    "|`scrapy startproject <name>`|start a new scrapy project|\n",
    "|`scrapy genspider <spider_name> <domain>`|generate a spider in the `spider` dir|\n",
    "|`scrapy runspider <spider_file>.py`|start the crawler|\n",
    "|||\n",
    "|||\n",
    "|||"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54b00cd-908f-4697-bd45-4210bd92753f",
   "metadata": {},
   "source": [
    "# 3. Creating a Scrapy project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c0b41c-7421-492e-bd84-2de27858413c",
   "metadata": {},
   "source": [
    "You should work in the virtual environment.\n",
    "\n",
    "```sh\n",
    "pip install --upgrade pip\n",
    "pip install scrapy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bddfd4-0ed5-410c-8203-5edfe65075c5",
   "metadata": {},
   "source": [
    "A **spider** is a Scrapy project that, like its arachnid namesake, is designed to crawl webs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56816e0-5467-4284-bbf3-ec9c5ae29d4b",
   "metadata": {},
   "source": [
    "```sh\n",
    "$ scrapy startproject test1\n",
    "```\n",
    "```\n",
    "New Scrapy project 'test1', using template directory '/home/commi/venv/venv3.12/lib/python3.12/site-packages/scrapy/templates/project', created in:\n",
    "    /home/commi/Yandex.Disk/it_learning/08_parsing_data/data/test1\n",
    "\n",
    "You can start your first spider with:\n",
    "    cd test1\n",
    "    scrapy genspider example example.com\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ce26a-c0cf-48bb-973d-9733f527a6dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Project dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ecea08b8-a347-44ee-b752-d7f9c0e8a704",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:30:54.703573Z",
     "iopub.status.busy": "2024-01-15T13:30:54.703002Z",
     "iopub.status.idle": "2024-01-15T13:30:54.808659Z",
     "shell.execute_reply": "2024-01-15T13:30:54.807658Z",
     "shell.execute_reply.started": "2024-01-15T13:30:54.703536Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cd /home/commi/Yandex.Disk/it_learning/08_parsing_data/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "379af829-3fb8-4e35-a5e7-d22faf074e50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:30:55.263463Z",
     "iopub.status.busy": "2024-01-15T13:30:55.262717Z",
     "iopub.status.idle": "2024-01-15T13:30:55.381147Z",
     "shell.execute_reply": "2024-01-15T13:30:55.380041Z",
     "shell.execute_reply.started": "2024-01-15T13:30:55.263422Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtest1\u001b[0m\n",
      "├── scrapy.cfg\n",
      "└── \u001b[01;34mtest1\u001b[0m\n",
      "    ├── __init__.py\n",
      "    ├── items.py\n",
      "    ├── middlewares.py\n",
      "    ├── pipelines.py\n",
      "    ├── settings.py\n",
      "    └── \u001b[01;34mspiders\u001b[0m\n",
      "        └── __init__.py\n",
      "\n",
      "3 directories, 7 files\n"
     ]
    }
   ],
   "source": [
    "tree test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d98d0f98-06b3-4f4d-b26a-4416c914f619",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:30:57.259154Z",
     "iopub.status.busy": "2024-01-15T13:30:57.258366Z",
     "iopub.status.idle": "2024-01-15T13:30:57.408565Z",
     "shell.execute_reply": "2024-01-15T13:30:57.407181Z",
     "shell.execute_reply.started": "2024-01-15T13:30:57.259088Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Automatically created by: scrapy startproject\n",
      "#\n",
      "# For more information about the [deploy] section see:\n",
      "# https://scrapyd.readthedocs.io/en/latest/deploy.html\n",
      "\n",
      "[settings]\n",
      "default = test1.settings\n",
      "\n",
      "[deploy]\n",
      "#url = http://localhost:6800/\n",
      "project = test1\n"
     ]
    }
   ],
   "source": [
    "cat test1/scrapy.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302e9893-67d6-45dd-9d7e-27fc53d0e030",
   "metadata": {},
   "source": [
    "### Deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "13f95372-ca7a-4e93-89b2-d204b8247c76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:30:59.654728Z",
     "iopub.status.busy": "2024-01-15T13:30:59.653435Z",
     "iopub.status.idle": "2024-01-15T13:30:59.765945Z",
     "shell.execute_reply": "2024-01-15T13:30:59.764669Z",
     "shell.execute_reply.started": "2024-01-15T13:30:59.654676Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtest1/test1\u001b[0m\n",
      "├── __init__.py\n",
      "├── items.py\n",
      "├── middlewares.py\n",
      "├── pipelines.py\n",
      "├── settings.py\n",
      "└── \u001b[01;34mspiders\u001b[0m\n",
      "    └── __init__.py\n",
      "\n",
      "2 directories, 6 files\n"
     ]
    }
   ],
   "source": [
    "tree test1/test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f60b913-9ac9-4002-9848-2583c61b947c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:31:02.723037Z",
     "iopub.status.busy": "2024-01-15T13:31:02.722598Z",
     "iopub.status.idle": "2024-01-15T13:31:02.836435Z",
     "shell.execute_reply": "2024-01-15T13:31:02.835624Z",
     "shell.execute_reply.started": "2024-01-15T13:31:02.723003Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Define here the models for your scraped items\n",
      "#\n",
      "# See documentation in:\n",
      "# https://docs.scrapy.org/en/latest/topics/items.html\n",
      "\n",
      "import scrapy\n",
      "\n",
      "\n",
      "class Test1Item(scrapy.Item):\n",
      "    # define the fields for your item here like:\n",
      "    # name = scrapy.Field()\n",
      "    pass\n"
     ]
    }
   ],
   "source": [
    "cat test1/test1/items.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d8f0246-7466-4db3-9666-f499c65503dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:31:05.807978Z",
     "iopub.status.busy": "2024-01-15T13:31:05.807297Z",
     "iopub.status.idle": "2024-01-15T13:31:06.006539Z",
     "shell.execute_reply": "2024-01-15T13:31:06.005002Z",
     "shell.execute_reply.started": "2024-01-15T13:31:05.807945Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Define here the models for your spider middleware\n",
      "#\n",
      "# See documentation in:\n",
      "# https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
      "\n",
      "from scrapy import signals\n",
      "\n",
      "# useful for handling different item types with a single interface\n",
      "from itemadapter import is_item, ItemAdapter\n",
      "\n",
      "\n",
      "class Test1SpiderMiddleware:\n",
      "    # Not all methods need to be defined. If a method is not defined,\n",
      "    # scrapy acts as if the spider middleware does not modify the\n",
      "    # passed objects.\n",
      "\n",
      "    @classmethod\n",
      "    def from_crawler(cls, crawler):\n",
      "        # This method is used by Scrapy to create your spiders.\n",
      "        s = cls()\n",
      "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
      "        return s\n",
      "\n",
      "    def process_spider_input(self, response, spider):\n",
      "        # Called for each response that goes through the spider\n",
      "        # middleware and into the spider.\n",
      "\n",
      "        # Should return None or raise an exception.\n",
      "        return None\n",
      "\n",
      "    def process_spider_output(self, response, result, spider):\n",
      "        # Called with the results returned from the Spider, after\n",
      "        # it has processed the response.\n",
      "\n",
      "        # Must return an iterable of Request, or item objects.\n",
      "        for i in result:\n",
      "            yield i\n",
      "\n",
      "    def process_spider_exception(self, response, exception, spider):\n",
      "        # Called when a spider or process_spider_input() method\n",
      "        # (from other spider middleware) raises an exception.\n",
      "\n",
      "        # Should return either None or an iterable of Request or item objects.\n",
      "        pass\n",
      "\n",
      "    def process_start_requests(self, start_requests, spider):\n",
      "        # Called with the start requests of the spider, and works\n",
      "        # similarly to the process_spider_output() method, except\n",
      "        # that it doesn’t have a response associated.\n",
      "\n",
      "        # Must return only requests (not items).\n",
      "        for r in start_requests:\n",
      "            yield r\n",
      "\n",
      "    def spider_opened(self, spider):\n",
      "        spider.logger.info(\"Spider opened: %s\" % spider.name)\n",
      "\n",
      "\n",
      "class Test1DownloaderMiddleware:\n",
      "    # Not all methods need to be defined. If a method is not defined,\n",
      "    # scrapy acts as if the downloader middleware does not modify the\n",
      "    # passed objects.\n",
      "\n",
      "    @classmethod\n",
      "    def from_crawler(cls, crawler):\n",
      "        # This method is used by Scrapy to create your spiders.\n",
      "        s = cls()\n",
      "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
      "        return s\n",
      "\n",
      "    def process_request(self, request, spider):\n",
      "        # Called for each request that goes through the downloader\n",
      "        # middleware.\n",
      "\n",
      "        # Must either:\n",
      "        # - return None: continue processing this request\n",
      "        # - or return a Response object\n",
      "        # - or return a Request object\n",
      "        # - or raise IgnoreRequest: process_exception() methods of\n",
      "        #   installed downloader middleware will be called\n",
      "        return None\n",
      "\n",
      "    def process_response(self, request, response, spider):\n",
      "        # Called with the response returned from the downloader.\n",
      "\n",
      "        # Must either;\n",
      "        # - return a Response object\n",
      "        # - return a Request object\n",
      "        # - or raise IgnoreRequest\n",
      "        return response\n",
      "\n",
      "    def process_exception(self, request, exception, spider):\n",
      "        # Called when a download handler or a process_request()\n",
      "        # (from other downloader middleware) raises an exception.\n",
      "\n",
      "        # Must either:\n",
      "        # - return None: continue processing this exception\n",
      "        # - return a Response object: stops process_exception() chain\n",
      "        # - return a Request object: stops process_exception() chain\n",
      "        pass\n",
      "\n",
      "    def spider_opened(self, spider):\n",
      "        spider.logger.info(\"Spider opened: %s\" % spider.name)\n"
     ]
    }
   ],
   "source": [
    "cat test1/test1/middlewares.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70eff45e-8748-48a5-a3d3-0f29e86cddc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:31:08.926648Z",
     "iopub.status.busy": "2024-01-15T13:31:08.925824Z",
     "iopub.status.idle": "2024-01-15T13:31:09.038088Z",
     "shell.execute_reply": "2024-01-15T13:31:09.036692Z",
     "shell.execute_reply.started": "2024-01-15T13:31:08.926579Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Define your item pipelines here\n",
      "#\n",
      "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
      "# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
      "\n",
      "\n",
      "# useful for handling different item types with a single interface\n",
      "from itemadapter import ItemAdapter\n",
      "\n",
      "\n",
      "class Test1Pipeline:\n",
      "    def process_item(self, item, spider):\n",
      "        return item\n"
     ]
    }
   ],
   "source": [
    "cat test1/test1/pipelines.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f5d4940-928f-431c-a4d0-db14cf71b459",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:31:09.533481Z",
     "iopub.status.busy": "2024-01-15T13:31:09.532629Z",
     "iopub.status.idle": "2024-01-15T13:31:09.721049Z",
     "shell.execute_reply": "2024-01-15T13:31:09.719769Z",
     "shell.execute_reply.started": "2024-01-15T13:31:09.533446Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Scrapy settings for test1 project\n",
      "#\n",
      "# For simplicity, this file contains only settings considered important or\n",
      "# commonly used. You can find more settings consulting the documentation:\n",
      "#\n",
      "#     https://docs.scrapy.org/en/latest/topics/settings.html\n",
      "#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
      "#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
      "\n",
      "BOT_NAME = \"test1\"\n",
      "\n",
      "SPIDER_MODULES = [\"test1.spiders\"]\n",
      "NEWSPIDER_MODULE = \"test1.spiders\"\n",
      "\n",
      "\n",
      "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
      "#USER_AGENT = \"test1 (+http://www.yourdomain.com)\"\n",
      "\n",
      "# Obey robots.txt rules\n",
      "ROBOTSTXT_OBEY = True\n",
      "\n",
      "# Configure maximum concurrent requests performed by Scrapy (default: 16)\n",
      "#CONCURRENT_REQUESTS = 32\n",
      "\n",
      "# Configure a delay for requests for the same website (default: 0)\n",
      "# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n",
      "# See also autothrottle settings and docs\n",
      "#DOWNLOAD_DELAY = 3\n",
      "# The download delay setting will honor only one of:\n",
      "#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n",
      "#CONCURRENT_REQUESTS_PER_IP = 16\n",
      "\n",
      "# Disable cookies (enabled by default)\n",
      "#COOKIES_ENABLED = False\n",
      "\n",
      "# Disable Telnet Console (enabled by default)\n",
      "#TELNETCONSOLE_ENABLED = False\n",
      "\n",
      "# Override the default request headers:\n",
      "#DEFAULT_REQUEST_HEADERS = {\n",
      "#    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
      "#    \"Accept-Language\": \"en\",\n",
      "#}\n",
      "\n",
      "# Enable or disable spider middlewares\n",
      "# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
      "#SPIDER_MIDDLEWARES = {\n",
      "#    \"test1.middlewares.Test1SpiderMiddleware\": 543,\n",
      "#}\n",
      "\n",
      "# Enable or disable downloader middlewares\n",
      "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
      "#DOWNLOADER_MIDDLEWARES = {\n",
      "#    \"test1.middlewares.Test1DownloaderMiddleware\": 543,\n",
      "#}\n",
      "\n",
      "# Enable or disable extensions\n",
      "# See https://docs.scrapy.org/en/latest/topics/extensions.html\n",
      "#EXTENSIONS = {\n",
      "#    \"scrapy.extensions.telnet.TelnetConsole\": None,\n",
      "#}\n",
      "\n",
      "# Configure item pipelines\n",
      "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
      "#ITEM_PIPELINES = {\n",
      "#    \"test1.pipelines.Test1Pipeline\": 300,\n",
      "#}\n",
      "\n",
      "# Enable and configure the AutoThrottle extension (disabled by default)\n",
      "# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n",
      "#AUTOTHROTTLE_ENABLED = True\n",
      "# The initial download delay\n",
      "#AUTOTHROTTLE_START_DELAY = 5\n",
      "# The maximum download delay to be set in case of high latencies\n",
      "#AUTOTHROTTLE_MAX_DELAY = 60\n",
      "# The average number of requests Scrapy should be sending in parallel to\n",
      "# each remote server\n",
      "#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n",
      "# Enable showing throttling stats for every response received:\n",
      "#AUTOTHROTTLE_DEBUG = False\n",
      "\n",
      "# Enable and configure HTTP caching (disabled by default)\n",
      "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n",
      "#HTTPCACHE_ENABLED = True\n",
      "#HTTPCACHE_EXPIRATION_SECS = 0\n",
      "#HTTPCACHE_DIR = \"httpcache\"\n",
      "#HTTPCACHE_IGNORE_HTTP_CODES = []\n",
      "#HTTPCACHE_STORAGE = \"scrapy.extensions.httpcache.FilesystemCacheStorage\"\n",
      "\n",
      "# Set settings whose default value is deprecated to a future-proof value\n",
      "REQUEST_FINGERPRINTER_IMPLEMENTATION = \"2.7\"\n",
      "TWISTED_REACTOR = \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n",
      "FEED_EXPORT_ENCODING = \"utf-8\"\n"
     ]
    }
   ],
   "source": [
    "cat test1/test1/settings.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9bec3b-c7d0-4785-865c-9324c62a282b",
   "metadata": {},
   "source": [
    "### Even deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "376e12d3-10e9-4912-9fb0-17dc2587138e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:31:15.439231Z",
     "iopub.status.busy": "2024-01-15T13:31:15.438518Z",
     "iopub.status.idle": "2024-01-15T13:31:15.547558Z",
     "shell.execute_reply": "2024-01-15T13:31:15.546528Z",
     "shell.execute_reply.started": "2024-01-15T13:31:15.439172Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtest1/test1/spiders\u001b[0m\n",
      "└── __init__.py\n",
      "\n",
      "1 directory, 1 file\n"
     ]
    }
   ],
   "source": [
    "tree test1/test1/spiders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba4805d6-5240-48ab-a6a2-782d1497fe24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:31:19.282686Z",
     "iopub.status.busy": "2024-01-15T13:31:19.281995Z",
     "iopub.status.idle": "2024-01-15T13:31:19.394365Z",
     "shell.execute_reply": "2024-01-15T13:31:19.393434Z",
     "shell.execute_reply.started": "2024-01-15T13:31:19.282629Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# This package will contain the spiders of your Scrapy project\n",
      "#\n",
      "# Please refer to the documentation for information on how to create and manage\n",
      "# your spiders.\n"
     ]
    }
   ],
   "source": [
    "cat test1/test1/spiders/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da861669-3563-45aa-899a-161f78cc54c3",
   "metadata": {},
   "source": [
    "# 4. Write a Simple Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa054743-1223-4d6e-904a-16739c75b092",
   "metadata": {},
   "source": [
    "To create a crawler, you will add a new file inside the spiders directory at test1/test1/spiders/bookspider.py."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8b64b2-cd87-4ee0-8c09-eed573ba84a9",
   "metadata": {},
   "source": [
    "```sh\n",
    "$ cd test1/test1/spiders/\n",
    "$ scrapy genspider bookspider books.toscrape.com\n",
    "```\n",
    "```\n",
    "Created spider 'bookspider' using template 'basic' in module:\n",
    "  test1.spiders.bookspider\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "abd55e5b-8d6f-4c17-af1c-75a2154b264f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:31:48.403396Z",
     "iopub.status.busy": "2024-01-15T13:31:48.402628Z",
     "iopub.status.idle": "2024-01-15T13:31:48.514078Z",
     "shell.execute_reply": "2024-01-15T13:31:48.512387Z",
     "shell.execute_reply.started": "2024-01-15T13:31:48.403331Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtest1/test1/spiders/\u001b[0m\n",
      "├── bookspider.py\n",
      "├── __init__.py\n",
      "└── \u001b[01;34m__pycache__\u001b[0m\n",
      "    └── __init__.cpython-312.pyc\n",
      "\n",
      "2 directories, 3 files\n"
     ]
    }
   ],
   "source": [
    "tree test1/test1/spiders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "30875633-3fa4-4bd9-bee8-60dba3b0e327",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:32:14.207312Z",
     "iopub.status.busy": "2024-01-15T13:32:14.206579Z",
     "iopub.status.idle": "2024-01-15T13:32:14.318134Z",
     "shell.execute_reply": "2024-01-15T13:32:14.315643Z",
     "shell.execute_reply.started": "2024-01-15T13:32:14.207255Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import scrapy\n",
      "\n",
      "\n",
      "class BookspiderSpider(scrapy.Spider):\n",
      "    name = \"bookspider\"\n",
      "    allowed_domains = [\"books.toscrape.com\"]\n",
      "    start_urls = [\"https://books.toscrape.com\"]\n",
      "\n",
      "    def parse(self, response):\n",
      "        pass\n"
     ]
    }
   ],
   "source": [
    "cat test1/test1/spiders/bookspider.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
