{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "968ee719-bf68-4e29-9408-ec844121f4c9",
   "metadata": {},
   "source": [
    "# SCRAPY [DOCUMENTATION](https://docs.scrapy.org/en/latest/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01be5cd1-1600-4414-a5de-aab700913727",
   "metadata": {},
   "source": [
    "Scrapy is a fast high-level web crawling and web scraping framework, used to crawl websites and extract structured data from their pages. It can be used for a wide range of purposes, from data mining to monitoring and automated testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920f46d7-496c-4360-b7a5-451108e6d521",
   "metadata": {},
   "source": [
    "# Basic commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dcc10f-fcb3-4ac9-8f77-872712283f6c",
   "metadata": {},
   "source": [
    "## Scrapy command line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b22b9f-ec2a-4403-b3d9-806eeaa5c25c",
   "metadata": {},
   "source": [
    "|Scrapy command line|Description|\n",
    "|-|-|\n",
    "|**Help**||\n",
    "|`scrapy -h`|(`--help`) list all the available commands (run from the project's directory)|\n",
    "|`scrapy <command> -h`|help on the given command|\n",
    "|**List info**||\n",
    "|`scrapy list`|list all available crawlers (run from the project's deirectory)|\n",
    "|**Crawl a random webpage in shell**||\n",
    "|`scrapy shell <url>`||\n",
    "|(inside shell) `view(response)`|open the resonse object in your browser|\n",
    "|**Project**||\n",
    "|`scrapy startproject project_name [project_dir]`|create a new project|\n",
    "|`scrapy genspider [-t template] <name> <domain or URL>`|Create a new spider in the current folder or in the current project’s spiders folder,|\n",
    "|`scrapy crawl spider_name`|run the spider (from the project's top level dir)|\n",
    "|||\n",
    "|||\n",
    "|||\n",
    "|||\n",
    "\n",
    "```\n",
    "scrapy --help\n",
    "Scrapy 2.11.0 - active project: tutorial\n",
    "\n",
    "Usage:\n",
    "  scrapy <command> [options] [args]\n",
    "\n",
    "Available commands:\n",
    "  bench         Run quick benchmark test\n",
    "  check         Check spider contracts\n",
    "  crawl         Run a spider\n",
    "  edit          Edit spider\n",
    "  fetch         Fetch a URL using the Scrapy downloader\n",
    "  genspider     Generate new spider using pre-defined templates\n",
    "  list          List available spiders\n",
    "  parse         Parse URL (using its spider) and print the results\n",
    "  runspider     Run a self-contained spider (without creating a project)\n",
    "  settings      Get settings values\n",
    "  shell         Interactive scraping console\n",
    "  startproject  Create new project\n",
    "  version       Print Scrapy version\n",
    "  view          Open URL in browser, as seen by Scrapy\n",
    "\n",
    "Use \"scrapy <command> -h\" to see more info about a command\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d33a9d-eda2-4f44-b287-6512d7cef5ff",
   "metadata": {},
   "source": [
    "## Scrapy extraction most common tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558e8530-49b4-4ec5-b347-b719697157af",
   "metadata": {},
   "source": [
    "|Scrapy extraction tools|Description|\n",
    "|-|-|\n",
    "|`view(response)`|open the response page from the shell in your web browser|\n",
    "|**Response status codes**||\n",
    "|`response.status`||\n",
    "|**CSS selectors**||\n",
    "|`response.css`||\n",
    "|`response.css(\"title::text\").getall()`|get only text from the SelectorList|\n",
    "|||\n",
    "|||\n",
    "|||"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00a0466-e563-49de-a9d1-b380ced0c1f9",
   "metadata": {},
   "source": [
    "# <b>1. Scrapy tutorial</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1eb925-6bb2-4b85-8261-56c0b924f4b6",
   "metadata": {},
   "source": [
    "This tutorial will walk you through these tasks:\n",
    "\n",
    "- Creating a new Scrapy project\n",
    "- Writing a spider to crawl a site and extract data\n",
    "- Exporting the scraped data using the command line\n",
    "- Changing spider to recursively follow links\n",
    "- Using spider arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ca1bcf-ddc7-4728-8b21-6f94070a72b1",
   "metadata": {},
   "source": [
    "# 1.0 Installation guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe05960c-b29b-4731-aae8-35713f2b0606",
   "metadata": {},
   "source": [
    "We strongly recommend that you install Scrapy in a dedicated virtualenv, to avoid conflicting with your system packages.\n",
    "\n",
    "```sh\n",
    "(venv) $ pip install Scrapy\n",
    "```\n",
    "\n",
    "Scrapy is written in pure Python and depends on a few key Python packages (among others):\n",
    "\n",
    "- lxml, an efficient XML and HTML parser\n",
    "- parsel, an HTML/XML data extraction library written on top of lxml,\n",
    "- w3lib, a multi-purpose helper for dealing with URLs and web page encodings\n",
    "- twisted, an asynchronous networking framework\n",
    "- cryptography and pyOpenSSL, to deal with various network-level security needs\n",
    "\n",
    "Some of these packages themselves depend on non-Python packages that might require additional installation steps depending on your platform. Please check [platform-specific guides](https://docs.scrapy.org/en/latest/intro/install.html#intro-install-platform-notes).\n",
    "\n",
    "In case of any trouble related to these dependencies, please refer to their respective installation instructions:\n",
    "\n",
    "- [lxml installation](https://lxml.de/installation.html)\n",
    "- [cryptography installation](https://cryptography.io/en/latest/installation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122d7112-8b38-4950-a190-6272f74f6903",
   "metadata": {},
   "source": [
    "# 1.1 Creating a project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7312e67a-4e29-47dd-9799-01db5661b765",
   "metadata": {},
   "source": [
    "Before you start scraping, you will have to set up a new Scrapy project. Enter a directory where you’d like to store your code and run:\n",
    "\n",
    "```sh\n",
    "scrapy startproject tutorial\n",
    "```\n",
    "```\n",
    "New Scrapy project 'tutorial', using template directory '/home/commi/venv/venv3.11/lib/python3.11/site-packages/scrapy/templates/project', created in:\n",
    "    /home/commi/Yandex.Disk/it_learning/08_web_scraping/02_scrapy/data/tutorial\n",
    "\n",
    "You can start your first spider with:\n",
    "    cd tutorial\n",
    "    scrapy genspider example example.com\n",
    "```\n",
    "\n",
    "This will create a `tutorial` directory with the following contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8e616c4-6e21-4f23-8aef-6a47dd54ed32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-04T20:41:14.321005Z",
     "iopub.status.busy": "2024-02-04T20:41:14.320026Z",
     "iopub.status.idle": "2024-02-04T20:41:14.486706Z",
     "shell.execute_reply": "2024-02-04T20:41:14.484683Z",
     "shell.execute_reply.started": "2024-02-04T20:41:14.320929Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtutorial\u001b[0m\n",
      "├── scrapy.cfg\n",
      "└── \u001b[01;34mtutorial\u001b[0m\n",
      "    ├── __init__.py\n",
      "    ├── items.py\n",
      "    ├── middlewares.py\n",
      "    ├── pipelines.py\n",
      "    ├── settings.py\n",
      "    └── \u001b[01;34mspiders\u001b[0m\n",
      "        └── __init__.py\n",
      "\n",
      "3 directories, 7 files\n"
     ]
    }
   ],
   "source": [
    "cd ./data\n",
    "tree tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f47d0d-42a0-45d1-81bc-19405b2282cd",
   "metadata": {},
   "source": [
    "```\n",
    "tutorial/\n",
    "    scrapy.cfg            # deploy configuration file\n",
    "\n",
    "    tutorial/             # project's Python module, you'll import your code from here\n",
    "        __init__.py\n",
    "\n",
    "        items.py          # project items definition file\n",
    "\n",
    "        middlewares.py    # project middlewares file\n",
    "\n",
    "        pipelines.py      # project pipelines file\n",
    "\n",
    "        settings.py       # project settings file\n",
    "\n",
    "        spiders/          # a directory where you'll later put your spiders\n",
    "            __init__.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4fa774-eea7-4e39-992d-743fb81519f4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1.2 Our first Spider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1932bf53-e3c9-4209-9533-3590627dd4b7",
   "metadata": {},
   "source": [
    "**Spiders** are classes that you define and that Scrapy uses to scrape information from a website (or a group of websites). They must subclass `Spider` and define the initial requests to make, optionally \n",
    "- how to follow links in the pages, and \n",
    "- how to parse the downloaded page content to extract data.\n",
    "\n",
    "This is the code for our first `Spider`. Save it in a file named `quotes_spider.py` under the `tutorial/spiders` directory in your project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d31f0595-b402-48ee-8f1b-fbffacb2d778",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-04T20:43:53.627476Z",
     "iopub.status.busy": "2024-02-04T20:43:53.625433Z",
     "iopub.status.idle": "2024-02-04T20:43:53.742875Z",
     "shell.execute_reply": "2024-02-04T20:43:53.741806Z",
     "shell.execute_reply.started": "2024-02-04T20:43:53.627403Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".:\n",
      "draft.py  \u001b[0m\u001b[01;34m__pycache__\u001b[0m  quotes.jsonl  quotes_spider.py  \u001b[01;34mtutorial\u001b[0m\n",
      "\n",
      "./__pycache__:\n",
      "draft.cpython-311.pyc\n",
      "\n",
      "./tutorial:\n",
      "scrapy.cfg  \u001b[01;34mtutorial\u001b[0m\n",
      "\n",
      "./tutorial/tutorial:\n",
      "__init__.py  items.py  middlewares.py  pipelines.py  settings.py  \u001b[01;34mspiders\u001b[0m\n",
      "\n",
      "./tutorial/tutorial/spiders:\n",
      "__init__.py\n"
     ]
    }
   ],
   "source": [
    "ls -R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fd6194-8510-423d-b568-2aedbd634900",
   "metadata": {},
   "source": [
    "```python\n",
    "from pathlib import Path\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            \"https://quotes.toscrape.com/page/1/\",\n",
    "            \"https://quotes.toscrape.com/page/2/\",\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = f\"quotes-{page}.html\"\n",
    "        Path(filename).write_bytes(response.body)\n",
    "        self.log(f\"Saved file {filename}\")\n",
    "```\n",
    "\n",
    "As you can see, our Spider subclasses `scrapy.Spider` and defines some attributes and methods:\n",
    "\n",
    "- `name`: identifies the Spider. It **must be unique within a project**, that is, you can’t set the same name for different Spiders.\n",
    "\n",
    "- `start_requests()`: must return an iterable of `Request`s (you can return a **list** of requests or write a **generator** function) which the Spider will begin to crawl from. Subsequent requests will be generated successively from these initial requests.\n",
    "\n",
    "- `parse()`: a method that will be called to handle the response downloaded for each of the requests made. The `response` parameter is an instance of `TextResponse` that holds the page content and has further helpful methods to handle it.\n",
    "\n",
    "The `parse()` method usually parses the `response`, extracting the scraped data as dicts and also finding new URLs to follow and creating new requests (`Request`) from them.\n",
    "\n",
    "_ChatGPT:_  \n",
    "In Scrapy, when you use the `yield` statement within a Spider callback method like `parse`, the yielded items are not stored directly. Instead, they are processed by the Scrapy framework, typically passed to [Item Pipeline](#2.7-Item-Pipeline) components.\n",
    "\n",
    "Here's what happens when you yield items in Scrapy:\n",
    "- When you `yield` an item from a Spider callback method like `parse`, Scrapy will send that item to the Item Pipeline.\n",
    "- The Item Pipeline is a mechanism for processing the items scraped by the Spider. It allows you to perform various tasks on the scraped items, such as cleaning, validation, and persistence.\n",
    "- Each item that is `yield`ed is processed through the Item Pipeline sequentially, allowing you to define various stages of processing.\n",
    "- The Item Pipeline can perform operations like validation and transformation on the items before they are saved to a storage backend such as a database, JSON file, or CSV file.\n",
    "- You can define your own custom Item Pipeline to process the scraped items according to your requirements.\n",
    "\n",
    "So, in summary, the yielded data from a Scrapy Spider is not stored directly within the Spider itself; instead, it's passed through the Item Pipeline for further processing and eventual storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93f33ea-00e0-464f-a5c2-35378c70e7af",
   "metadata": {},
   "source": [
    "## How to run our spider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b388f81-c947-4fea-95f8-4a94758c5fd1",
   "metadata": {},
   "source": [
    "To put our spider to work, go to the project’s top level directory and run:\n",
    "\n",
    "```sh\n",
    "scrapy crawl quotes\n",
    "```\n",
    "This command runs the spider with name `quotes` that we’ve just added, that will send some requests for the `quotes.toscrape.com` domain. You will get an output similar to this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e7e96b-75f6-4259-9664-5ddc3d3c8bc6",
   "metadata": {},
   "source": [
    "```\n",
    "2024-02-05 01:57:09 [scrapy.utils.log] INFO: Scrapy 2.11.0 started (bot: tutorial)\n",
    "2024-02-05 01:57:09 [scrapy.utils.log] INFO: Versions: lxml 5.1.0.0, libxml2 2.12.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.11.2 (main, Mar 13 2023, 12:18:29) [GCC 12.2.0], pyOpenSSL 24.0.0 (OpenSSL 3.2.1 30 Jan 2024), cryptography 42.0.2, Platform Linux-6.1.0-17-amd64-x86_64-with-glibc2.36\n",
    "2024-02-05 01:57:09 [scrapy.addons] INFO: Enabled addons:\n",
    "[]\n",
    "2024-02-05 01:57:09 [asyncio] DEBUG: Using selector: EpollSelector\n",
    "2024-02-05 01:57:09 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
    "2024-02-05 01:57:09 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
    "2024-02-05 01:57:09 [scrapy.extensions.telnet] INFO: Telnet Password: b9a7a03404bf04d7\n",
    "2024-02-05 01:57:09 [scrapy.middleware] INFO: Enabled extensions:\n",
    "['scrapy.extensions.corestats.CoreStats',\n",
    " 'scrapy.extensions.telnet.TelnetConsole',\n",
    " 'scrapy.extensions.memusage.MemoryUsage',\n",
    " 'scrapy.extensions.logstats.LogStats']\n",
    "2024-02-05 01:57:09 [scrapy.crawler] INFO: Overridden settings:\n",
    "{'BOT_NAME': 'tutorial',\n",
    " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
    " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
    " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
    " 'ROBOTSTXT_OBEY': True,\n",
    " 'SPIDER_MODULES': ['tutorial.spiders'],\n",
    " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
    "2024-02-05 01:57:09 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
    "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
    " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
    " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
    " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
    " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
    " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
    " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
    " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
    " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
    " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
    " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
    " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
    "2024-02-05 01:57:09 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
    "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
    " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
    " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
    " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
    " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
    "2024-02-05 01:57:09 [scrapy.middleware] INFO: Enabled item pipelines:\n",
    "[]\n",
    "2024-02-05 01:57:09 [scrapy.core.engine] INFO: Spider opened\n",
    "2024-02-05 01:57:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
    "2024-02-05 01:57:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
    "2024-02-05 01:57:09 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://quotes.toscrape.com/robots.txt> (referer: None)\n",
    "2024-02-05 01:57:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/1/> (referer: None)\n",
    "2024-02-05 01:57:10 [quotes] DEBUG: Saved file quotes-1.html\n",
    "2024-02-05 01:57:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/2/> (referer: None)\n",
    "2024-02-05 01:57:10 [quotes] DEBUG: Saved file quotes-2.html\n",
    "2024-02-05 01:57:10 [scrapy.core.engine] INFO: Closing spider (finished)\n",
    "2024-02-05 01:57:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
    "{'downloader/request_bytes': 684,\n",
    " 'downloader/request_count': 3,\n",
    " 'downloader/request_method_count/GET': 3,\n",
    " 'downloader/response_bytes': 25556,\n",
    " 'downloader/response_count': 3,\n",
    " 'downloader/response_status_count/200': 2,\n",
    " 'downloader/response_status_count/404': 1,\n",
    " 'elapsed_time_seconds': 1.235283,\n",
    " 'finish_reason': 'finished',\n",
    " 'finish_time': datetime.datetime(2024, 2, 4, 20, 57, 10, 482312, tzinfo=datetime.timezone.utc),\n",
    " 'log_count/DEBUG': 8,\n",
    " 'log_count/INFO': 10,\n",
    " 'memusage/max': 65585152,\n",
    " 'memusage/startup': 65585152,\n",
    " 'response_received_count': 3,\n",
    " 'robotstxt/request_count': 1,\n",
    " 'robotstxt/response_count': 1,\n",
    " 'robotstxt/response_status_count/404': 1,\n",
    " 'scheduler/dequeued': 2,\n",
    " 'scheduler/dequeued/memory': 2,\n",
    " 'scheduler/enqueued': 2,\n",
    " 'scheduler/enqueued/memory': 2,\n",
    " 'start_time': datetime.datetime(2024, 2, 4, 20, 57, 9, 247029, tzinfo=datetime.timezone.utc)}\n",
    "2024-02-05 01:57:10 [scrapy.core.engine] INFO: Spider closed (finished)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9498b6dc-d419-43a6-975f-c2fe9809be7a",
   "metadata": {},
   "source": [
    "Now, check the files in the current directory. You should notice that two new files have been created: \n",
    "- quotes-1.html and \n",
    "- quotes-2.html, \n",
    "\n",
    "with the content for the respective URLs, as our parse method instructs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "898572ae-7c3c-4152-9ec0-c516d00087ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-04T22:18:03.312774Z",
     "iopub.status.busy": "2024-02-04T22:18:03.312022Z",
     "iopub.status.idle": "2024-02-04T22:18:03.422712Z",
     "shell.execute_reply": "2024-02-04T22:18:03.421645Z",
     "shell.execute_reply.started": "2024-02-04T22:18:03.312747Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quotes-1.html  quotes-2.html  scrapy.cfg  \u001b[0m\u001b[01;34mtutorial\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ls ./tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a184ab66-4896-47d7-9c7e-1643e6ca04b5",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-02-04T22:18:41.703321Z",
     "iopub.status.busy": "2024-02-04T22:18:41.702513Z",
     "iopub.status.idle": "2024-02-04T22:18:41.914813Z",
     "shell.execute_reply": "2024-02-04T22:18:41.913439Z",
     "shell.execute_reply.started": "2024-02-04T22:18:41.703251Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "<head>\n",
      "\t<meta charset=\"UTF-8\">\n",
      "\t<title>Quotes to Scrape</title>\n",
      "    <link rel=\"stylesheet\" href=\"/static/bootstrap.min.css\">\n",
      "    <link rel=\"stylesheet\" href=\"/static/main.css\">\n",
      "</head>\n",
      "<body>\n",
      "    <div class=\"container\">\n",
      "        <div class=\"row header-box\">\n",
      "            <div class=\"col-md-8\">\n",
      "                <h1>\n",
      "                    <a href=\"/\" style=\"text-decoration: none\">Quotes to Scrape</a>\n",
      "                </h1>\n",
      "            </div>\n",
      "            <div class=\"col-md-4\">\n",
      "                <p>\n",
      "                \n",
      "                    <a href=\"/login\">Login</a>\n",
      "                \n",
      "                </p>\n",
      "            </div>\n",
      "        </div>\n",
      "    \n",
      "\n",
      "<div class=\"row\">\n",
      "    <div class=\"col-md-8\">\n",
      "\n",
      "    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n",
      "        <span class=\"text\" itemprop=\"text\">“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”</span>\n",
      "        <span>by <small class=\"author\" itemprop=\"author\">Albert Einstein</small>\n",
      "        <a href=\"/author/Albert-Einstein\">(about)</a>\n",
      "        </span>\n",
      "        <div class=\"tags\">\n",
      "            Tags:\n",
      "            <meta class=\"keywords\" itemprop=\"keywords\" content=\"change,deep-thoughts,thinking,world\" /    > \n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/change/page/1/\">change</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/deep-thoughts/page/1/\">deep-thoughts</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/thinking/page/1/\">thinking</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/world/page/1/\">world</a>\n",
      "            \n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n",
      "        <span class=\"text\" itemprop=\"text\">“It is our choices, Harry, that show what we truly are, far more than our abilities.”</span>\n",
      "        <span>by <small class=\"author\" itemprop=\"author\">J.K. Rowling</small>\n",
      "        <a href=\"/author/J-K-Rowling\">(about)</a>\n",
      "        </span>\n",
      "        <div class=\"tags\">\n",
      "            Tags:\n",
      "            <meta class=\"keywords\" itemprop=\"keywords\" content=\"abilities,choices\" /    > \n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/abilities/page/1/\">abilities</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/choices/page/1/\">choices</a>\n",
      "            \n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n",
      "        <span class=\"text\" itemprop=\"text\">“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”</span>\n",
      "        <span>by <small class=\"author\" itemprop=\"author\">Albert Einstein</small>\n",
      "        <a href=\"/author/Albert-Einstein\">(about)</a>\n",
      "        </span>\n",
      "        <div class=\"tags\">\n",
      "            Tags:\n",
      "            <meta class=\"keywords\" itemprop=\"keywords\" content=\"inspirational,life,live,miracle,miracles\" /    > \n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/inspirational/page/1/\">inspirational</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/life/page/1/\">life</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/live/page/1/\">live</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/miracle/page/1/\">miracle</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/miracles/page/1/\">miracles</a>\n",
      "            \n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n",
      "        <span class=\"text\" itemprop=\"text\">“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”</span>\n",
      "        <span>by <small class=\"author\" itemprop=\"author\">Jane Austen</small>\n",
      "        <a href=\"/author/Jane-Austen\">(about)</a>\n",
      "        </span>\n",
      "        <div class=\"tags\">\n",
      "            Tags:\n",
      "            <meta class=\"keywords\" itemprop=\"keywords\" content=\"aliteracy,books,classic,humor\" /    > \n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/aliteracy/page/1/\">aliteracy</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/books/page/1/\">books</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/classic/page/1/\">classic</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/humor/page/1/\">humor</a>\n",
      "            \n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n",
      "        <span class=\"text\" itemprop=\"text\">“Imperfection is beauty, madness is genius and it&#39;s better to be absolutely ridiculous than absolutely boring.”</span>\n",
      "        <span>by <small class=\"author\" itemprop=\"author\">Marilyn Monroe</small>\n",
      "        <a href=\"/author/Marilyn-Monroe\">(about)</a>\n",
      "        </span>\n",
      "        <div class=\"tags\">\n",
      "            Tags:\n",
      "            <meta class=\"keywords\" itemprop=\"keywords\" content=\"be-yourself,inspirational\" /    > \n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/be-yourself/page/1/\">be-yourself</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/inspirational/page/1/\">inspirational</a>\n",
      "            \n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n",
      "        <span class=\"text\" itemprop=\"text\">“Try not to become a man of success. Rather become a man of value.”</span>\n",
      "        <span>by <small class=\"author\" itemprop=\"author\">Albert Einstein</small>\n",
      "        <a href=\"/author/Albert-Einstein\">(about)</a>\n",
      "        </span>\n",
      "        <div class=\"tags\">\n",
      "            Tags:\n",
      "            <meta class=\"keywords\" itemprop=\"keywords\" content=\"adulthood,success,value\" /    > \n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/adulthood/page/1/\">adulthood</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/success/page/1/\">success</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/value/page/1/\">value</a>\n",
      "            \n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n",
      "        <span class=\"text\" itemprop=\"text\">“It is better to be hated for what you are than to be loved for what you are not.”</span>\n",
      "        <span>by <small class=\"author\" itemprop=\"author\">André Gide</small>\n",
      "        <a href=\"/author/Andre-Gide\">(about)</a>\n",
      "        </span>\n",
      "        <div class=\"tags\">\n",
      "            Tags:\n",
      "            <meta class=\"keywords\" itemprop=\"keywords\" content=\"life,love\" /    > \n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/life/page/1/\">life</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/love/page/1/\">love</a>\n",
      "            \n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n",
      "        <span class=\"text\" itemprop=\"text\">“I have not failed. I&#39;ve just found 10,000 ways that won&#39;t work.”</span>\n",
      "        <span>by <small class=\"author\" itemprop=\"author\">Thomas A. Edison</small>\n",
      "        <a href=\"/author/Thomas-A-Edison\">(about)</a>\n",
      "        </span>\n",
      "        <div class=\"tags\">\n",
      "            Tags:\n",
      "            <meta class=\"keywords\" itemprop=\"keywords\" content=\"edison,failure,inspirational,paraphrased\" /    > \n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/edison/page/1/\">edison</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/failure/page/1/\">failure</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/inspirational/page/1/\">inspirational</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/paraphrased/page/1/\">paraphrased</a>\n",
      "            \n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n",
      "        <span class=\"text\" itemprop=\"text\">“A woman is like a tea bag; you never know how strong it is until it&#39;s in hot water.”</span>\n",
      "        <span>by <small class=\"author\" itemprop=\"author\">Eleanor Roosevelt</small>\n",
      "        <a href=\"/author/Eleanor-Roosevelt\">(about)</a>\n",
      "        </span>\n",
      "        <div class=\"tags\">\n",
      "            Tags:\n",
      "            <meta class=\"keywords\" itemprop=\"keywords\" content=\"misattributed-eleanor-roosevelt\" /    > \n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/misattributed-eleanor-roosevelt/page/1/\">misattributed-eleanor-roosevelt</a>\n",
      "            \n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <div class=\"quote\" itemscope itemtype=\"http://schema.org/CreativeWork\">\n",
      "        <span class=\"text\" itemprop=\"text\">“A day without sunshine is like, you know, night.”</span>\n",
      "        <span>by <small class=\"author\" itemprop=\"author\">Steve Martin</small>\n",
      "        <a href=\"/author/Steve-Martin\">(about)</a>\n",
      "        </span>\n",
      "        <div class=\"tags\">\n",
      "            Tags:\n",
      "            <meta class=\"keywords\" itemprop=\"keywords\" content=\"humor,obvious,simile\" /    > \n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/humor/page/1/\">humor</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/obvious/page/1/\">obvious</a>\n",
      "            \n",
      "            <a class=\"tag\" href=\"/tag/simile/page/1/\">simile</a>\n",
      "            \n",
      "        </div>\n",
      "    </div>\n",
      "\n",
      "    <nav>\n",
      "        <ul class=\"pager\">\n",
      "            \n",
      "            \n",
      "            <li class=\"next\">\n",
      "                <a href=\"/page/2/\">Next <span aria-hidden=\"true\">&rarr;</span></a>\n",
      "            </li>\n",
      "            \n",
      "        </ul>\n",
      "    </nav>\n",
      "    </div>\n",
      "    <div class=\"col-md-4 tags-box\">\n",
      "        \n",
      "            <h2>Top Ten tags</h2>\n",
      "            \n",
      "            <span class=\"tag-item\">\n",
      "            <a class=\"tag\" style=\"font-size: 28px\" href=\"/tag/love/\">love</a>\n",
      "            </span>\n",
      "            \n",
      "            <span class=\"tag-item\">\n",
      "            <a class=\"tag\" style=\"font-size: 26px\" href=\"/tag/inspirational/\">inspirational</a>\n",
      "            </span>\n",
      "            \n",
      "            <span class=\"tag-item\">\n",
      "            <a class=\"tag\" style=\"font-size: 26px\" href=\"/tag/life/\">life</a>\n",
      "            </span>\n",
      "            \n",
      "            <span class=\"tag-item\">\n",
      "            <a class=\"tag\" style=\"font-size: 24px\" href=\"/tag/humor/\">humor</a>\n",
      "            </span>\n",
      "            \n",
      "            <span class=\"tag-item\">\n",
      "            <a class=\"tag\" style=\"font-size: 22px\" href=\"/tag/books/\">books</a>\n",
      "            </span>\n",
      "            \n",
      "            <span class=\"tag-item\">\n",
      "            <a class=\"tag\" style=\"font-size: 14px\" href=\"/tag/reading/\">reading</a>\n",
      "            </span>\n",
      "            \n",
      "            <span class=\"tag-item\">\n",
      "            <a class=\"tag\" style=\"font-size: 10px\" href=\"/tag/friendship/\">friendship</a>\n",
      "            </span>\n",
      "            \n",
      "            <span class=\"tag-item\">\n",
      "            <a class=\"tag\" style=\"font-size: 8px\" href=\"/tag/friends/\">friends</a>\n",
      "            </span>\n",
      "            \n",
      "            <span class=\"tag-item\">\n",
      "            <a class=\"tag\" style=\"font-size: 8px\" href=\"/tag/truth/\">truth</a>\n",
      "            </span>\n",
      "            \n",
      "            <span class=\"tag-item\">\n",
      "            <a class=\"tag\" style=\"font-size: 6px\" href=\"/tag/simile/\">simile</a>\n",
      "            </span>\n",
      "            \n",
      "        \n",
      "    </div>\n",
      "</div>\n",
      "\n",
      "    </div>\n",
      "    <footer class=\"footer\">\n",
      "        <div class=\"container\">\n",
      "            <p class=\"text-muted\">\n",
      "                Quotes by: <a href=\"https://www.goodreads.com/quotes\">GoodReads.com</a>\n",
      "            </p>\n",
      "            <p class=\"copyright\">\n",
      "                Made with <span class='zyte'>❤</span> by <a class='zyte' href=\"https://www.zyte.com\">Zyte</a>\n",
      "            </p>\n",
      "        </div>\n",
      "    </footer>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "cat ./tutorial/quotes-1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc12bb0-3ad8-475c-8c62-f34a35fd7775",
   "metadata": {},
   "source": [
    "## What just happened under the hood?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbf9bea-4482-499c-b60a-8e16f745f855",
   "metadata": {},
   "source": [
    "Scrapy schedules the `scrapy.Request` objects returned by the `start_requests` method of the Spider. Upon receiving a `response` for each one, it instantiates `Response` objects and calls the callback method associated with the `request` (in this case, the `parse` method) passing the response as argument."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1817e5-2ae7-4871-84e6-bdceceb6cad1",
   "metadata": {},
   "source": [
    "## A shortcut to the `start_requests` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d87e87-96ac-4dda-8585-daf4d42c0839",
   "metadata": {},
   "source": [
    "Instead of implementing a `start_requests()` method that generates `scrapy.Request` objects from URLs, you can just define a `start_urls` class attribute with a list of URLs. This list will then be used by the default implementation of `start_requests()` to create the initial requests for your spider.\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    urls = [\n",
    "        \"https://quotes.toscrape.com/page/1/\",\n",
    "        \"https://quotes.toscrape.com/page/2/\",\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = f\"quotes-{page}.html\"\n",
    "        Path(filename).write_bytes(response.body)\n",
    "        self.log(f\"Saved file {filename}\")\n",
    "```\n",
    "\n",
    "The `parse()` method will be called to handle each of the requests for those URLs, even though we haven’t explicitly told Scrapy to do so. This happens because `parse()` is Scrapy’s default callback method, which is called for requests without an explicitly assigned callback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7cb801-1c53-4875-8d28-06c6864a880d",
   "metadata": {},
   "source": [
    "## Extracting data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5074321-f17a-4daf-bdb7-9d4e84341021",
   "metadata": {},
   "source": [
    "The best way to learn how to extract data with Scrapy is trying **selectors** using the Scrapy shell. Run:\n",
    "\n",
    "> Note: Remember to always enclose urls in quotes when running Scrapy shell from command-line, otherwise urls containing arguments (i.e. & character) will not work.<br>\n",
    "</br>\n",
    "On Windows, use double quotes instead:<br>\n",
    "</br>\n",
    "`scrapy shell \"https://quotes.toscrape.com/page/1/\"`\n",
    "\n",
    "```sh\n",
    "scrapy shell 'https://quotes.toscrape.com/page/1/'\n",
    "```\n",
    "```\n",
    "2024-02-06 14:47:14 [scrapy.utils.log] INFO: Scrapy 2.11.0 started (bot: tutorial)\n",
    "2024-02-06 14:47:14 [scrapy.utils.log] INFO: Versions: lxml 5.1.0.0, libxml2 2.12.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.11.2 (main, Mar 13 2023, 12:18:29) [GCC 12.2.0], pyOpenSSL 24.0.0 (OpenSSL 3.2.1 30 Jan 2024), cryptography 42.0.2, Platform Linux-6.1.0-17-amd64-x86_64-with-glibc2.36\n",
    "2024-02-06 14:47:14 [scrapy.addons] INFO: Enabled addons:\n",
    "[]\n",
    "2024-02-06 14:47:14 [asyncio] DEBUG: Using selector: EpollSelector\n",
    "2024-02-06 14:47:14 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
    "2024-02-06 14:47:14 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
    "2024-02-06 14:47:14 [scrapy.extensions.telnet] INFO: Telnet Password: 53d4e3939b5fb7e7\n",
    "2024-02-06 14:47:14 [scrapy.middleware] INFO: Enabled extensions:\n",
    "['scrapy.extensions.corestats.CoreStats',\n",
    " 'scrapy.extensions.telnet.TelnetConsole',\n",
    " 'scrapy.extensions.memusage.MemoryUsage']\n",
    "2024-02-06 14:47:14 [scrapy.crawler] INFO: Overridden settings:\n",
    "{'BOT_NAME': 'tutorial',\n",
    " 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',\n",
    " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
    " 'LOGSTATS_INTERVAL': 0,\n",
    " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
    " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
    " 'ROBOTSTXT_OBEY': True,\n",
    " 'SPIDER_MODULES': ['tutorial.spiders'],\n",
    " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
    "2024-02-06 14:47:14 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
    "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
    " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
    " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
    " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
    " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
    " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
    " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
    " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
    " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
    " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
    " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
    " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
    "2024-02-06 14:47:14 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
    "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
    " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
    " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
    " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
    " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
    "2024-02-06 14:47:14 [scrapy.middleware] INFO: Enabled item pipelines:\n",
    "[]\n",
    "2024-02-06 14:47:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
    "2024-02-06 14:47:14 [scrapy.core.engine] INFO: Spider opened\n",
    "2024-02-06 14:47:15 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://quotes.toscrape.com/robots.txt> (referer: None)\n",
    "2024-02-06 14:47:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/1/> (referer: None)\n",
    "[s] Available Scrapy objects:\n",
    "[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\n",
    "[s]   crawler    <scrapy.crawler.Crawler object at 0x7f5b3ece0cd0>\n",
    "[s]   item       {}\n",
    "[s]   request    <GET https://quotes.toscrape.com/page/1/>\n",
    "[s]   response   <200 https://quotes.toscrape.com/page/1/>\n",
    "[s]   settings   <scrapy.settings.Settings object at 0x7f5b3ffddc10>\n",
    "[s]   spider     <DefaultSpider 'default' at 0x7f5b3e7fa950>\n",
    "[s] Useful shortcuts:\n",
    "[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)\n",
    "[s]   fetch(req)                  Fetch a scrapy.Request and update local objects \n",
    "[s]   shelp()           Shell help (print this help)\n",
    "[s]   view(response)    View response in a browser\n",
    "2024-02-06 14:47:16 [asyncio] DEBUG: Using selector: EpollSelector\n",
    "```\n",
    "```ipython\n",
    "In [1]: \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724096bd-8857-43c4-914f-5f51b35222d9",
   "metadata": {},
   "source": [
    "Using the shell, you can try selecting elements using [CSS](https://www.w3.org/TR/selectors) with the `response` object:\n",
    "\n",
    "```ipython\n",
    "In [1]: response.css(\"title\")\n",
    "Out[1]: [<Selector query='descendant-or-self::title' data='<title>Quotes to Scrape</title>'>]\n",
    "\n",
    "In [2]: response.status\n",
    "Out[2]: 200\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b34527-4301-4b70-be21-f50099b57f90",
   "metadata": {},
   "source": [
    "### `get_all()` and `get()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86093451-8b67-4636-98ed-91283d3236a6",
   "metadata": {},
   "source": [
    "The result of running `response.css('title')` is a list-like object called **SelectorList**, which represents a list of Selector objects that wrap around XML/HTML elements and allow you to run further queries to fine-grain the selection or extract the data.\n",
    "\n",
    "To extract the text from the title above, you can do:\n",
    "\n",
    "```ipython\n",
    "In [9]: response.css(\"title::text\").getall()\n",
    "Out[9]: ['Quotes to Scrape']\n",
    "```\n",
    "\n",
    "There are two things to note here: one is that we’ve added `::text` to the CSS query, to mean we want to select only the text elements directly inside `<title>` element. If we don’t specify `::text`, we’d get the full title element, including its tags:\n",
    "\n",
    "```ipython\n",
    "In [11]: response.css(\"title\").getall()\n",
    "Out[11]: ['<title>Quotes to Scrape</title>']\n",
    "```\n",
    "\n",
    "The other thing is that the result of calling `.getall()` is a _list_: it is possible that a selector returns more than one result, so we extract them all. When you know you just want the first result, as in this case, you can do:\n",
    "\n",
    "```ipython\n",
    "In [12]: response.css(\"title::text\").get()\n",
    "Out[12]: 'Quotes to Scrape'\n",
    "```\n",
    "\n",
    "As an alternative, you could’ve written:\n",
    "\n",
    "```ipython\n",
    "In [16]: response.css(\"title::text\")[0].get()\n",
    "Out[16]: 'Quotes to Scrape'\n",
    "```\n",
    "\n",
    "Accessing an index on a SelectorList instance will raise an `IndexError` exception if there are no results. You might want to use `.get()` directly on the SelectorList instance instead, which returns `None` if there are no results:\n",
    "\n",
    "```ipython\n",
    "In [17]: response.css(\"noelement\").get()\n",
    "In [18]: response.css(\"noelement\")[0].get()\n",
    "---------------------------------------------------------------------------\n",
    "IndexError                                Traceback (most recent call last)\n",
    "...\n",
    "IndexError: list index out of range\n",
    "```\n",
    "\n",
    "There’s a lesson here: for most scraping code, you want it to be resilient to errors due to things not being found on a page, so that even if some parts fail to be scraped, you can at least get some data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55e7840-8d39-445e-8504-060085130009",
   "metadata": {},
   "source": [
    "### `re()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7055b91-e2f2-431e-9d30-c3b36a2750f8",
   "metadata": {},
   "source": [
    "Besides the `getall()` and `get()` methods, you can also use the `re()` method to extract using [regular expressions](https://docs.python.org/3/library/re.html):\n",
    "\n",
    "```ipython\n",
    "In [20]: response.css(\"title::text\").re(r\".*uot.*\")\n",
    "Out[20]: ['Quotes to Scrape']\n",
    "\n",
    "In [21]: response.css(\"title::text\").re(r\"Q\\w+\")\n",
    "Out[21]: ['Quotes']\n",
    "\n",
    "In [22]: response.css(\"title::text\").re(r\"(\\w+) to (\\w+)\")\n",
    "Out[22]: ['Quotes', 'Scrape']\n",
    "```\n",
    "\n",
    "- `\\w` represents any alphanumeric character (equivalent to `[a-zA-Z0-9_]`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dadabc-6323-4054-9a69-20753f17d10a",
   "metadata": {},
   "source": [
    "### `view(response)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de88dc81-81b3-4be4-b122-93cb530599a3",
   "metadata": {},
   "source": [
    "In order to find the proper CSS selectors to use, you might find it useful to open the `response` page from the shell in your web browser using `view(response)`. You can use your browser’s developer tools to inspect the HTML and come up with a selector (see [Using your browser’s Developer Tools for scraping](https://docs.scrapy.org/en/latest/topics/developer-tools.html#topics-developer-tools)).\n",
    "\n",
    "[Selector Gadget](https://selectorgadget.com/) is also a nice tool to quickly find CSS selector for visually selected elements, which works in many browsers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3677014c-d779-438a-b57d-dee723631675",
   "metadata": {},
   "source": [
    "### `XPath`: a brief intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74464531-cb38-4b0f-9444-3c2533fbf37f",
   "metadata": {},
   "source": [
    "See [XPath](../XPath_tutorial.ipynb#XPath).\n",
    "\n",
    "Besides CSS, Scrapy selectors also support using XPath expressions:\n",
    "\n",
    "```ipython\n",
    "In [24]: response.xpath(\"//title\")\n",
    "Out[24]: [<Selector query='//title' data='<title>Quotes to Scrape</title>'>]\n",
    "\n",
    "In [25]: response.xpath(\"//title/text()\").get()\n",
    "Out[25]: 'Quotes to Scrape'\n",
    "```\n",
    "\n",
    "XPath expressions are very powerful, and are the foundation of Scrapy Selectors. In fact, CSS selectors are converted to XPath under-the-hood. [You can see that](#Extracting-data) if you read closely the text representation of the selector objects in the shell.\n",
    "\n",
    "While perhaps not as popular as CSS selectors, XPath expressions offer more power because besides navigating the structure, it can also look at the content. Using XPath, you’re able to select things like: _select the link that contains the text “Next Page”_. This makes XPath very fitting to the task of scraping, and we encourage you to learn XPath even if you already know how to construct CSS selectors, it will make scraping much easier.\n",
    "\n",
    "We won’t cover much of XPath here, but you can read more about using [XPath with Scrapy Selectors](https://docs.scrapy.org/en/latest/topics/selectors.html#topics-selectors). To learn more about XPath, we recommend this [tutorial to learn XPath through examples](http://zvon.org/comp/r/tut-XPath_1.html), and this tutorial to learn [“how to think in XPath”](http://plasmasturm.org/log/xpath101/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fe8ac6-9ebe-4464-b0a9-7b0dea6e4569",
   "metadata": {},
   "source": [
    "### Extracting quotes and authors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035b167e-98de-4068-9c34-61ecdb081034",
   "metadata": {},
   "source": [
    "Now that you know a bit about selection and extraction, let’s complete our spider by writing the code to extract the quotes from the web page.\n",
    "\n",
    "Each quote in `https://quotes.toscrape.com` is represented by HTML elements that look like this:\n",
    "\n",
    "```html\n",
    "<div class=\"quote\">\n",
    "    <span class=\"text\">“The world as we have created it is a process of our\n",
    "    thinking. It cannot be changed without changing our thinking.”</span>\n",
    "    <span>\n",
    "        by <small class=\"author\">Albert Einstein</small>\n",
    "        <a href=\"/author/Albert-Einstein\">(about)</a>\n",
    "    </span>\n",
    "    <div class=\"tags\">\n",
    "        Tags:\n",
    "        <a class=\"tag\" href=\"/tag/change/page/1/\">change</a>\n",
    "        <a class=\"tag\" href=\"/tag/deep-thoughts/page/1/\">deep-thoughts</a>\n",
    "        <a class=\"tag\" href=\"/tag/thinking/page/1/\">thinking</a>\n",
    "        <a class=\"tag\" href=\"/tag/world/page/1/\">world</a>\n",
    "    </div>\n",
    "</div>\n",
    "```\n",
    "\n",
    "Let’s open up `scrapy shell` and play a bit to find out how to extract the data we want. We get a list of selectors for the quote HTML elements with:\n",
    "\n",
    "```sh\n",
    "scrapy shell 'https://quotes.toscrape.com'\n",
    "```\n",
    "```ipython\n",
    "In [1]: response.css(\"div.quote\")\n",
    "Out[1]: \n",
    "[<Selector query=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>,\n",
    " <Selector query=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>,\n",
    " <Selector query=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>,\n",
    " <Selector query=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>,\n",
    " <Selector query=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>,\n",
    " <Selector query=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>,\n",
    " <Selector query=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>,\n",
    " <Selector query=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>,\n",
    " <Selector query=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>,\n",
    " <Selector query=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>]\n",
    "```\n",
    "\n",
    "Each of the selectors returned by the query above allows us to run further queries over their sub-elements. Let’s assign the first selector to a variable, so that we can run our CSS selectors directly on a particular quote:\n",
    "\n",
    "```ipython\n",
    "In [3]: quote = response.css(\"div.quote\")[0]\n",
    "```\n",
    "\n",
    "Now, let’s extract `text`, `author` and the `tags` from that quote using the `quote` object we just created:\n",
    "\n",
    "```ipython\n",
    "In [4]: text = quote.css(\"span.text::text\").get()\n",
    "In [5]: text\n",
    "Out[5]: '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”'\n",
    "```\n",
    "\n",
    "Given that the `tags` are a list of strings, we can use the `.getall()` method to get all of them:\n",
    "\n",
    "```ipython\n",
    "In [6]: tags = quote.css(\"div.tags a.tag::text\").getall()\n",
    "In [7]: tags\n",
    "Out[7]: ['change', 'deep-thoughts', 'thinking', 'world']\n",
    "```\n",
    "\n",
    "Having figured out how to extract each bit, we can now iterate over all the quotes elements and put them together into a Python dictionary:\n",
    "\n",
    "```ipython\n",
    "In [8]: for quote in response.css(\"div.quote\"):\n",
    "   ...:     text = quote.css(\"span.text::text\").get()\n",
    "   ...:     author = quote.css(\"small.author::text\").get()\n",
    "   ...:     tags = quote.css(\"div.tags a.tag::text\").getall()\n",
    "   ...:     print(dict(text=text, author=author, tags=tags))\n",
    "   ...: \n",
    "{'text': '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”', 'author': 'Albert Einstein', 'tags': ['change', 'deep-thoughts', 'thinking', 'world']}\n",
    "{'text': '“It is our choices, Harry, that show what we truly are, far more than our abilities.”', 'author': 'J.K. Rowling', 'tags': ['abilities', 'choices']}\n",
    "{'text': '“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”', 'author': 'Albert Einstein', 'tags': ['inspirational', 'life', 'live', 'miracle', 'miracles']}\n",
    "{'text': '“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”', 'author': 'Jane Austen', 'tags': ['aliteracy', 'books', 'classic', 'humor']}\n",
    "{'text': \"“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\", 'author': 'Marilyn Monroe', 'tags': ['be-yourself', 'inspirational']}\n",
    "{'text': '“Try not to become a man of success. Rather become a man of value.”', 'author': 'Albert Einstein', 'tags': ['adulthood', 'success', 'value']}\n",
    "{'text': '“It is better to be hated for what you are than to be loved for what you are not.”', 'author': 'André Gide', 'tags': ['life', 'love']}\n",
    "{'text': \"“I have not failed. I've just found 10,000 ways that won't work.”\", 'author': 'Thomas A. Edison', 'tags': ['edison', 'failure', 'inspirational', 'paraphrased']}\n",
    "{'text': \"“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\", 'author': 'Eleanor Roosevelt', 'tags': ['misattributed-eleanor-roosevelt']}\n",
    "{'text': '“A day without sunshine is like, you know, night.”', 'author': 'Steve Martin', 'tags': ['humor', 'obvious', 'simile']}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc322623-939f-49b5-940d-ab90693866c1",
   "metadata": {},
   "source": [
    "### Extracting data in our spider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eaf8d7-9f0e-4a5e-b4a0-5491acdf0a4e",
   "metadata": {},
   "source": [
    "Let’s get back to our spider. Until now, it doesn’t extract any data in particular, just saves the whole HTML page to a local file. Let’s integrate the extraction logic above into our spider.\n",
    "\n",
    "A Scrapy spider typically generates many dictionaries containing the data extracted from the page. To do that, we use the `yield` Python keyword in the `callback`, as you can see below:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    urls = [\n",
    "        \"https://quotes.toscrape.com/page/1/\",\n",
    "        \"https://quotes.toscrape.com/page/2/\",\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css(\"div.quote\"):\n",
    "            yield {\n",
    "                \"text\": quote.css(\"span.text::text\").get(),\n",
    "                \"author\": quote.css(\"small.author::text\").get(),\n",
    "                \"tags\": quote.css(\"div.tags a.tag::text\").getall(),\n",
    "            }\n",
    "```\n",
    "\n",
    "To run this spider, exit the `scrapy shell` and run the crawler:\n",
    "\n",
    "```sh\n",
    "quit()\n",
    "scrapy crawl quotes\n",
    "```\n",
    "```\n",
    "2024-02-06 23:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
    "{'text': '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”', 'author': 'Albert Einstein', 'tags': ['change', 'deep-thoughts', 'thinking', 'world']}\n",
    "2024-02-06 23:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
    "{'text': '“It is our choices, Harry, that show what we truly are, far more than our abilities.”', 'author': 'J.K. Rowling', 'tags': ['abilities', 'choices']}\n",
    "2024-02-06 23:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
    "{'text': '“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”', 'author': 'Albert Einstein', 'tags': ['inspirational', 'life', 'live', 'miracle', 'miracles']}\n",
    "2024-02-06 23:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
    "{'text': '“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”', 'author': 'Jane Austen', 'tags': ['aliteracy', 'books', 'classic', 'humor']}\n",
    "2024-02-06 23:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
    "{'text': \"“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\", 'author': 'Marilyn Monroe', 'tags': ['be-yourself', 'inspirational']}\n",
    "2024-02-06 23:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
    "{'text': '“Try not to become a man of success. Rather become a man of value.”', 'author': 'Albert Einstein', 'tags': ['adulthood', 'success', 'value']}\n",
    "2024-02-06 23:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
    "{'text': '“It is better to be hated for what you are than to be loved for what you are not.”', 'author': 'André Gide', 'tags': ['life', 'love']}\n",
    "2024-02-06 23:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
    "{'text': \"“I have not failed. I've just found 10,000 ways that won't work.”\", 'author': 'Thomas A. Edison', 'tags': ['edison', 'failure', 'inspirational', 'paraphrased']}\n",
    "2024-02-06 23:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
    "{'text': \"“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\", 'author': 'Eleanor Roosevelt', 'tags': ['misattributed-eleanor-roosevelt']}\n",
    "2024-02-06 23:53:52 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
    "{'text': '“A day without sunshine is like, you know, night.”', 'author': 'Steve Martin', 'tags': ['humor', 'obvious', 'simile']}\n",
    "2024-02-06 23:53:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/2/> (referer: None)\n",
    "2024-02-06 23:53:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
    "{'text': \"“This life is what you make it. No matter what, you're going to mess up sometimes, it's a universal truth. But the good part is you get to decide how you're going to mess it up. Girls will be your friends - they'll act like it anyway. But just remember, some come, some go. The ones that stay with you through everything - they're your true best friends. Don't let go of them. Also remember, sisters make the best friends in the world. As for lovers, well, they'll come and go too. And baby, I hate to say it, most of them - actually pretty much all of them are going to break your heart, but you can't give up because if you give up, you'll never find your soulmate. You'll never find that half who makes you whole and that goes for everything. Just because you fail once, doesn't mean you're gonna fail at everything. Keep trying, hold on, and always, always, always believe in yourself, because if you don't, then who will, sweetie? So keep your head high, keep your chin up, and most importantly, keep smiling, because life's a beautiful thing and there's so much to smile about.”\", 'author': 'Marilyn Monroe', 'tags': ['friends', 'heartbreak', 'inspirational', 'life', 'love', 'sisters']}\n",
    "2024-02-06 23:53:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
    "{'text': '“It takes a great deal of bravery to stand up to our enemies, but just as much to stand up to our friends.”', 'author': 'J.K. Rowling', 'tags': ['courage', 'friends']}\n",
    "2024-02-06 23:53:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
    "{'text': \"“If you can't explain it to a six year old, you don't understand it yourself.”\", 'author': 'Albert Einstein', 'tags': ['simplicity', 'understand']}\n",
    "2024-02-06 23:53:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
    "{'text': \"“You may not be her first, her last, or her only. She loved before she may love again. But if she loves you now, what else matters? She's not perfect—you aren't either, and the two of you may never be perfect together but if she can make you laugh, cause you to think twice, and admit to being human and making mistakes, hold onto her and give her the most you can. She may not be thinking about you every second of the day, but she will give you a part of her that she knows you can break—her heart. So don't hurt her, don't change her, don't analyze and don't expect more than she can give. Smile when she makes you happy, let her know when she makes you mad, and miss her when she's not there.”\", 'author': 'Bob Marley', 'tags': ['love']}\n",
    "2024-02-06 23:53:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
    "{'text': '“I like nonsense, it wakes up the brain cells. Fantasy is a necessary ingredient in living.”', 'author': 'Dr. Seuss', 'tags': ['fantasy']}\n",
    "2024-02-06 23:53:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
    "{'text': '“I may not have gone where I intended to go, but I think I have ended up where I needed to be.”', 'author': 'Douglas Adams', 'tags': ['life', 'navigation']}\n",
    "2024-02-06 23:53:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
    "{'text': \"“The opposite of love is not hate, it's indifference. The opposite of art is not ugliness, it's indifference. The opposite of faith is not heresy, it's indifference. And the opposite of life is not death, it's indifference.”\", 'author': 'Elie Wiesel', 'tags': ['activism', 'apathy', 'hate', 'indifference', 'inspirational', 'love', 'opposite', 'philosophy']}\n",
    "2024-02-06 23:53:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
    "{'text': '“It is not a lack of love, but a lack of friendship that makes unhappy marriages.”', 'author': 'Friedrich Nietzsche', 'tags': ['friendship', 'lack-of-friendship', 'lack-of-love', 'love', 'marriage', 'unhappy-marriage']}\n",
    "2024-02-06 23:53:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
    "{'text': '“Good friends, good books, and a sleepy conscience: this is the ideal life.”', 'author': 'Mark Twain', 'tags': ['books', 'contentment', 'friends', 'friendship', 'life']}\n",
    "2024-02-06 23:53:53 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/2/>\n",
    "{'text': '“Life is what happens to us while we are making other plans.”', 'author': 'Allen Saunders', 'tags': ['fate', 'life', 'misattributed-john-lennon', 'planning', 'plans']}\n",
    "2024-02-06 23:53:53 [scrapy.core.engine] INFO: Closing spider (finished)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0468328-06de-44fa-a1ed-8cff4be03ee4",
   "metadata": {},
   "source": [
    "## Storing the scraped data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c31e6f5-1a84-4e62-bbed-e2a0e860f768",
   "metadata": {},
   "source": [
    "The simplest way to store the scraped data is by using [Feed exports](#2.8-Feed-exports), with the following command:\n",
    "\n",
    "```sh\n",
    "scrapy crawl quotes -O quotes.json\n",
    "```\n",
    "\n",
    "That will generate a `quotes.json` file containing all scraped items, serialized in JSON.\n",
    "\n",
    "The `-O` command-line switch overwrites any existing file; use `-o` instead to append new content to any existing file. However, appending to a JSON file makes the file contents invalid JSON. When appending to a file, consider using a different serialization format, such as `JSON Lines`:\n",
    "\n",
    "```sh\n",
    "scrapy crawl quotes -o quotes.jsonl\n",
    "```\n",
    "\n",
    "The [JSON Lines format](http://jsonlines.org/) is useful because it’s stream-like, you can easily append new records to it. It doesn’t have the same problem of JSON when you run twice. Also, as each record is a separate line, you can process big files without having to fit everything in memory, there are tools like `JQ` to help do that at the command-line.\n",
    "\n",
    "In small projects (like the one in this tutorial), that should be enough. However, if you want to perform more complex things with the scraped items, you can write an [Item Pipeline](#2.7-Item-Pipeline). A placeholder file for Item Pipelines has been set up for you when the project is created, in `tutorial/pipelines.py`. Though you don’t need to implement any item pipelines if you just want to store the scraped items.\n",
    "\n",
    "```sh\n",
    "$ scrapy crawl -h\n",
    "```\n",
    "```\n",
    "Usage\n",
    "=====\n",
    "  scrapy crawl [options] <spider>\n",
    "\n",
    "Run a spider\n",
    "\n",
    "Options\n",
    "=======\n",
    "  -h, --help            show this help message and exit\n",
    "  -a NAME=VALUE         set spider argument (may be repeated)\n",
    "  -o FILE, --output FILE\n",
    "                        append scraped items to the end of FILE (use - for stdout), to define format set a colon at the end of the output\n",
    "                        URI (i.e. -o FILE:FORMAT)\n",
    "  -O FILE, --overwrite-output FILE\n",
    "                        dump scraped items into FILE, overwriting any existing file, to define format set a colon at the end of the\n",
    "                        output URI (i.e. -O FILE:FORMAT)\n",
    "  -t FORMAT, --output-format FORMAT\n",
    "                        format to use for dumping items\n",
    "\n",
    "Global Options\n",
    "--------------\n",
    "  --logfile FILE        log file. if omitted stderr will be used\n",
    "  -L LEVEL, --loglevel LEVEL\n",
    "                        log level (default: DEBUG)\n",
    "  --nolog               disable logging completely\n",
    "  --profile FILE        write python cProfile stats to FILE\n",
    "  --pidfile FILE        write process ID to FILE\n",
    "  -s NAME=VALUE, --set NAME=VALUE\n",
    "                        set/override setting (may be repeated)\n",
    "  --pdb                 enable pdb on failure\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1edd287b-96a0-4af3-a5f5-95f69dc6e739",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-06T19:19:42.039144Z",
     "iopub.status.busy": "2024-02-06T19:19:42.037500Z",
     "iopub.status.idle": "2024-02-06T19:19:42.052495Z",
     "shell.execute_reply": "2024-02-06T19:19:42.048680Z",
     "shell.execute_reply.started": "2024-02-06T19:19:42.039015Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# man jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d7db66-c191-448e-9218-a196f8ff5fbe",
   "metadata": {},
   "source": [
    "## Following links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f35f7cb-a8f6-4334-9b0f-9051deb9ed3f",
   "metadata": {},
   "source": [
    "Let’s say, instead of just scraping the stuff from the first two pages from https://quotes.toscrape.com, you want quotes from all the pages in the website.\n",
    "\n",
    "Now that you know how to extract data from pages, let’s see how to follow links from them.\n",
    "\n",
    "First thing is to extract the link to the page we want to follow. Examining our page, we can see there is a link to the next page with the following markup:\n",
    "\n",
    "```html\n",
    "<ul class=\"pager\">\n",
    "    <li class=\"next\">\n",
    "        <a href=\"/page/2/\">Next <span aria-hidden=\"true\">&rarr;</span></a>\n",
    "    </li>\n",
    "</ul>\n",
    "```\n",
    "\n",
    "We can try extracting it in the shell:\n",
    "\n",
    "```ipython\n",
    "In [1]: response.css(\"li.next a\")\n",
    "Out[1]: [<Selector query=\"descendant-or-self::li[@class and contains(concat(' ', normalize-space(@class), ' '), ' next ')]/descendant-or-self::*/a\" data='<a href=\"/page/2/\">Next <span aria-hi...'>]\n",
    "\n",
    "In [2]: response.css(\"li.next a\").get()\n",
    "Out[2]: '<a href=\"/page/2/\">Next <span aria-hidden=\"true\">→</span></a>'\n",
    "```\n",
    "\n",
    "This gets the anchor element, but we want the attribute `href`. For that, Scrapy supports a CSS extension that lets you select the attribute contents, like this:\n",
    "\n",
    "```ipython\n",
    "In [3]: response.css(\"li.next a::attr(href)\").get()\n",
    "Out[3]: '/page/2/'\n",
    "```\n",
    "There is also an `attrib` property available (see [Selecting element attributes](#Selecting-element-attributes) for more):\n",
    "\n",
    "```ipython\n",
    "In [4]: response.css(\"li.next a\").attrib[\"href\"]\n",
    "Out[4]: '/page/2/'\n",
    "```\n",
    "\n",
    "Let’s see now our spider modified to recursively follow the link to the next page, extracting data from it:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        \"https://quotes.toscrape.com/page/1/\",\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css(\"div.quote\"):\n",
    "            yield {\n",
    "                \"text\": quote.css(\"span.text::text\").get(),\n",
    "                \"author\": quote.css(\"small.author::text\").get(),\n",
    "                \"tags\": quote.css(\"div.tags a.tag::text\").getall(),\n",
    "            }\n",
    "\n",
    "        next_page = response.css(\"li.next a::attr(href)\").get()\n",
    "        if next_page is not None:\n",
    "            next_page = response.urljoin(next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "```\n",
    "\n",
    "Now, after extracting the data, the `parse()` method looks for the link to the next page, builds a full absolute URL using the `urljoin()` method (since the links can be relative) and `yield`s a new request to the next page, registering itself as callback to handle the data extraction for the next page and to keep the crawling going through all the pages.\n",
    "\n",
    "What you see here is Scrapy’s mechanism of following links: when you `yield` a `Request` in a callback method, Scrapy will schedule that request to be sent and register a callback method to be executed when that request finishes.\n",
    "\n",
    "Using this, you can build complex crawlers that follow links according to rules you define, and extract different kinds of data depending on the page it’s visiting.\n",
    "\n",
    "In our example, it creates a sort of loop, following all the links to the next page until it doesn’t find one – handy for crawling blogs, forums and other sites with **pagination**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcfc124-ed0e-4829-aee8-0adb585de4b3",
   "metadata": {},
   "source": [
    "### A shortcut for creating Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d1ec72-a2f2-48e9-a8e0-2a32a371c98d",
   "metadata": {},
   "source": [
    "As a shortcut for creating `Request` objects you can use `response.follow`:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        \"https://quotes.toscrape.com/page/1/\",\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css(\"div.quote\"):\n",
    "            yield {\n",
    "                \"text\": quote.css(\"span.text::text\").get(),\n",
    "                \"author\": quote.css(\"span small::text\").get(),\n",
    "                \"tags\": quote.css(\"div.tags a.tag::text\").getall(),\n",
    "            }\n",
    "\n",
    "        next_page = response.css(\"li.next a::attr(href)\").get()\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, callback=self.parse)\n",
    "```\n",
    "\n",
    "Unlike `scrapy.Request`, `response.follow` supports relative URLs directly - no need to call `urljoin`. Note that `response.follow` just returns a `Request` instance; you still have to `yield` this `Request`.\n",
    "\n",
    "You can also pass a selector to `response.follow` instead of a string; this selector should extract necessary attributes:\n",
    "\n",
    "```python\n",
    "for href in response.css(\"ul.pager a::attr(href)\"):\n",
    "    yield response.follow(href, callback=self.parse)\n",
    "```\n",
    "\n",
    "For `<a>` elements there is a shortcut: `response.follow` uses their `href` attribute automatically. So the code can be shortened further:\n",
    "\n",
    "```python\n",
    "for a in response.css(\"ul.pager a\"):\n",
    "    yield response.follow(a, callback=self.parse)\n",
    "```\n",
    "\n",
    "To create multiple requests from an iterable, you can use `response.follow_all` instead:\n",
    "\n",
    "```python\n",
    "anchors = response.css(\"ul.pager a\")\n",
    "yield from response.follow_all(anchors, callback=self.parse)\n",
    "```\n",
    "\n",
    "or, shortening it further:\n",
    "\n",
    "```python\n",
    "yield from response.follow_all(css=\"ul.pager a\", callback=self.parse)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3462a3bf-fff6-4f92-9fde-ee15248b9743",
   "metadata": {},
   "source": [
    "## More examples and patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523e5ad1-05c5-4636-98c9-8a016fae4ead",
   "metadata": {},
   "source": [
    "Here is another spider that illustrates callbacks and following links, this time for scraping author information:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class AuthorSpider(scrapy.Spider):\n",
    "    name = \"author\"\n",
    "\n",
    "    start_urls = [\"https://quotes.toscrape.com/\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        author_page_links = response.css(\".author + a\")\n",
    "        yield from resonse.follow_all(author_page_links, self.parse_author)\n",
    "\n",
    "        pagination_links = response.css(\"li.next a\")\n",
    "        yield from response.follow_all(pagination_links, self.parse)\n",
    "\n",
    "    def parse_author(self, response):\n",
    "        def extract_with_css(query):\n",
    "            return response.css(query).get(default=\"\").strip()\n",
    "\n",
    "        yield {\n",
    "            \"name\": extract_with_css(\"h3.author-title::text\"),\n",
    "            \"birthdate\": extract_with_css(\".author-born-date::text\"),\n",
    "            \"bio\": extract_with_css(\".author-description::text\"),\n",
    "        }\n",
    "```\n",
    "\n",
    "This spider will start from the main page, it will follow all the links to the authors pages calling the `parse_author` callback for each of them, and also the pagination links with the `parse` callback as we saw before.\n",
    "\n",
    "Here we’re passing callbacks to `response.follow_all` as positional arguments to make the code shorter; it also works for `Request`.\n",
    "\n",
    "The `parse_author` callback defines a helper function to extract and cleanup the data from a CSS query and `yield`s the Python `dict` with the author data.\n",
    "\n",
    "Another interesting thing this spider demonstrates is that, even if there are many quotes from the same author, we don’t need to worry about visiting the same author page multiple times. By default, Scrapy filters out duplicated requests to URLs already visited, avoiding the problem of hitting servers too much because of a programming mistake. This can be configured by the setting `DUPEFILTER_CLASS`.\n",
    "\n",
    "Hopefully by now you have a good understanding of how to use the mechanism of following links and callbacks with Scrapy.\n",
    "\n",
    "As yet another example spider that leverages the mechanism of following links, check out the `CrawlSpider` class for a generic spider that implements a small rules engine that you can use to write your crawlers on top of it.\n",
    "\n",
    "Also, a common pattern is to build an item with data from more than one page, using a trick to [pass additional data to the callbacks](#Passing-additional-data-to-callback-functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d457fd-147e-4115-bbc2-bb2b52127752",
   "metadata": {},
   "source": [
    "# 1.3 Using spider arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1381d791-26f4-408d-a798-caa6c5727581",
   "metadata": {},
   "source": [
    "You can provide command line arguments to your spiders by using the `-a` option when running them:\n",
    "\n",
    "```sh\n",
    "scrapy crawl quotes -O quotes-humor.json -a tag=humor\n",
    "```\n",
    "\n",
    "These arguments are passed to the Spider’s `__init__` method and become spider attributes by default.\n",
    "\n",
    "In this example, the value provided for the `tag` argument will be available via `self.tag`. You can use this to make your spider fetch only quotes with a specific tag, building the URL based on the argument:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        url = \"https://quotes.toscrape.com/\"\n",
    "        tag = getattr(self, \"tag\", None)\n",
    "        if tag is not None:\n",
    "            url += \"tag/\" + tag\n",
    "\n",
    "        yield scrapy.Request(url, self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css(\"div.quote\"):\n",
    "            yield {\n",
    "                \"text\": quote.css(\"span,text::text\").get(),\n",
    "                \"author\": qutote.css(\"small.author::text\").get(),\n",
    "            }\n",
    "\n",
    "        next_page = response.css(\"li.next a::attr(href)\").get()\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, self.parse)\n",
    "```\n",
    "\n",
    "If you pass the `tag=humor` argument to this spider, you’ll notice that it will only visit URLs from the humor tag, such as `https://quotes.toscrape.com/tag/humor`.\n",
    "\n",
    "You can learn more about [handling spider arguments here](#Spider-arguments)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f185efe-b616-43a1-a5e6-b353428a8bb9",
   "metadata": {},
   "source": [
    "# <b>2. Basic concepts</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcf98ed-4b96-4d47-bf9a-5541f63e6e1d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2.1 Command line tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af389cd8-1346-4b66-a4f7-493eb9e002b0",
   "metadata": {},
   "source": [
    "Scrapy is controlled through the scrapy command-line tool, to be referred here as the **“Scrapy tool”** to differentiate it from the sub-commands, which we just call **“commands”** or **“Scrapy commands”**.\n",
    "\n",
    "The Scrapy tool provides several commands, for multiple purposes, and each one accepts a different set of arguments and options.\n",
    "\n",
    "(The scrapy deploy command has been removed in 1.0 in favor of the standalone `scrapyd-deploy`. See [Deploying your project](#https://scrapyd.readthedocs.io/en/latest/deploy.html).)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ba4f14-0e35-4937-a359-047d7f31b446",
   "metadata": {},
   "source": [
    "## Configuration settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47412c3-d7dd-42fb-a252-01a02c749cbe",
   "metadata": {},
   "source": [
    "Scrapy will look for configuration parameters in ini-style `scrapy.cfg` files in standard locations:\n",
    "- `/etc/scrapy.cfg` or `c:\\scrapy\\scrapy.cfg` (system-wide),\n",
    "- `~/.config/scrapy.cfg` (`$XDG_CONFIG_HOME`) and `~/.scrapy.cfg` (`$HOME`) for global (user-wide) settings, and\n",
    "- `scrapy.cfg` inside a Scrapy project’s root (see next section).\n",
    "\n",
    "Settings from these files are merged in the listed order of preference: \n",
    "- user-defined values have higher priority than system-wide defaults and \n",
    "- project-wide settings will override all others, when defined.\n",
    "\n",
    "Scrapy also understands, and can be configured through, a number of environment variables. Currently these are:\n",
    "- SCRAPY_SETTINGS_MODULE (see [Designating the settings](#Designating-the-settings))\n",
    "- SCRAPY_PROJECT (see [Sharing the root directory between projects](#Sharing-the-root-directory-between-projects))\n",
    "- SCRAPY_PYTHON_SHELL (see [Scrapy shell](#2.3-Scrapy-shell))\n",
    "\n",
    "_ChatGPT:_  \n",
    "Ini-style, short for \"Initialization style,\" refers to a simple text-based file format used for configuration or initialization files in computing. It is named after the \".ini\" file extension commonly associated with these types of files in Windows environments.\n",
    "\n",
    "Ini-style files consist of sections, each containing key-value pairs, typically used to represent configuration settings for applications or systems. The structure is straightforward, with sections enclosed in square brackets \"`[ ]`\" and key-value pairs separated by an equals sign \"`=`\" or a colon \"`:`\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6e9a81-6f5f-43bd-92b0-ed446b46eb08",
   "metadata": {},
   "source": [
    "### Default structure of Scrapy projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68e269f-b48c-48d8-8762-f77af389761a",
   "metadata": {},
   "source": [
    "Before delving into the command-line tool and its sub-commands, let’s first understand the directory structure of a Scrapy project.\n",
    "\n",
    "Though it can be modified, all Scrapy projects have the same file structure by default, similar to this:\n",
    "\n",
    "```\n",
    "scrapy.cfg\n",
    "myproject/\n",
    "    __init__.py\n",
    "    items.py\n",
    "    middlewares.py\n",
    "    pipelines.py\n",
    "    settings.py\n",
    "    spiders/\n",
    "        __init__.py\n",
    "        spider1.py\n",
    "        spider2.py\n",
    "        ...\n",
    "```\n",
    "\n",
    "The directory where the `scrapy.cfg` file resides is known as the **project root directory**. That file contains the name of the python module that defines the project settings. Here is an example:\n",
    "\n",
    "```\n",
    "[settings]\n",
    "default = myproject.settings\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7001a7c7-757b-47bc-922c-1dec0ac324ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T13:24:10.551128Z",
     "iopub.status.busy": "2024-02-08T13:24:10.550692Z",
     "iopub.status.idle": "2024-02-08T13:24:10.656234Z",
     "shell.execute_reply": "2024-02-08T13:24:10.655199Z",
     "shell.execute_reply.started": "2024-02-08T13:24:10.551091Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/commi/Yandex.Disk/it_learning/08_web_scraping/02_scrapy\n"
     ]
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa09ec7b-fb2b-4281-9797-6881b097d5a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T13:25:15.012909Z",
     "iopub.status.busy": "2024-02-08T13:25:15.012624Z",
     "iopub.status.idle": "2024-02-08T13:25:15.126321Z",
     "shell.execute_reply": "2024-02-08T13:25:15.125041Z",
     "shell.execute_reply.started": "2024-02-08T13:25:15.012886Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mdata/tutorial\u001b[0m\n",
      "├── scrapy.cfg\n",
      "└── \u001b[01;34mtutorial\u001b[0m\n",
      "    ├── __init__.py\n",
      "    ├── items.py\n",
      "    ├── middlewares.py\n",
      "    ├── pipelines.py\n",
      "    ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "    │   ├── __init__.cpython-311.pyc\n",
      "    │   └── settings.cpython-311.pyc\n",
      "    ├── settings.py\n",
      "    └── \u001b[01;34mspiders\u001b[0m\n",
      "        ├── __init__.py\n",
      "        ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "        │   ├── __init__.cpython-311.pyc\n",
      "        │   ├── quotes_spider.cpython-311.pyc\n",
      "        │   └── tmp.cpython-311.pyc\n",
      "        ├── quotes_spider.py\n",
      "        └── tmp.py\n",
      "\n",
      "5 directories, 14 files\n"
     ]
    }
   ],
   "source": [
    "tree data/tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d9cc80-c93d-4b37-a4ee-43a2eb1a91a9",
   "metadata": {},
   "source": [
    "### Sharing the root directory between projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa40661-12a0-46a1-8f32-ae53547311ad",
   "metadata": {},
   "source": [
    "A project root directory, the one that contains the `scrapy.cfg`, may be shared by multiple Scrapy projects, each with its own settings module.\n",
    "\n",
    "In that case, you must define one or more aliases for those settings modules under `[settings]` in your `scrapy.cfg` file:\n",
    "\n",
    "```\n",
    "[settings]\n",
    "default = myproject1.settings\n",
    "project1 = myproject1.settings\n",
    "project2 = myproject2.settings\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64896358-a49f-4c6f-a891-356dc17f0562",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-08T13:26:09.487762Z",
     "iopub.status.busy": "2024-02-08T13:26:09.487365Z",
     "iopub.status.idle": "2024-02-08T13:26:09.596102Z",
     "shell.execute_reply": "2024-02-08T13:26:09.595094Z",
     "shell.execute_reply.started": "2024-02-08T13:26:09.487731Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Automatically created by: scrapy startproject\n",
      "#\n",
      "# For more information about the [deploy] section see:\n",
      "# https://scrapyd.readthedocs.io/en/latest/deploy.html\n",
      "\n",
      "[settings]\n",
      "default = tutorial.settings\n",
      "\n",
      "[deploy]\n",
      "#url = http://localhost:6800/\n",
      "project = tutorial\n"
     ]
    }
   ],
   "source": [
    "cat data/tutorial/scrapy.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e23f15e-2618-4b56-ada8-8228802be2ed",
   "metadata": {},
   "source": [
    "By default, the scrapy command-line tool will use the default settings. Use the `SCRAPY_PROJECT` environment variable to specify a different project for scrapy to use:\n",
    "\n",
    "```sh\n",
    "$ scrapy settings --get BOT_NAME\n",
    "Project 1 Bot\n",
    "$ export SCRAPY_PROJECT=project2\n",
    "$ scrapy settings --get BOT_NAME\n",
    "Project 2 Bot\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e59a853-45ee-45ec-9582-dd76c9ab0315",
   "metadata": {},
   "source": [
    "## Using the `scrapy` tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e3ddb8-0d24-433a-9710-c2fb35857f20",
   "metadata": {},
   "source": [
    "### General info and help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2d48da-5088-4f18-bf4d-770b5a58ecd0",
   "metadata": {},
   "source": [
    "You can start by running the Scrapy tool with no arguments and it will print some usage help and the available commands:\n",
    "\n",
    "```sh\n",
    "Scrapy X.Y - no active project\n",
    "\n",
    "Usage:\n",
    "  scrapy <command> [options] [args]\n",
    "\n",
    "Available commands:\n",
    "  crawl         Run a spider\n",
    "  fetch         Fetch a URL using the Scrapy downloader\n",
    "[...]\n",
    "```\n",
    "\n",
    "The first line will print the currently active project if you’re inside a Scrapy project. In this example it was run from outside a project. If run from inside a project it would have printed something like this:\n",
    "\n",
    "```sh\n",
    "Scrapy X.Y - project: myproject\n",
    "\n",
    "Usage:\n",
    "  scrapy <command> [options] [args]\n",
    "\n",
    "[...]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4357dd-ef05-48e8-8b4e-40c9c6346c26",
   "metadata": {},
   "source": [
    "### Creating projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b4069e-43d8-4c03-b075-b06666e5cd58",
   "metadata": {},
   "source": [
    "The first thing you typically do with the `scrapy` tool is create your Scrapy project:\n",
    "\n",
    "```sh\n",
    "scrapy startproject myproject [project_dir]\n",
    "```\n",
    "\n",
    "That will create a Scrapy project under the `project_dir` directory. If `project_dir` wasn’t specified, `project_dir` will be the same as `myproject`.\n",
    "\n",
    "Next, you go inside the new project directory:\n",
    "\n",
    "``` sh\n",
    "cd project_dir\n",
    "```\n",
    "\n",
    "And you’re ready to use the `scrapy` command to manage and control your project from there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a05beb3-9d7c-454e-b9b8-bbcf0df0a5ac",
   "metadata": {},
   "source": [
    "### Controlling projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864a1ee2-9024-4945-a01a-aa388e45a24c",
   "metadata": {},
   "source": [
    "You use the `scrapy` tool _from inside_ your projects to control and manage them.\n",
    "\n",
    "For example, to create a new spider:\n",
    "\n",
    "```\n",
    "scrapy genspider mydomain mydomain.com\n",
    "```\n",
    "\n",
    "Some Scrapy commands (like `crawl`) must be run from inside a Scrapy project. See the [commands reference](#Available-tool-commands) below for more information on which commands must be run from inside projects, and which not.\n",
    "\n",
    "Also keep in mind that some commands may have slightly different behaviours when running them from inside projects. For example, the `fetch` command will use spider-overridden behaviours (such as the `user_agent` attribute to override the user-agent) if the url being fetched is associated with some specific spider. This is intentional, as the `fetch` command is meant to be used to check how spiders are downloading pages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55e7475-de82-47d1-8944-63d700c1fa72",
   "metadata": {},
   "source": [
    "## Available tool commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75c6413-6b2a-40d6-98d8-7385c7346512",
   "metadata": {},
   "source": [
    "This section contains a list of the available built-in commands with a description and some usage examples. Remember, you can always get more info about each command by running:\n",
    "\n",
    "```sh\n",
    "scrapy <command> -h\n",
    "```\n",
    "\n",
    "And you can see all available commands with:\n",
    "\n",
    "```sh\n",
    "scrapy -h\n",
    "```\n",
    "\n",
    "There are two kinds of commands, \n",
    "- those that only work from inside a `Scrapy` project (**Project-specific commands**) and \n",
    "- those that also work without an active Scrapy project (**Global commands**), \n",
    "\n",
    "though they may behave slightly different when running from inside a project (as they would use the project overridden settings)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2465e2e4-2e8a-482a-b9ac-21225ab2a8c3",
   "metadata": {},
   "source": [
    "### Global commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce85567c-e09f-4a3b-a0a4-a69e9cf8904b",
   "metadata": {},
   "source": [
    "- `startproject`\n",
    "- `genspider`\n",
    "- `settings`\n",
    "- `runspider`\n",
    "- `shell`\n",
    "- `fetch`\n",
    "- `view`\n",
    "- `version`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1242a3-0a5c-4f4d-b89d-e77c163c76f5",
   "metadata": {},
   "source": [
    "#### `startproject`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5556c4-dd0f-4266-9b94-b91888db76f9",
   "metadata": {},
   "source": [
    "Syntax: `scrapy startproject <project_name> [project_dir]`\n",
    "\n",
    "Requires project: no\n",
    "\n",
    "Creates a new Scrapy project named `project_name`, under the `project_dir` directory. If `project_dir` wasn’t specified, `project_dir` will be the same as `project_name`.\n",
    "\n",
    "Usage example:\n",
    "```sh\n",
    "$ scrapy startproject myproject\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb24cc9-46a7-4f54-abf9-44bdf6ed5629",
   "metadata": {},
   "source": [
    "#### `genspider`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea80d55-9768-4625-83a8-d9c4f49efe3b",
   "metadata": {},
   "source": [
    "Syntax: `scrapy genspider [-t template] <name> <domain or URL>`\n",
    "\n",
    "Requires project: no\n",
    "\n",
    "New in version 2.6.0: The ability to pass a URL instead of a domain.\n",
    "\n",
    "Create a new spider in the current folder or in the current project’s spiders folder, if called from inside a project. The `<name>` parameter is set as the spider’s name, while `<domain or URL>` is used to generate the `allowed_domains` and `start_urls` spider’s attributes.\n",
    "\n",
    "Usage example:\n",
    "\n",
    "```sh\n",
    "$ scrapy genspider -l\n",
    "```\n",
    "```\n",
    "Available templates:\n",
    "  basic\n",
    "  crawl\n",
    "  csvfeed\n",
    "  xmlfeed\n",
    "```\n",
    "```sh\n",
    "$ scrapy genspider example example.com\n",
    "```\n",
    "```\n",
    "Created spider 'example' using template 'basic'\n",
    "```\n",
    "```sh\n",
    "$ scrapy genspider -t crawl scrapyorg scrapy.org\n",
    "```\n",
    "```\n",
    "Created spider 'scrapyorg' using template 'crawl'\n",
    "```\n",
    "\n",
    "This is just a convenience shortcut command for creating spiders based on pre-defined templates, but certainly not the only way to create spiders. \n",
    "\n",
    "> You can just create the spider source code files yourself, instead of using this command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68c17c5-9995-4470-97d8-d20102db32de",
   "metadata": {},
   "source": [
    "#### `fetch`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee76f657-e31d-418a-b954-8098a043f17a",
   "metadata": {},
   "source": [
    "Syntax: `scrapy fetch <url>`\n",
    "\n",
    "Requires project: no\n",
    "\n",
    "Downloads the given URL using the Scrapy downloader and writes the contents to standard output.\n",
    "\n",
    "The interesting thing about this command is that it fetches the page how the spider would download it. For example, if the spider has a `USER_AGENT` attribute which overrides the User Agent, it will use that one.\n",
    "\n",
    "So this command can be used to “see” how your spider would fetch a certain page.\n",
    "\n",
    "If used outside a project, no particular per-spider behaviour would be applied and it will just use the default Scrapy downloader settings.\n",
    "\n",
    "Supported options:\n",
    "- `--spider SPIDER`: bypass spider autodetection and force use of specific spider\n",
    "- `--headers`: print the response’s HTTP headers instead of the response’s body\n",
    "- `--no-redirect`: do not follow HTTP 3xx redirects (default is to follow them)\n",
    "\n",
    "Usage examples:\n",
    "\n",
    "```sh\n",
    "$ scrapy fetch --nolog http://www.example.com/some/page.html\n",
    "[ ... html content here ... ]\n",
    "\n",
    "$ scrapy fetch --nolog --headers http://www.example.com/\n",
    "{'Accept-Ranges': ['bytes'],\n",
    " 'Age': ['1263   '],\n",
    " 'Connection': ['close     '],\n",
    " 'Content-Length': ['596'],\n",
    " 'Content-Type': ['text/html; charset=UTF-8'],\n",
    " 'Date': ['Wed, 18 Aug 2010 23:59:46 GMT'],\n",
    " 'Etag': ['\"573c1-254-48c9c87349680\"'],\n",
    " 'Last-Modified': ['Fri, 30 Jul 2010 15:30:18 GMT'],\n",
    " 'Server': ['Apache/2.2.3 (CentOS)']}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1db59a4-1160-4dae-ba13-29399aa2701f",
   "metadata": {},
   "source": [
    "#### `shell`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3c60d7-2e40-4cf7-b7b1-71abbb6e788a",
   "metadata": {},
   "source": [
    "Syntax: `scrapy shell [url]`\n",
    "\n",
    "Requires project: no\n",
    "\n",
    "Starts the Scrapy shell for the given URL (if given) or empty if no URL is given. Also supports UNIX-style local file paths, either relative with `./` or `../` prefixes or absolute file paths. See [Scrapy shell](#2.6-Scrapy-shell) for more info.\n",
    "\n",
    "Supported options:\n",
    "- `--spider=SPIDER`: bypass spider autodetection and force use of specific spider\n",
    "- `-c code`: evaluate the code in the shell, print the result and exit\n",
    "- `--no-redirect`: do not follow HTTP 3xx redirects (default is to follow them); this only affects the URL you may pass as argument on the command line; once you are inside the shell, fetch(url) will still follow HTTP redirects by default.\n",
    "\n",
    "Usage example:\n",
    "\n",
    "```sh\n",
    "$ scrapy shell http://www.example.com/some/page.html\n",
    "```\n",
    "```\n",
    "[ ... scrapy shell starts ... ]\n",
    "```\n",
    "```sh\n",
    "$ scrapy shell --nolog http://www.example.com/ -c '(response.status, response.url)'\n",
    "```\n",
    "```\n",
    "(200, 'http://www.example.com/')\n",
    "```\n",
    "```sh\n",
    "# shell follows HTTP redirects by default\n",
    "$ scrapy shell --nolog http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F -c '(response.status, response.url)'\n",
    "```\n",
    "```\n",
    "(200, 'http://example.com/')\n",
    "```\n",
    "```sh\n",
    "# you can disable this with --no-redirect\n",
    "# (only for the URL passed as command line argument)\n",
    "$ scrapy shell --no-redirect --nolog http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F -c '(response.status, response.url)'\n",
    "```\n",
    "```\n",
    "(302, 'http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9e15f1-7f4f-4c7a-aaab-2c348b3ff58b",
   "metadata": {},
   "source": [
    "#### `view`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5319b6b-d66c-407b-b94b-a5fdf178e9fa",
   "metadata": {},
   "source": [
    "Syntax: `scrapy view <url>`\n",
    "\n",
    "Requires project: no\n",
    "\n",
    "Opens the given URL in a browser, as your Scrapy spider would “see” it. Sometimes spiders see pages differently from regular users, so this can be used to check what the spider “sees” and confirm it’s what you expect.\n",
    "\n",
    "Supported options:\n",
    "- `--spider SPIDER`: bypass spider autodetection and force use of specific spider\n",
    "- `--no-redirect`: do not follow HTTP 3xx redirects (default is to follow them)\n",
    "\n",
    "Usage example:\n",
    "```sh\n",
    "$ scrapy view http://www.example.com/some/page.html\n",
    "[ ... browser starts ... ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3455b1d4-9dfe-4ae4-b064-04bdfa0310bd",
   "metadata": {},
   "source": [
    "#### `settings`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d11ea3e-69f0-4ec0-b129-2662e703a239",
   "metadata": {},
   "source": [
    "Syntax: `scrapy settings [options]`\n",
    "\n",
    "Requires project: no\n",
    "\n",
    "Get the value of a Scrapy setting.\n",
    "\n",
    "If used inside a project it’ll show the project setting value, otherwise it’ll show the default Scrapy value for that setting.\n",
    "\n",
    "Example usage:\n",
    "```sh\n",
    "$ scrapy settings --get BOT_NAME\n",
    "scrapybot\n",
    "$ scrapy settings --get DOWNLOAD_DELAY\n",
    "0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d7a7e9-4f69-4246-9f5a-870c36bd8bb7",
   "metadata": {},
   "source": [
    "#### `runspider`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69893570-95cc-4db5-9590-e75f44c9b285",
   "metadata": {},
   "source": [
    "Syntax: `scrapy runspider <spider_file.py>`\n",
    "\n",
    "Requires project: no\n",
    "\n",
    "Run a spider self-contained in a Python file, without having to create a project.\n",
    "\n",
    "Example usage:\n",
    "```sh\n",
    "$ scrapy runspider myspider.py\n",
    "[ ... spider starts crawling ... ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad821fe-a7a6-414e-bd73-1360c6a7ee1a",
   "metadata": {},
   "source": [
    "#### `version`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cccbee-4ff7-484d-8476-946fbb25f811",
   "metadata": {},
   "source": [
    "Syntax: `scrapy version [-v]`\n",
    "\n",
    "Requires project: no\n",
    "\n",
    "Prints the Scrapy version. If used with `-v` it also prints Python, Twisted and Platform info, which is useful for bug reports."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83cbd90-a31e-4cef-8844-78df1a206d20",
   "metadata": {},
   "source": [
    "#### `bench`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c804180-568c-4db6-8512-1a7e41a1d7a2",
   "metadata": {},
   "source": [
    "Syntax: `scrapy bench`\n",
    "\n",
    "Requires project: no\n",
    "\n",
    "Run a quick benchmark test. [Benchmarking](#4.12-Benchmarking)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187bbb2b-c1e0-4d18-87a2-8b48c2c060cc",
   "metadata": {},
   "source": [
    "### Project-only commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3815f3-e657-42fb-8ea8-4a37491682d3",
   "metadata": {},
   "source": [
    "- `crawl`\n",
    "- `check`\n",
    "- `list`\n",
    "- `edit`\n",
    "- `parse`\n",
    "- `bench`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e1e36d-3ac3-416a-9d2b-a7d674c3ec92",
   "metadata": {},
   "source": [
    "#### `crawl`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f72c64f-4d54-4cd2-80c4-a9c4f0b7c90a",
   "metadata": {},
   "source": [
    "Syntax: `scrapy crawl <spider>`\n",
    "\n",
    "Requires project: yes\n",
    "\n",
    "Start crawling using a spider.\n",
    "\n",
    "Supported options:\n",
    "- `-h`, `--help`: show a help message and exit\n",
    "- `-a NAME=VALUE`: set a spider argument (may be repeated)\n",
    "- `--output FILE` or `-o FILE`: append scraped items to the end of `FILE` (use - for stdout), to define format set a colon at the end of the output URI (i.e. `-o FILE:FORMAT`)\n",
    "- `--overwrite-output FILE` or `-O FILE`: dump scraped items into `FILE`, overwriting any existing file, to define format set a colon at the end of the output URI (i.e. `-O FILE:FORMAT`)\n",
    "- `--output-format FORMAT` or `-t FORMAT`: deprecated way to define format to use for dumping items, does not work in combination with `-O`\n",
    "\n",
    "Usage examples:\n",
    "```sh\n",
    "$ scrapy crawl myspider\n",
    "```\n",
    "```\n",
    "[ ... myspider starts crawling ... ]\n",
    "```\n",
    "```sh\n",
    "$ scrapy crawl -o myfile:csv myspider\n",
    "```\n",
    "```\n",
    "[ ... myspider starts crawling and appends the result to the file myfile in csv format ... ]\n",
    "```\n",
    "```sh\n",
    "$ scrapy crawl -O myfile:json myspider\n",
    "```\n",
    "```\n",
    "[ ... myspider starts crawling and saves the result in myfile in json format overwriting the original content... ]\n",
    "```\n",
    "```sh\n",
    "$ scrapy crawl -o myfile -t csv myspider\n",
    "```\n",
    "```\n",
    "[ ... myspider starts crawling and appends the result to the file myfile in csv format ... ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af19fab-8920-4671-8a4d-603ba7540dc7",
   "metadata": {},
   "source": [
    "#### check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6813288-ea3d-40a7-bbc3-00b5b7c61002",
   "metadata": {},
   "source": [
    "Syntax: `scrapy check [options] <spider>`\n",
    "\n",
    "Requires project: yes\n",
    "\n",
    "Check spider contracts.\n",
    "\n",
    "Options:\n",
    "\n",
    "```\n",
    "  -h, --help            show this help message and exit\n",
    "  -l, --list            only list contracts, without checking them\n",
    "  -v, --verbose         print contract tests for all spiders\n",
    "\n",
    "Global Options\n",
    "--------------\n",
    "  --logfile FILE        log file. if omitted stderr will be used\n",
    "  -L LEVEL, --loglevel LEVEL\n",
    "                        log level (default: DEBUG)\n",
    "  --nolog               disable logging completely\n",
    "  --profile FILE        write python cProfile stats to FILE\n",
    "  --pidfile FILE        write process ID to FILE\n",
    "  -s NAME=VALUE, --set NAME=VALUE\n",
    "                        set/override setting (may be repeated)\n",
    "  --pdb                 enable pdb on failure\n",
    "```\n",
    "\n",
    "Usage examples:\n",
    "\n",
    "```sh\n",
    "$ scrapy check -l\n",
    "first_spider\n",
    "  * parse\n",
    "  * parse_item\n",
    "second_spider\n",
    "  * parse\n",
    "  * parse_item\n",
    "\n",
    "$ scrapy check\n",
    "[FAILED] first_spider:parse_item\n",
    ">>> 'RetailPricex' field is missing\n",
    "\n",
    "[FAILED] first_spider:parse\n",
    ">>> Returned 92 requests, expected 0..4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c34bdad-5244-4dcf-ba70-ca37ff894e6c",
   "metadata": {},
   "source": [
    "##### Spider conracts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f80a975-dada-4ba0-9d33-888de08573d2",
   "metadata": {},
   "source": [
    "_ChatGPT:_  \n",
    "In Scrapy, `\"spider contracts\"` refer to a feature designed to enforce certain rules or constraints on the output of your spiders (the crawlers you create using Scrapy). These contracts are defined using the `scrapy.contracts` module and are meant to ensure that your spiders are behaving as expected and producing the desired output.\n",
    "\n",
    "Spider contracts can be particularly useful for ensuring data quality and consistency, especially when you're dealing with large-scale web scraping projects where data structure or content might vary across different pages or domains.\n",
    "\n",
    "Here's a brief overview of how spider contracts work in Scrapy:\n",
    "\n",
    "- **Defining Contracts:** You define contracts using Python classes that subclass `scrapy.contracts.Contract`. Within these classes, you define methods to verify specific aspects of the spider output, such as the presence of certain fields or the structure of items.\n",
    "\n",
    "- **Implementing Verification Logic:** Within the contract class methods, you write the logic to verify whether the spider output meets the defined criteria. This logic typically involves inspecting the scraped data and raising exceptions if the criteria are not met.\n",
    "\n",
    "- **Enforcing Contracts:** Once you've defined your contracts, you can enable them for specific spiders by adding the contracts attribute to your spider classes and specifying which contracts to apply.\n",
    "\n",
    "- **Running Contract Checks:** When you run your spiders, Scrapy will automatically apply the specified contracts to the spider output and perform the verification checks. If any of the checks fail, Scrapy will raise an exception, indicating that the spider output does not conform to the defined contracts.\n",
    "\n",
    "By using spider contracts, you can ensure that your spiders are producing reliable and consistent output, which can be essential for downstream processing and analysis of the scraped data.\n",
    "\n",
    "Here's a simple example of a spider contract class that verifies the presence of certain fields in the spider output:\n",
    "\n",
    "```python\n",
    "from scrapy.contracts import Contract\n",
    "\n",
    "\n",
    "class RequiredFieldsContract(Contract):\n",
    "    \"\"\"Contract to verify the presence of required fields in spider output\"\"\"\n",
    "\n",
    "    name = 'required_fields'\n",
    "\n",
    "    def verify(self, output):\n",
    "        required_fields = ['title', 'url', 'content']\n",
    "        for item in output:\n",
    "            for field in required_fields:\n",
    "                if field not in item:\n",
    "                    raise ContractFail(f\"Required field '{field}' missing in item: {item}\")\n",
    "```\n",
    "\n",
    "And here's how you would enable this contract for a specific spider:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = 'myspider'\n",
    "    start_urls = ['http://example.com']\n",
    "\n",
    "    contracts = [RequiredFieldsContract]\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Your parsing logic here\n",
    "        pass\n",
    "```\n",
    "\n",
    "With this setup, Scrapy will verify that each item produced by MySpider contains the required fields specified in the `RequiredFieldsContract`. If any item fails this verification, Scrapy will raise a `ContractFail` exception."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b61150-9b0b-407d-b9d8-94fd1bda2456",
   "metadata": {},
   "source": [
    "#### `list`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc7c1f6-1f6a-4f00-b8d2-e1c04f180554",
   "metadata": {},
   "source": [
    "Syntax: `scrapy list`\n",
    "\n",
    "Requires project: yes\n",
    "\n",
    "List all available spiders in the current project. The output is one spider per line.\n",
    "\n",
    "Usage example:\n",
    "\n",
    "```sh\n",
    "$ scrapy list\n",
    "spider1\n",
    "spider2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddac57f-c471-4df5-9099-e6c17c1d074c",
   "metadata": {},
   "source": [
    "#### `edit`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3a49f7-c3b0-4be7-9cc2-f2fae06ad004",
   "metadata": {},
   "source": [
    "Syntax: `scrapy edit <spider>`\n",
    "\n",
    "Requires project: yes\n",
    "\n",
    "Edit the given spider using the editor defined in the `EDITOR` environment variable or (if unset) the `EDITOR` setting.\n",
    "\n",
    "This command is provided only as a convenience shortcut for the most common case, the developer is of course free to choose any tool or IDE to write and debug spiders.\n",
    "\n",
    "Usage example:\n",
    "```sh\n",
    "$ scrapy edit spider1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafd92c4-d913-4326-b241-eb4b7d1e46df",
   "metadata": {},
   "source": [
    "#### `parse`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0685754-6672-46e3-98bb-9768aaec1fbe",
   "metadata": {},
   "source": [
    "Syntax: `scrapy parse <url> [options]`\n",
    "\n",
    "Requires project: yes\n",
    "\n",
    "Fetches the given URL and parses it with the spider that handles it, using the method passed with the `--callback` option, or parse if not given.\n",
    "\n",
    "Supported options:\n",
    "- `--spider=SPIDER`: bypass spider autodetection and force use of specific spider\n",
    "- `--a NAME=VALUE`: set spider argument (may be repeated)\n",
    "- `--callback` or `-c`: spider method to use as callback for parsing the response\n",
    "- `--meta` or `-m`: additional request meta that will be passed to the callback request. This must be a valid json string. Example: `–meta=’{“foo” : “bar”}`’\n",
    "- `--cbkwargs`: additional keyword arguments that will be passed to the callback. This must be a valid json string. Example: `–cbkwargs=’{“foo” : “bar”}`’\n",
    "- `--pipelines`: process items through pipelines\n",
    "- `--rules` or `-r`: use `CrawlSpider` rules to discover the callback (i.e. spider method) to use for parsing the response\n",
    "- `--noitems`: don’t show scraped items\n",
    "- `--nolinks`: don’t show extracted links\n",
    "- `--nocolour`: avoid using pygments to colorize the output\n",
    "- `--depth` or `-d`: depth level for which the requests should be followed recursively (default: 1)\n",
    "- `--verbose` or `-v`: display information for each depth level\n",
    "- `--output` or `-o`: dump scraped items to a file\n",
    "\n",
    "New in version 2.3.\n",
    "\n",
    "Usage example:\n",
    "```sh\n",
    "$ scrapy parse http://www.example.com/ -c parse_item\n",
    "[ ... scrapy log lines crawling example.com spider ... ]\n",
    "\n",
    ">>> STATUS DEPTH LEVEL 1 <<<\n",
    "# Scraped Items  ------------------------------------------------------------\n",
    "[{'name': 'Example item',\n",
    " 'category': 'Furniture',\n",
    " 'length': '12 cm'}]\n",
    "\n",
    "# Requests  -----------------------------------------------------------------\n",
    "[]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39354f2-ed1f-4197-8cc1-2110a94b17da",
   "metadata": {},
   "source": [
    "## Custom project commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de89e9d9-4476-4739-bc20-a9d2fa44f66b",
   "metadata": {},
   "source": [
    "You can also add your custom project commands by using the `COMMANDS_MODULE` setting. See the Scrapy commands in [scrapy/commands](https://github.com/scrapy/scrapy/tree/master/scrapy/commands) for examples on how to implement your commands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc05d3e8-effa-46d8-aa1a-e70a56b092a2",
   "metadata": {},
   "source": [
    "### `COMMANDS_MODULE`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8376e57c-a9fb-4455-9aeb-92a2e8f0bd80",
   "metadata": {},
   "source": [
    "Default: `''` (empty string)\n",
    "\n",
    "A module to use for looking up custom Scrapy commands. This is used to add custom commands for your Scrapy project.\n",
    "\n",
    "Example:\n",
    "```sh\n",
    "COMMANDS_MODULE = \"mybot.commands\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc21c0e-28e7-486e-97b0-1ffdd34fa5e7",
   "metadata": {},
   "source": [
    "### Register commands via `setup.py` entry points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4e606d-945a-4fed-8f16-87198095e466",
   "metadata": {},
   "source": [
    "You can also add Scrapy commands from an external library by adding a `scrapy.commands` section in the entry points of the library `setup.py` file.\n",
    "\n",
    "The following example adds `my_command` command:\n",
    "\n",
    "```python\n",
    "from setuptools import setup, find_packages\n",
    "\n",
    "setup(\n",
    "    name=\"scrapy-mymodule\",\n",
    "    entry_points={\n",
    "        \"scrapy.commands\": [\n",
    "            \"my_command=my_scrapy_module.commands:MyCommand\",\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eb0cc9-17fc-4dce-bd26-e5e0521a3e29",
   "metadata": {},
   "source": [
    "# 2.2 Spiders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23da3b48-32dc-4d2d-a293-97a6c9ed9a6b",
   "metadata": {},
   "source": [
    "**Spiders** are classes which define how a certain site (or a group of sites) will be scraped, including \n",
    "- how to perform the crawl (i.e. follow links) and \n",
    "- how to extract structured data from their pages (i.e. scraping items). \n",
    "\n",
    "In other words, Spiders are the place where you define the custom behaviour for crawling and parsing pages for a particular site (or, in some cases, a group of sites).\n",
    "\n",
    "For spiders, the scraping cycle goes through something like this:\n",
    "\n",
    "1. You start by generating the initial **Requests** to crawl the first URLs, and specify a **callback function** to be called with the `response` downloaded from those requests.\n",
    "\n",
    "    The first requests to perform are obtained by calling the `start_requests()` method which (by default) generates `Request` for the URLs specified in the `start_urls` and the `parse` method as callback function for the Requests.\n",
    "\n",
    "1. In the callback function, you parse the **response** (web page) and return \n",
    "- [item objects](#2.4-Items), \n",
    "- `Request` objects, or \n",
    "- an iterable of these objects. \n",
    "\n",
    "    Those Requests will also contain a callback (maybe the same) and will then be downloaded by Scrapy and then their response handled by the specified callback.\n",
    "\n",
    "3. In callback functions, you parse the page contents, typically using [Selectors](#2.3-Selectors) (but you can also use `BeautifulSoup`, `lxml` or whatever mechanism you prefer) and generate items with the parsed data.\n",
    "\n",
    "4. Finally, the items returned from the spider will be typically \n",
    "- persisted to a database (in some [Item Pipeline](#2.7-Item-Pipeline)) or \n",
    "- written to a file using [Feed exports](#2.8-Feed-exports).\n",
    "\n",
    "Even though this cycle applies (more or less) to any kind of spider, there are different kinds of default spiders bundled into Scrapy for different purposes. We will talk about those types here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc61fdf-4c86-470c-a0a1-1522cab0a38f",
   "metadata": {},
   "source": [
    "## `scrapy.Spider`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff575310-89e1-4257-addb-f5adf0784bdf",
   "metadata": {},
   "source": [
    "```python\n",
    "class scrapy.spiders.Spider\n",
    "class scrapy.Spider\n",
    "```\n",
    "\n",
    "This is the simplest spider, and the one from which every other spider must inherit (including spiders that come bundled with Scrapy, as well as spiders that you write yourself). It doesn’t provide any special functionality. It just provides a default `start_requests()` implementation which sends requests from the `start_urls` spider attribute and calls the spider’s method `parse` for each of the resulting responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4745f42a-c903-4e03-85e8-ba1d6c3bb485",
   "metadata": {},
   "source": [
    "### `name`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92720f61-00f0-4f41-a7fa-0a5920f436bf",
   "metadata": {},
   "source": [
    "A string which defines the name for this spider. \n",
    "\n",
    "The spider `name` is how the spider is located (and instantiated) by Scrapy, so **it must be unique**. However, nothing prevents you from instantiating more than one instance of the same spider. This is the most important spider attribute and it’s required.\n",
    "\n",
    "If the spider scrapes a single domain, a common practice is to name the spider after the domain, with or without the TLD. So, for example, a spider that crawls `mywebsite.com` would often be called `mywebsite`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa2ce8d-9341-464e-a9e8-8ccdede9469a",
   "metadata": {},
   "source": [
    "### `allowed_domains`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa92a733-1013-41f4-bf8c-5ddbdcde39c8",
   "metadata": {},
   "source": [
    "An optional list of strings containing domains that this spider is allowed to crawl. Requests for URLs not belonging to the domain names specified in this list (or their subdomains) won’t be followed if `OffsiteMiddleware` is enabled.\n",
    "\n",
    "Let’s say your target url is `https://www.example.com/1.html`, then add '`example.com`' to the list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96206b29-ff4d-42fd-b294-82cded02b6c9",
   "metadata": {},
   "source": [
    "### `start_urls`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7fc88b-0ab9-46b1-b443-81edf5fc1915",
   "metadata": {},
   "source": [
    "A list of URLs where the spider will begin to crawl from, when no particular URLs are specified. So, the first pages downloaded will be those listed here. The subsequent `Request` will be generated successively from data contained in the start URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ab07e2-1f6b-46bc-ab55-029dddd16834",
   "metadata": {},
   "source": [
    "### `custom_settings`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21e5c20-ed27-4bf7-a705-dc005aba57b5",
   "metadata": {},
   "source": [
    "A dictionary of settings that will be overridden from the project wide configuration when running this spider. It must be defined as a class attribute since the settings are updated before instantiation.\n",
    "\n",
    "For a list of available built-in settings see: [Built-in settings reference](#Built-in-settings-reference)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e79ecce-f740-4b21-bc50-9b9dfb16d004",
   "metadata": {},
   "source": [
    "### `crawler`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15b4cf3-296d-4810-b3a8-a4c4e9d2d9d1",
   "metadata": {},
   "source": [
    "This attribute is set by the `from_crawler()` class method after initializing the class, and links to the `Crawler` object to which this spider instance is bound.\n",
    "\n",
    "Crawlers encapsulate a lot of components in the project for their single entry access (such as extensions, middlewares, signals managers, etc). See [Crawler API](#Crawler-API) to know more about them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410ee02d-2cfb-488d-9713-30771f24b59d",
   "metadata": {},
   "source": [
    "### `settings`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7754e708-f842-45c0-8a4c-63b580516507",
   "metadata": {},
   "source": [
    "Configuration for running this spider. This is a `Settings` instance, see the [Settings](#2.11-Settings) topic for a detailed introduction on this subject."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d907670e-9820-4bd9-8bbb-f317ad16b07f",
   "metadata": {},
   "source": [
    "### `logger`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0212c630-32c0-40e2-96b9-922e4f3026f9",
   "metadata": {},
   "source": [
    "Python logger created with the Spider’s name. You can use it to send log messages through it as described on [Logging from Spiders](#Logging-from-Spiders)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91ffd3e-9476-4491-9ccc-b4978b497cd4",
   "metadata": {},
   "source": [
    "### `state`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1264b83c-fc26-489f-af6d-5dadcace5162",
   "metadata": {},
   "source": [
    "A `dict` you can use to persist some spider state between batches. See [Keeping persistent state between batches](#Keeping-persistent-state-between-batches) to know more about it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efab0609-e32f-4618-8720-bf80a27a8e0e",
   "metadata": {},
   "source": [
    "### `from_crawler(crawler, *args, **kwargs)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdc3f04-d689-459c-8119-1ad2b4be92fe",
   "metadata": {},
   "source": [
    "This is the class method used by Scrapy to create your spiders.\n",
    "\n",
    "You probably won’t need to override this directly because the default implementation acts as a proxy to the `__init__()` method, calling it with the given arguments `args` and named arguments `kwargs`.\n",
    "\n",
    "Nonetheless, this method sets the crawler and settings attributes in the new instance so they can be accessed later inside the spider’s code.\n",
    "\n",
    "_Changed in version 2.11:_ The settings in `crawler.settings` can now be modified in this method, which is handy if you want to modify them based on arguments. As a consequence, these settings aren’t the final values as they can be modified later by e.g. [add-ons](#5.2-Add-ons). For the same reason, most of the `Crawler` attributes aren’t initialized at this point.\n",
    "\n",
    "The final settings and the initialized `Crawler` attributes are available in the `start_requests()` method, handlers of the `engine_started` signal and later.\n",
    "\n",
    "**Parameters**\n",
    "- `crawler` (Crawler instance) – crawler to which the spider will be bound\n",
    "- `args` (list) – arguments passed to the `__init__()` method\n",
    "- `kwargs` (dict) – keyword arguments passed to the `__init__()` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d480951-a736-4d04-856c-048b646378f8",
   "metadata": {},
   "source": [
    "### `classmethod update_settings(settings)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a8be40-9650-431d-8140-2911138a174f",
   "metadata": {},
   "source": [
    "The `update_settings()` method is used to modify the spider’s settings and is called during initialization of a spider instance.\n",
    "\n",
    "It takes a `Settings` object as a parameter and can add or update the spider’s configuration values. This method is a class method, meaning that it is called on the Spider class and allows all instances of the spider to share the same configuration.\n",
    "\n",
    "While per-spider settings can be set in `custom_settings`, using `update_settings()` allows you to dynamically \n",
    "- add, \n",
    "- remove or \n",
    "- change settings \n",
    "\n",
    "based on other \n",
    "- settings, \n",
    "- spider attributes or \n",
    "- other factors \n",
    "\n",
    "and use setting priorities other than 'spider'. Also, it’s easy to extend `update_settings()` in a subclass by overriding it, while doing the same with `custom_settings` can be hard.\n",
    "\n",
    "For example, suppose a spider needs to modify `FEEDS`:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = \"myspider\"\n",
    "    custom_feed = {\n",
    "        \"/home/user/documents/items.json\": {\n",
    "            \"format\": \"json\",\n",
    "            \"indent\": 4,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def update_settings(cls, settings):\n",
    "        super().update_settings(settings)\n",
    "        settings.setdefault(\"FEEDS\", {}).update(cls.custom_feed)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aea074-29d9-458e-90eb-a41ecccca030",
   "metadata": {},
   "source": [
    "### `parse(response)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca3b02b-a092-4354-b43b-bc6e598bdca6",
   "metadata": {},
   "source": [
    "This is the default callback used by Scrapy to process downloaded `response`s, when their requests don’t specify a callback.\n",
    "\n",
    "The `parse` method is in charge of processing the `response` and returning scraped data and/or more URLs to follow. Other `Request`s callbacks have the same requirements as the Spider class.\n",
    "\n",
    "This method, as well as any other `Request` callback, must return \n",
    "- a `Request` object, \n",
    "- an [`item` object](#2.4-Items), \n",
    "- an iterable of `Request` objects and/or \n",
    "- [item objects](#2.4-Items), or \n",
    "- `None`.\n",
    "\n",
    "**Parameters**\n",
    "- `response` (Response) – the response to parse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c626767c-3fba-4f9c-b42f-d89327d382da",
   "metadata": {},
   "source": [
    "### `log(message[, level, component])`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bf1671-20a7-4c54-acd0-cb1d8cb5177b",
   "metadata": {},
   "source": [
    "Wrapper that sends a log message through the Spider’s logger, kept for backward compatibility. For more information see [Logging from Spiders](#Logging-from-Spiders)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf3c034-f640-4c9b-a663-bab745d377a8",
   "metadata": {},
   "source": [
    "### `closed(reason)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ad622a-1ae5-4e3d-80b9-283eadd7079c",
   "metadata": {},
   "source": [
    "Called when the spider closes. This method provides a shortcut to `signals.connect()` for the `spider_closed` signal.\n",
    "\n",
    "Let’s see an example:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = \"example.com\"\n",
    "    allowed_domains = [\"example.com\"]\n",
    "    start_urls = [\n",
    "        \"http://www.example.com/1.html\",\n",
    "        \"http://www.example.com/2.html\",\n",
    "        \"http://www.example.com/3.html\",\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        self.logger.info(\"A response from %s just arrived!\", response.url)\n",
    "```\n",
    "\n",
    "Return multiple Requests and items from a single callback:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = \"example.com\"\n",
    "    allowed_domains = [\"example.com\"]\n",
    "    start_urls = [\n",
    "        \"http://www.example.com/1.html\",\n",
    "        \"http://www.example.com/2.html\",\n",
    "        \"http://www.example.com/3.html\",\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for h3 in response.xpath(\"//h3\").getall():\n",
    "            yield {\"title\": h3}\n",
    "\n",
    "        for href in response.xpath(\"//a/@href\").getall():\n",
    "            yield scrapy.Request(response.urljoin(href), self.parse)\n",
    "```\n",
    "\n",
    "Instead of `start_urls` you can use `start_requests()` directly; to give data more structure you can use `Item` objects:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "from myproject.items import MyItem\n",
    "\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = \"example.com\"\n",
    "    allowed_domains = [\"example.com\"]\n",
    "\n",
    "    def start_requests(self):\n",
    "        yield scrapy.Request(\"http://www.example.com/1.html\", self.parse)\n",
    "        yield scrapy.Request(\"http://www.example.com/2.html\", self.parse)\n",
    "        yield scrapy.Request(\"http://www.example.com/3.html\", self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        for h3 in response.xpath(\"//h3\").getall():\n",
    "            yield MyItem(title=h3)\n",
    "\n",
    "        for href in response.xpath(\"//a/@href\").getall():\n",
    "            yield scrapy.Request(response.urljoin(href), self.parse)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7491c1a-a1ff-45c5-8a35-67d9b42768f7",
   "metadata": {},
   "source": [
    "## Spider arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df023785-1f13-428f-9aef-933b0321fac0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f7c0409-6f64-46ea-ad99-3f6fc56ec0d8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfc78896-2348-44cd-96a7-d3c814727382",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ba05a7a-7db4-4a02-a329-f3f49ad5cdb0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d977100-d354-4111-b0ba-3c47b01dc83b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e19e123-9f7c-410a-933a-5b801a56ff97",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c79cc364-bab4-4ec7-a406-9fb39123b321",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fdd19ae-e6c6-44e8-8635-114b42a6c59d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6e8b05b-8ef2-4551-b3af-525802c5efd6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a7c41ee-29d6-41dc-b4f6-e11ecc64c8c7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "205b828a-f19c-4691-8c62-fc677a0810c9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c846f7c-3076-4db2-a87d-e424a6cdbfce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2189794a-fbbd-4ded-ad53-6ed1013cb4ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49b0b42d-6347-4a43-aab8-20bb01772048",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18ceefc5-e0af-4ec1-84d5-9584c00f29a2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d6c292e-3009-44bb-90bb-0b5dc96d3a7c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92f7690b-00ad-442f-a3e8-15f6a30b6e77",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79114932-10e2-43ed-940b-caa414176c50",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "185810a8-9745-4192-a975-d2a2ca48c395",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37b4f647-24be-4523-b9bd-23c3c42338ea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4c3f633-4055-4804-ae79-f8a9444b6eef",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83170bb3-b1b3-4fa7-8d0e-fcd84eab63a7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7a60bd0-ad5d-46c6-944e-14d7695864ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f6dcf4a-f99e-49f2-a3ac-049295411656",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe3e123c-2d5e-4d8e-826c-eb7f443738ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a54236f-61d7-4c88-bd69-acb69d1ad4e7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34a2c405-8dc2-45ee-abe1-23ea61d59eb4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c83020f-fc3e-4427-a177-b2df7142013d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7031e3ad-d653-46c9-a687-ac0aad49f74c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6441ac21-65bf-4e7f-9485-bc165e86f97c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c1951ba-9574-437c-9e7c-e0bcd5c1fd0f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27348b79-4b71-4335-bd11-7c53540bf7c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31ee05b0-0c8f-4e6f-8976-49cd2c634a5e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2984623-9c75-4959-a1a1-d178a46e8613",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fc17133-e134-4f4d-b1bc-c8fa3a1cfef1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bf12c77-842b-44aa-800c-18883be943c9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f652e9d9-a6f6-4650-9dc4-c571f587cf6b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc836c1b-9bee-4650-b047-0d18f712b678",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2176a897-1c04-490b-841e-ecebc15eb3cb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ecaa713-653c-4cf7-a6c3-969752d7b1cb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44db82ca-3722-4043-a68a-88d4fa9d30b4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52231378-329c-43a4-8701-fa6137937fde",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60c13c34-91e1-4cd5-a1e9-8ae0aaaa5a54",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e5dc560-4c6d-4e4b-8ebc-1fc575c34659",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "663f94ff-d95c-4053-a8e9-79e867447bbc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bed6e9b-829a-450b-8a36-4aa84906c352",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a23d0878-9763-4b9e-a877-bfdf3bc27d37",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fab8a2b6-fa94-4b24-9c15-f79c06af98bc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42910fa5-47c9-472a-8e0f-dc10a41e4fbd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba37ea4d-a6d0-4698-a277-eeb5c1b012f7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7def67f-f742-431b-ac65-b3ea702e6954",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2bbe11d-9beb-4388-9680-a7c09ef0b2df",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "370fbc1d-37f7-4bf1-ae2b-a484d036a935",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "756c4442-aae3-4181-9445-92355a1282ce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "963fb61a-9fce-4afe-ab7c-c53fb634b853",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2174d96-3070-4a33-a0ea-20e754bf193f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e843363b-eb9c-4253-a47d-b7ad41d2e1ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f22b85b9-2501-4017-b488-d08eb25743f5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74c35179-0fdd-4341-a0a0-0ed82abfd706",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ef4ec3e-18d5-47d8-aea9-3d0610a46435",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db8cc4a6-b05f-44d4-8177-2e67c1aaa102",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99390406-04a2-40a8-90a7-9a3b4e1cd11e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffff4872-52a2-4860-bafd-a062478f3ffa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74a97104-6490-41c9-a0fe-fb8703ef9f71",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0df0a8e5-e6dd-4b6f-9b00-3e4ab13669e9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "077ba53a-e7fb-45b3-97e2-4beae36d7f9e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb848ec9-39d0-42ac-91c4-7b84f37bf252",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3181b307-2f04-496d-a677-c3b96eb5fb60",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eacc428c-31c2-4103-9c17-a04a6a0f1685",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64bd4f4e-438e-47dd-b64c-96e6d94bd196",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "813c96d3-162f-49e5-aecf-38ddd9103128",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8634c1aa-a276-44bd-a6c9-3f40363f9cd0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b6ce8f1-50a8-45e3-8212-0bbc585aaaf2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf199003-b181-4e04-84c3-7e3d7e982f78",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b63e285-5871-4b4c-88b0-839ef9e656bd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c84a1e2e-f440-4a2c-a4e6-0f5d8736ba16",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7bb0f0ba-8443-4793-99e3-0c0d63ac9b5d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aef02e1f-3dc3-41c2-803a-9932d82ee2f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3dbea2aa-6ae9-4ad6-bbe1-5eb0377842a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ef75585-9444-45a2-981b-ea6eb0ac1783",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ba3cefa-e189-4289-8ad2-b0f19bc04755",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f39c2953-e301-4263-9ff4-3853d1d5792d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8e3e31c-431b-4b98-86e2-098d40f3d688",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "700c5ae8-ffb1-4ad0-8291-2ea0c4121364",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3af5fbe0-ca65-4916-911e-c37447109de7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fccd541b-c0a2-42f1-ac25-706608d546d2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecb79f98-fb05-4d0e-9d2d-a6d99bb81814",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5207d4c0-f716-4c41-a63b-c00971935d59",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cb82df8-fe35-4c9b-b98d-4fb12873374e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1877b67a-6b89-4b79-b328-08c48b022de8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8256ddb6-dac5-4108-9894-181ae50d42c5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f3f2ae5-98e6-40dc-b601-e63dd6113382",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e84ac4e9-7f5f-4b33-b604-49aa3d621e28",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "472efb7b-8cbd-463e-bd08-ec33bee6eb1c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7cbeb0e-acf7-4910-bd45-4cc032f7ebd7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc1d34d0-b7e1-4574-828a-d223509a7bb2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a24a664-e8e1-4340-baaa-7d69793bf577",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6718497-b1df-4418-b908-2a19a454bd03",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d672216-6f4c-4d33-8f9d-54e982df129c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09585f29-6d09-4a7b-a1cb-950cb6cb8b40",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b21878e8-7559-4f86-bce2-1a52e07842c2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc9291a8-65ea-4f7b-a65e-5825ebdec945",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a80fa174-b7cc-4239-b555-798a2e16bd3c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd1689b0-2dac-4e39-a78b-2102a852513b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40edd53a-d1d9-4116-92c0-8ba9f3bc841f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ecc5dc1-43e2-4e2d-8740-7160a8163323",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58ac8e3b-9988-432f-ad7f-0977036b9d8e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "837750af-85a2-4ffa-a2c6-c62b1d1b64d7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6015d606-ce34-4e61-8356-3510e7c7d9b6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8640ef78-4b36-4cd2-908d-fcdd2d34662e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3a80c95-bd96-456e-9b43-03fa87873628",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "467cea9d-8aa7-4559-ab74-54bc6d00579f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddc93cf2-696d-4539-89a6-caea293d452c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07cc3a4c-1518-4065-adff-285f8d03c39d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd47149e-ac49-48c9-992b-56cbc1d9272f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "703e4ba1-651e-4f00-99a2-149e7d6f5cb8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2082f614-df62-49b2-9cc2-4579d7e53bf5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9285a32-3415-40b9-8bcc-d5cad5df0106",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8d52766-f930-4c94-8f76-9ff95d79316f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80bf1236-d949-4e58-aa67-1e64679d4d1f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11f5bb3b-e413-4c5b-96b4-6a3e358fe209",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "443d4275-6b2d-478f-a46f-4bb058e4d02a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "632916b5-e088-46d5-90f2-256683b10555",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f2e2bc8-6065-407f-84a3-737e485eec91",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72102130-3c38-498f-b557-1f9ddd237e4f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c396704-0a72-458d-a053-d250c1789065",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "282fedd4-15f6-4dd6-8981-219ddd7c7699",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "251fc71a-48e6-40a2-9e85-6919f3f8383f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f9884e0-bf7f-4b84-84e9-d7b770c4bcaa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1481a87f-402e-4a60-ab24-4c102fe73124",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d25b01c-5051-4075-8705-4fca893f8aa5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64a13653-8f21-4b50-8525-dfde3428d151",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8dea94e1-ccdf-4ddc-bca5-07ce0eef88aa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "822881be-d4be-4f2a-88b2-b108ac30c3e6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90028eae-181d-4549-913b-dec120bd491d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cc3c57a-887b-422c-bb55-8efc57ebe40f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92f23c25-862a-4414-94a1-5d2f37bdadcf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b49ddc06-1291-4361-94b4-fc5a8c0e7664",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1976517-6bcd-4e8c-ae9a-1eab4a2e944a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "588f33f1-5628-47bb-91f8-3a3193cc085b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac71d957-d161-4881-ac2e-ea9b06ab0b11",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb933203-edda-48ca-8ca5-7873c6f5c247",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf53ce88-8b56-4fba-9cfa-78aad57b9775",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "130fc6cf-34b2-4c0a-a297-67e17a9076ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83d30708-5834-46f4-bbe3-b2a5a6788833",
   "metadata": {},
   "source": [
    "# 2.3 Selectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad2efea-b857-4781-bf32-6605780aae57",
   "metadata": {},
   "source": [
    "## Selecting element attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf2385d-9394-4362-b6c2-7e0480e91e88",
   "metadata": {},
   "source": [
    "# 2.4 Items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7de247-b110-4754-b084-dc58d92cc4b2",
   "metadata": {},
   "source": [
    "# 2.6 Scrapy shell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b13653d-cb7b-49f0-a03c-ef2793afdb41",
   "metadata": {},
   "source": [
    "# 2.7 Item Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75551f7a-22a1-4fe1-9efa-e06e9f59fe86",
   "metadata": {},
   "source": [
    "# 2.8 Feed exports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc64ab36-c300-4809-a7b6-f881b010d0b7",
   "metadata": {},
   "source": [
    "# 2.9 Requests and Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6fba9f-0132-48ff-96b5-85c77107a00b",
   "metadata": {},
   "source": [
    "## Passing additional data to callback functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe246e0-224f-4421-89b5-4cf89ff45756",
   "metadata": {},
   "source": [
    "# 2.11 Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909bceb4-a7e3-46c6-9e44-509d67bb8f96",
   "metadata": {},
   "source": [
    "## Designating the settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4061dd4c-c4f4-4a50-b94b-847f86fe479d",
   "metadata": {},
   "source": [
    "## Built-in settings reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cbb154-6ac6-4f26-9f54-766c8aee6b91",
   "metadata": {},
   "source": [
    "# <b>3. Built-in Services</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07071642-bcdf-4074-9ac5-683c2c174547",
   "metadata": {},
   "source": [
    "# 3.1 Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9547c2-32b1-4ef1-a376-58939698db56",
   "metadata": {},
   "source": [
    "## Logging from Spiders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5acafe-f8ea-475a-be3b-8cddb02400a6",
   "metadata": {},
   "source": [
    "# <b>4. Solving specific problems</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08087fea-2315-437e-a6e1-68153d8c8982",
   "metadata": {},
   "source": [
    "# 4.12 Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afaa1fd-a706-4e34-85bf-fe6d65ff8dbc",
   "metadata": {},
   "source": [
    "# 4.13 Jobs: pausing and resuming crawls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8d9545-3f29-45c6-8a48-68aced2ada84",
   "metadata": {},
   "source": [
    "## Keeping persistent state between batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942d63ed-16e6-4ef7-ae12-73b5ea5dacc9",
   "metadata": {},
   "source": [
    "# <b>5. Extending Scrapy</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70898892-f4eb-4bcf-b4ca-fbde54e3bfdb",
   "metadata": {},
   "source": [
    "# 5.2 Add-ons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61d7184-97d7-42b7-9de6-0ed960ddc994",
   "metadata": {},
   "source": [
    "# 5.10 Core API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567a1eee-4dfd-4916-b023-a5711160ddc4",
   "metadata": {},
   "source": [
    "## Crawler API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfd090d-5027-4557-8865-67fc52ba39fd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fa5b299-6db3-4817-9b8b-2181bf669e30",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec7d5878-3fc0-43ee-b9ec-7beee99cac23",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04fe65ea-be06-4164-a3a5-8869eb302c86",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53afa41e-90fd-4a5f-b2fe-5f4d31655f6b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "923fd1d2-62ca-441e-befb-60c89be1c1de",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f346de2-ee90-48ac-b0ea-210554a663da",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c20a108c-7fc2-40c7-879c-329010edd376",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1fba1ab-b2f5-4818-910b-0392fc1cd818",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb881491-fd84-4b29-a5fb-fee985ad96bb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e936ef46-109c-4de9-9506-c0733572634d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d124c33-d47c-47a9-a074-591c7b848819",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7972708a-47b8-452b-9392-ea12104b0c55",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "298337ff-5d60-4495-be9a-b076eabe85b8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "973eb397-17f7-4766-a31e-8b328f1660f4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c618290-1313-4223-8574-a1257864c22c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcd9cde4-b0a9-47ce-8e1a-566e584f4b3c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18cbaacd-c56b-4df4-a8d1-c03f5b5fa05f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b3d7b5c-ceb7-41e6-a129-3162c4c8d68a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a0f813b-33a9-4044-95e2-d780bf967ee0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "677b6177-7e5f-446e-bf72-96ad5ae717ca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cad7ed3c-25ac-476b-bbcd-6668807863ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20dbbaa1-1768-4adb-a85f-4d08269fa1d8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ed00abc-dbf8-42f3-899b-a0d27bb0266f",
   "metadata": {},
   "source": [
    "# <b>(FORGET) Additional</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a179bc2-3bcc-4353-8412-5bf253a7bb25",
   "metadata": {},
   "source": [
    "|bash|description|\n",
    "|-|-|\n",
    "|`scrapy startproject <name>`|start a new scrapy project|\n",
    "|`scrapy genspider <spider_name> <domain>`|generate a spider in the `spider` dir|\n",
    "|`scrapy runspider <spider_file>.py`|start the crawler|\n",
    "|||\n",
    "|||\n",
    "|||"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54b00cd-908f-4697-bd45-4210bd92753f",
   "metadata": {},
   "source": [
    "# 3. Creating a Scrapy project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c0b41c-7421-492e-bd84-2de27858413c",
   "metadata": {},
   "source": [
    "You should work in the virtual environment.\n",
    "\n",
    "```sh\n",
    "pip install --upgrade pip\n",
    "pip install scrapy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bddfd4-0ed5-410c-8203-5edfe65075c5",
   "metadata": {},
   "source": [
    "A **spider** is a Scrapy project that, like its arachnid namesake, is designed to crawl webs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56816e0-5467-4284-bbf3-ec9c5ae29d4b",
   "metadata": {},
   "source": [
    "```sh\n",
    "$ scrapy startproject test1\n",
    "```\n",
    "```\n",
    "New Scrapy project 'test1', using template directory '/home/commi/venv/venv3.12/lib/python3.12/site-packages/scrapy/templates/project', created in:\n",
    "    /home/commi/Yandex.Disk/it_learning/08_parsing_data/data/test1\n",
    "\n",
    "You can start your first spider with:\n",
    "    cd test1\n",
    "    scrapy genspider example example.com\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ce26a-c0cf-48bb-973d-9733f527a6dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Project dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ecea08b8-a347-44ee-b752-d7f9c0e8a704",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:30:54.703573Z",
     "iopub.status.busy": "2024-01-15T13:30:54.703002Z",
     "iopub.status.idle": "2024-01-15T13:30:54.808659Z",
     "shell.execute_reply": "2024-01-15T13:30:54.807658Z",
     "shell.execute_reply.started": "2024-01-15T13:30:54.703536Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cd /home/commi/Yandex.Disk/it_learning/08_parsing_data/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "379af829-3fb8-4e35-a5e7-d22faf074e50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:30:55.263463Z",
     "iopub.status.busy": "2024-01-15T13:30:55.262717Z",
     "iopub.status.idle": "2024-01-15T13:30:55.381147Z",
     "shell.execute_reply": "2024-01-15T13:30:55.380041Z",
     "shell.execute_reply.started": "2024-01-15T13:30:55.263422Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtest1\u001b[0m\n",
      "├── scrapy.cfg\n",
      "└── \u001b[01;34mtest1\u001b[0m\n",
      "    ├── __init__.py\n",
      "    ├── items.py\n",
      "    ├── middlewares.py\n",
      "    ├── pipelines.py\n",
      "    ├── settings.py\n",
      "    └── \u001b[01;34mspiders\u001b[0m\n",
      "        └── __init__.py\n",
      "\n",
      "3 directories, 7 files\n"
     ]
    }
   ],
   "source": [
    "tree test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d98d0f98-06b3-4f4d-b26a-4416c914f619",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:30:57.259154Z",
     "iopub.status.busy": "2024-01-15T13:30:57.258366Z",
     "iopub.status.idle": "2024-01-15T13:30:57.408565Z",
     "shell.execute_reply": "2024-01-15T13:30:57.407181Z",
     "shell.execute_reply.started": "2024-01-15T13:30:57.259088Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Automatically created by: scrapy startproject\n",
      "#\n",
      "# For more information about the [deploy] section see:\n",
      "# https://scrapyd.readthedocs.io/en/latest/deploy.html\n",
      "\n",
      "[settings]\n",
      "default = test1.settings\n",
      "\n",
      "[deploy]\n",
      "#url = http://localhost:6800/\n",
      "project = test1\n"
     ]
    }
   ],
   "source": [
    "cat test1/scrapy.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302e9893-67d6-45dd-9d7e-27fc53d0e030",
   "metadata": {},
   "source": [
    "### Deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "13f95372-ca7a-4e93-89b2-d204b8247c76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:30:59.654728Z",
     "iopub.status.busy": "2024-01-15T13:30:59.653435Z",
     "iopub.status.idle": "2024-01-15T13:30:59.765945Z",
     "shell.execute_reply": "2024-01-15T13:30:59.764669Z",
     "shell.execute_reply.started": "2024-01-15T13:30:59.654676Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtest1/test1\u001b[0m\n",
      "├── __init__.py\n",
      "├── items.py\n",
      "├── middlewares.py\n",
      "├── pipelines.py\n",
      "├── settings.py\n",
      "└── \u001b[01;34mspiders\u001b[0m\n",
      "    └── __init__.py\n",
      "\n",
      "2 directories, 6 files\n"
     ]
    }
   ],
   "source": [
    "tree test1/test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f60b913-9ac9-4002-9848-2583c61b947c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:31:02.723037Z",
     "iopub.status.busy": "2024-01-15T13:31:02.722598Z",
     "iopub.status.idle": "2024-01-15T13:31:02.836435Z",
     "shell.execute_reply": "2024-01-15T13:31:02.835624Z",
     "shell.execute_reply.started": "2024-01-15T13:31:02.723003Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Define here the models for your scraped items\n",
      "#\n",
      "# See documentation in:\n",
      "# https://docs.scrapy.org/en/latest/topics/items.html\n",
      "\n",
      "import scrapy\n",
      "\n",
      "\n",
      "class Test1Item(scrapy.Item):\n",
      "    # define the fields for your item here like:\n",
      "    # name = scrapy.Field()\n",
      "    pass\n"
     ]
    }
   ],
   "source": [
    "cat test1/test1/items.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d8f0246-7466-4db3-9666-f499c65503dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:31:05.807978Z",
     "iopub.status.busy": "2024-01-15T13:31:05.807297Z",
     "iopub.status.idle": "2024-01-15T13:31:06.006539Z",
     "shell.execute_reply": "2024-01-15T13:31:06.005002Z",
     "shell.execute_reply.started": "2024-01-15T13:31:05.807945Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Define here the models for your spider middleware\n",
      "#\n",
      "# See documentation in:\n",
      "# https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
      "\n",
      "from scrapy import signals\n",
      "\n",
      "# useful for handling different item types with a single interface\n",
      "from itemadapter import is_item, ItemAdapter\n",
      "\n",
      "\n",
      "class Test1SpiderMiddleware:\n",
      "    # Not all methods need to be defined. If a method is not defined,\n",
      "    # scrapy acts as if the spider middleware does not modify the\n",
      "    # passed objects.\n",
      "\n",
      "    @classmethod\n",
      "    def from_crawler(cls, crawler):\n",
      "        # This method is used by Scrapy to create your spiders.\n",
      "        s = cls()\n",
      "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
      "        return s\n",
      "\n",
      "    def process_spider_input(self, response, spider):\n",
      "        # Called for each response that goes through the spider\n",
      "        # middleware and into the spider.\n",
      "\n",
      "        # Should return None or raise an exception.\n",
      "        return None\n",
      "\n",
      "    def process_spider_output(self, response, result, spider):\n",
      "        # Called with the results returned from the Spider, after\n",
      "        # it has processed the response.\n",
      "\n",
      "        # Must return an iterable of Request, or item objects.\n",
      "        for i in result:\n",
      "            yield i\n",
      "\n",
      "    def process_spider_exception(self, response, exception, spider):\n",
      "        # Called when a spider or process_spider_input() method\n",
      "        # (from other spider middleware) raises an exception.\n",
      "\n",
      "        # Should return either None or an iterable of Request or item objects.\n",
      "        pass\n",
      "\n",
      "    def process_start_requests(self, start_requests, spider):\n",
      "        # Called with the start requests of the spider, and works\n",
      "        # similarly to the process_spider_output() method, except\n",
      "        # that it doesn’t have a response associated.\n",
      "\n",
      "        # Must return only requests (not items).\n",
      "        for r in start_requests:\n",
      "            yield r\n",
      "\n",
      "    def spider_opened(self, spider):\n",
      "        spider.logger.info(\"Spider opened: %s\" % spider.name)\n",
      "\n",
      "\n",
      "class Test1DownloaderMiddleware:\n",
      "    # Not all methods need to be defined. If a method is not defined,\n",
      "    # scrapy acts as if the downloader middleware does not modify the\n",
      "    # passed objects.\n",
      "\n",
      "    @classmethod\n",
      "    def from_crawler(cls, crawler):\n",
      "        # This method is used by Scrapy to create your spiders.\n",
      "        s = cls()\n",
      "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
      "        return s\n",
      "\n",
      "    def process_request(self, request, spider):\n",
      "        # Called for each request that goes through the downloader\n",
      "        # middleware.\n",
      "\n",
      "        # Must either:\n",
      "        # - return None: continue processing this request\n",
      "        # - or return a Response object\n",
      "        # - or return a Request object\n",
      "        # - or raise IgnoreRequest: process_exception() methods of\n",
      "        #   installed downloader middleware will be called\n",
      "        return None\n",
      "\n",
      "    def process_response(self, request, response, spider):\n",
      "        # Called with the response returned from the downloader.\n",
      "\n",
      "        # Must either;\n",
      "        # - return a Response object\n",
      "        # - return a Request object\n",
      "        # - or raise IgnoreRequest\n",
      "        return response\n",
      "\n",
      "    def process_exception(self, request, exception, spider):\n",
      "        # Called when a download handler or a process_request()\n",
      "        # (from other downloader middleware) raises an exception.\n",
      "\n",
      "        # Must either:\n",
      "        # - return None: continue processing this exception\n",
      "        # - return a Response object: stops process_exception() chain\n",
      "        # - return a Request object: stops process_exception() chain\n",
      "        pass\n",
      "\n",
      "    def spider_opened(self, spider):\n",
      "        spider.logger.info(\"Spider opened: %s\" % spider.name)\n"
     ]
    }
   ],
   "source": [
    "cat test1/test1/middlewares.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70eff45e-8748-48a5-a3d3-0f29e86cddc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:31:08.926648Z",
     "iopub.status.busy": "2024-01-15T13:31:08.925824Z",
     "iopub.status.idle": "2024-01-15T13:31:09.038088Z",
     "shell.execute_reply": "2024-01-15T13:31:09.036692Z",
     "shell.execute_reply.started": "2024-01-15T13:31:08.926579Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Define your item pipelines here\n",
      "#\n",
      "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
      "# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
      "\n",
      "\n",
      "# useful for handling different item types with a single interface\n",
      "from itemadapter import ItemAdapter\n",
      "\n",
      "\n",
      "class Test1Pipeline:\n",
      "    def process_item(self, item, spider):\n",
      "        return item\n"
     ]
    }
   ],
   "source": [
    "cat test1/test1/pipelines.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f5d4940-928f-431c-a4d0-db14cf71b459",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:31:09.533481Z",
     "iopub.status.busy": "2024-01-15T13:31:09.532629Z",
     "iopub.status.idle": "2024-01-15T13:31:09.721049Z",
     "shell.execute_reply": "2024-01-15T13:31:09.719769Z",
     "shell.execute_reply.started": "2024-01-15T13:31:09.533446Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Scrapy settings for test1 project\n",
      "#\n",
      "# For simplicity, this file contains only settings considered important or\n",
      "# commonly used. You can find more settings consulting the documentation:\n",
      "#\n",
      "#     https://docs.scrapy.org/en/latest/topics/settings.html\n",
      "#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
      "#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
      "\n",
      "BOT_NAME = \"test1\"\n",
      "\n",
      "SPIDER_MODULES = [\"test1.spiders\"]\n",
      "NEWSPIDER_MODULE = \"test1.spiders\"\n",
      "\n",
      "\n",
      "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
      "#USER_AGENT = \"test1 (+http://www.yourdomain.com)\"\n",
      "\n",
      "# Obey robots.txt rules\n",
      "ROBOTSTXT_OBEY = True\n",
      "\n",
      "# Configure maximum concurrent requests performed by Scrapy (default: 16)\n",
      "#CONCURRENT_REQUESTS = 32\n",
      "\n",
      "# Configure a delay for requests for the same website (default: 0)\n",
      "# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n",
      "# See also autothrottle settings and docs\n",
      "#DOWNLOAD_DELAY = 3\n",
      "# The download delay setting will honor only one of:\n",
      "#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n",
      "#CONCURRENT_REQUESTS_PER_IP = 16\n",
      "\n",
      "# Disable cookies (enabled by default)\n",
      "#COOKIES_ENABLED = False\n",
      "\n",
      "# Disable Telnet Console (enabled by default)\n",
      "#TELNETCONSOLE_ENABLED = False\n",
      "\n",
      "# Override the default request headers:\n",
      "#DEFAULT_REQUEST_HEADERS = {\n",
      "#    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
      "#    \"Accept-Language\": \"en\",\n",
      "#}\n",
      "\n",
      "# Enable or disable spider middlewares\n",
      "# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
      "#SPIDER_MIDDLEWARES = {\n",
      "#    \"test1.middlewares.Test1SpiderMiddleware\": 543,\n",
      "#}\n",
      "\n",
      "# Enable or disable downloader middlewares\n",
      "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
      "#DOWNLOADER_MIDDLEWARES = {\n",
      "#    \"test1.middlewares.Test1DownloaderMiddleware\": 543,\n",
      "#}\n",
      "\n",
      "# Enable or disable extensions\n",
      "# See https://docs.scrapy.org/en/latest/topics/extensions.html\n",
      "#EXTENSIONS = {\n",
      "#    \"scrapy.extensions.telnet.TelnetConsole\": None,\n",
      "#}\n",
      "\n",
      "# Configure item pipelines\n",
      "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
      "#ITEM_PIPELINES = {\n",
      "#    \"test1.pipelines.Test1Pipeline\": 300,\n",
      "#}\n",
      "\n",
      "# Enable and configure the AutoThrottle extension (disabled by default)\n",
      "# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n",
      "#AUTOTHROTTLE_ENABLED = True\n",
      "# The initial download delay\n",
      "#AUTOTHROTTLE_START_DELAY = 5\n",
      "# The maximum download delay to be set in case of high latencies\n",
      "#AUTOTHROTTLE_MAX_DELAY = 60\n",
      "# The average number of requests Scrapy should be sending in parallel to\n",
      "# each remote server\n",
      "#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n",
      "# Enable showing throttling stats for every response received:\n",
      "#AUTOTHROTTLE_DEBUG = False\n",
      "\n",
      "# Enable and configure HTTP caching (disabled by default)\n",
      "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n",
      "#HTTPCACHE_ENABLED = True\n",
      "#HTTPCACHE_EXPIRATION_SECS = 0\n",
      "#HTTPCACHE_DIR = \"httpcache\"\n",
      "#HTTPCACHE_IGNORE_HTTP_CODES = []\n",
      "#HTTPCACHE_STORAGE = \"scrapy.extensions.httpcache.FilesystemCacheStorage\"\n",
      "\n",
      "# Set settings whose default value is deprecated to a future-proof value\n",
      "REQUEST_FINGERPRINTER_IMPLEMENTATION = \"2.7\"\n",
      "TWISTED_REACTOR = \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n",
      "FEED_EXPORT_ENCODING = \"utf-8\"\n"
     ]
    }
   ],
   "source": [
    "cat test1/test1/settings.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9bec3b-c7d0-4785-865c-9324c62a282b",
   "metadata": {},
   "source": [
    "### Even deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "376e12d3-10e9-4912-9fb0-17dc2587138e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:31:15.439231Z",
     "iopub.status.busy": "2024-01-15T13:31:15.438518Z",
     "iopub.status.idle": "2024-01-15T13:31:15.547558Z",
     "shell.execute_reply": "2024-01-15T13:31:15.546528Z",
     "shell.execute_reply.started": "2024-01-15T13:31:15.439172Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtest1/test1/spiders\u001b[0m\n",
      "└── __init__.py\n",
      "\n",
      "1 directory, 1 file\n"
     ]
    }
   ],
   "source": [
    "tree test1/test1/spiders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba4805d6-5240-48ab-a6a2-782d1497fe24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:31:19.282686Z",
     "iopub.status.busy": "2024-01-15T13:31:19.281995Z",
     "iopub.status.idle": "2024-01-15T13:31:19.394365Z",
     "shell.execute_reply": "2024-01-15T13:31:19.393434Z",
     "shell.execute_reply.started": "2024-01-15T13:31:19.282629Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# This package will contain the spiders of your Scrapy project\n",
      "#\n",
      "# Please refer to the documentation for information on how to create and manage\n",
      "# your spiders.\n"
     ]
    }
   ],
   "source": [
    "cat test1/test1/spiders/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da861669-3563-45aa-899a-161f78cc54c3",
   "metadata": {},
   "source": [
    "# 4. Write a Simple Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa054743-1223-4d6e-904a-16739c75b092",
   "metadata": {},
   "source": [
    "To create a crawler, you will add a new file inside the spiders directory at test1/test1/spiders/bookspider.py."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8b64b2-cd87-4ee0-8c09-eed573ba84a9",
   "metadata": {},
   "source": [
    "```sh\n",
    "$ cd test1/test1/spiders/\n",
    "$ scrapy genspider bookspider books.toscrape.com\n",
    "```\n",
    "```\n",
    "Created spider 'bookspider' using template 'basic' in module:\n",
    "  test1.spiders.bookspider\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "abd55e5b-8d6f-4c17-af1c-75a2154b264f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:31:48.403396Z",
     "iopub.status.busy": "2024-01-15T13:31:48.402628Z",
     "iopub.status.idle": "2024-01-15T13:31:48.514078Z",
     "shell.execute_reply": "2024-01-15T13:31:48.512387Z",
     "shell.execute_reply.started": "2024-01-15T13:31:48.403331Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtest1/test1/spiders/\u001b[0m\n",
      "├── bookspider.py\n",
      "├── __init__.py\n",
      "└── \u001b[01;34m__pycache__\u001b[0m\n",
      "    └── __init__.cpython-312.pyc\n",
      "\n",
      "2 directories, 3 files\n"
     ]
    }
   ],
   "source": [
    "tree test1/test1/spiders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "30875633-3fa4-4bd9-bee8-60dba3b0e327",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:32:14.207312Z",
     "iopub.status.busy": "2024-01-15T13:32:14.206579Z",
     "iopub.status.idle": "2024-01-15T13:32:14.318134Z",
     "shell.execute_reply": "2024-01-15T13:32:14.315643Z",
     "shell.execute_reply.started": "2024-01-15T13:32:14.207255Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import scrapy\n",
      "\n",
      "\n",
      "class BookspiderSpider(scrapy.Spider):\n",
      "    name = \"bookspider\"\n",
      "    allowed_domains = [\"books.toscrape.com\"]\n",
      "    start_urls = [\"https://books.toscrape.com\"]\n",
      "\n",
      "    def parse(self, response):\n",
      "        pass\n"
     ]
    }
   ],
   "source": [
    "cat test1/test1/spiders/bookspider.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
