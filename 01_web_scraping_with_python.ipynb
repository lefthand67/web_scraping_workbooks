{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "833398a9-2146-4b73-b90f-15e962c5c6cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# WEB SCRAPING WITH PYTHON\n",
    "\n",
    "Second Edition by Ryan Mitchell (O’Reilly). Copyright 2018 Ryan Mitchell, 978-1-491-998557-1.\n",
    "\n",
    "Code examples on [github](https://github.com/REMitchell/python-scraping)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3768c98-d83f-4238-bcbe-602b6bebf1b8",
   "metadata": {},
   "source": [
    "_ChatGPT:_  \n",
    "These modules are commonly used for web scraping tasks in Python and are known for their flexibility, ease of use, and robust functionality.\n",
    "\n",
    "- **Beautiful Soup**: A powerful parsing library for extracting data from HTML and XML files.\n",
    "- **Scrapy**: An open-source web crawling and scraping framework for Python.\n",
    "- **Requests**: A simple and easy-to-use library for making HTTP requests and handling responses.\n",
    "- **Selenium**: A web browser automation tool that can be used for web scraping by simulating user interactions.\n",
    "- **LXML**: A fast and feature-rich XML and HTML processing library.\n",
    "- **PyQuery**: A library that provides jQuery-like syntax for parsing and manipulating HTML and XML documents.\n",
    "- **Pandas**: A data manipulation library that can be useful for organizing and cleaning data retrieved from web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57eb8e8-1f1f-4812-99ba-23bd27e6be6d",
   "metadata": {},
   "source": [
    "# <b>Preface</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef18dba-e485-4a50-a679-b0d51994b49f",
   "metadata": {},
   "source": [
    "## What Is Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ff853e-b13a-4430-a762-25b8e91278f3",
   "metadata": {},
   "source": [
    "The automated gathering of data from the internet is nearly as old as the internet itself. Although web scraping is not a new term, in years past the practice has been more commonly known as \n",
    "- screen scraping, \n",
    "- data mining, \n",
    "- web harvesting,\n",
    "\n",
    "or similar variations. General consensus today seems to favor web scraping, so that is the term I use throughout the book, although I also refer to programs that specifically traverse multiple pages as web crawlers or refer to the web scraping programs themselves as bots.\n",
    "\n",
    "In theory, \n",
    "\n",
    "> **web scraping** is the practice of gathering data through any means other than a program interacting with an API (or, obviously, through a human using a web browser). \n",
    "\n",
    "This is most commonly accomplished by writing an automated program that \n",
    "- queries a web server, \n",
    "- requests data (usually in the form of HTML and other files that compose web pages), and then \n",
    "- parses that data to extract needed information.\n",
    "\n",
    "In practice, web scraping encompasses a wide variety of programming techniques and technologies, such as \n",
    "- data analysis, \n",
    "- natural language parsing, and \n",
    "- information security. \n",
    "\n",
    "Because the scope of the field is so broad, this book covers the fundamental basics of web scraping and crawling in Part I and delves into advanced topics in Part II. I suggest that all readers carefully study the first part and delve into the more specific in the second part as needed.\n",
    "\n",
    "With few exceptions, if you can view data in your browser, you can access it via a Python script. If you can access it in a script, you can store it in a database. And if you can store it in a database, you can do virtually anything with that data.\n",
    "\n",
    "Web scraping is a relatively disparate subject, with practices that require the use of databases, web servers, HTTP, HTML, internet security, image processing, data science, and other tools. This book attempts to cover all of these, and other topics, from the perspective of “data gathering.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecf4aa9-fb6b-4887-9e39-c307f37a0c40",
   "metadata": {},
   "source": [
    "# <b>PART I. BUILDING SCRAPERS</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eacea4-08d8-4063-b514-f61bbf023412",
   "metadata": {},
   "source": [
    "- how to use Python to request information from a web server, \n",
    "- how to perform basic handling of the server’s response, and \n",
    "- how to begin interacting with a website in an automated fashion.\n",
    "\n",
    "In all likelihood, 90% of web scraping projects you’ll encounter will draw on techniques used in just the next six chapters. This section covers what the general (albeit technically savvy) public tends to think of when they think of “web scrapers”:\n",
    "- Retrieving HTML data from a domain name,\n",
    "- Parsing that data for target information,\n",
    "- Storing the target information,\n",
    "- Optionally, moving to another page to repeat the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad08683-3998-4de6-a2ed-c6f267da69c5",
   "metadata": {},
   "source": [
    "# <b>1. Your First Web Scraper</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfadb3e3-3fc1-4227-a02a-1c5d77057477",
   "metadata": {},
   "source": [
    "# 1.1 Connecting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d8f8fc-1a8e-4c80-a5a3-636f8c0ca526",
   "metadata": {},
   "source": [
    "In fact, browsers are a relatively recent invention in the history of the internet, considering Nexus was released in 1990.\n",
    "\n",
    "Yes, the web browser is a useful application for creating these packets of information, telling your operating system to send them off, and interpreting the data you get back as pretty pictures, sounds, videos, and text. However, a web browser is just code, and code can be taken apart, broken into its basic components, rewritten, reused, and made to do anything you want. A web browser can tell the processor to send data to the application that handles your wireless (or wired) interface, but you can do the same thing in Python with just three lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1114be2-5ed4-4564-9ce9-419d93751d4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T10:06:46.968968Z",
     "iopub.status.busy": "2024-01-21T10:06:46.968352Z",
     "iopub.status.idle": "2024-01-21T10:06:47.772141Z",
     "shell.execute_reply": "2024-01-21T10:06:47.769981Z",
     "shell.execute_reply.started": "2024-01-21T10:06:46.968922Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<html>\\n<head>\\n<title>A Useful Page</title>\\n</head>\\n<body>\\n<h1>An Interesting Title</h1>\\n<div>\\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n</div>\\n</body>\\n</html>\\n'\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "html = urlopen('http://pythonscraping.com/pages/page1.html')\n",
    "print(html.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8d466b-0b0e-4989-8f30-eba438747a03",
   "metadata": {},
   "source": [
    "This command outputs the complete HTML code for `page1` located at the URL `http://pythonscraping.com/pages/page1.html`. More accurately, this outputs the HTML file `page1.html`, found in the directory `<web root>/pages`, on the server located at the domain name `http://pythonscraping.com`.\n",
    "\n",
    "Why is it important to start thinking of these addresses as “files” rather than “pages”? Most modern web pages have many resource files associated with them. These could be image files, JavaScript files, CSS files, or any other content that the page you are requesting is linked to. When a web browser hits a tag such as `<img src=\"cuteKitten.jpg\">`, the browser knows that it needs to make another request to the server to get the data at the file `cuteKitten.jpg` in order to fully render the page for the user.\n",
    "\n",
    "Of course, your Python script doesn’t have the logic to go back and request multiple files (yet); it can only read the single HTML file that you’ve directly requested.\n",
    "\n",
    "`urllib` is a standard Python library and contains functions for requesting data across the web, handling cookies, and even changing metadata such as headers and your user agent. \n",
    "\n",
    "`urlopen` is used to open a remote object across a network and read it. Because it is a fairly generic function (it can read HTML files, image files, or any other file stream with ease), we will be using it quite frequently throughout the book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeae654-7b4d-47e1-8928-bc87833b5740",
   "metadata": {},
   "source": [
    "# 1.2 An Introduction to BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1e7dd3-0fb2-4b22-8814-fe3c6d8e6ed2",
   "metadata": {},
   "source": [
    "Like its Wonderland namesake, `BeautifulSoup` tries to make sense of the nonsensical; it helps format and organize the messy web by fixing bad HTML and presenting us with easily traversable Python objects representing XML structures.\n",
    "\n",
    "```sh\n",
    "(venv) user@host:~$ pip install beautifulsoup4\n",
    "```\n",
    "\n",
    "You cannot easily access venv in bash from Jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b60b15b1-8b81-46b6-afcc-5f69aa450194",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T10:13:37.619379Z",
     "iopub.status.busy": "2024-01-21T10:13:37.616785Z",
     "iopub.status.idle": "2024-01-21T10:13:37.865734Z",
     "shell.execute_reply": "2024-01-21T10:13:37.864436Z",
     "shell.execute_reply.started": "2024-01-21T10:13:37.619222Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.1\n",
      "Python 3.11.2\n"
     ]
    }
   ],
   "source": [
    "! source ~/venv/venv3.12/bin/activate && echo $(python -V)\n",
    "! python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5ff07cd-4c4f-4a58-9e1c-e21765009cb7",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-01-18T16:03:26.555378Z",
     "iopub.status.busy": "2024-01-18T16:03:26.553961Z",
     "iopub.status.idle": "2024-01-18T16:03:26.595810Z",
     "shell.execute_reply": "2024-01-18T16:03:26.593982Z",
     "shell.execute_reply.started": "2024-01-18T16:03:26.555322Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package bs4:\n",
      "\n",
      "NAME\n",
      "    bs4 - Beautiful Soup Elixir and Tonic - \"The Screen-Scraper's Friend\".\n",
      "\n",
      "DESCRIPTION\n",
      "    http://www.crummy.com/software/BeautifulSoup/\n",
      "\n",
      "    Beautiful Soup uses a pluggable XML or HTML parser to parse a\n",
      "    (possibly invalid) document into a tree representation. Beautiful Soup\n",
      "    provides methods and Pythonic idioms that make it easy to navigate,\n",
      "    search, and modify the parse tree.\n",
      "\n",
      "    Beautiful Soup works with Python 3.6 and up. It works better if lxml\n",
      "    and/or html5lib is installed.\n",
      "\n",
      "    For more than you ever wanted to know about Beautiful Soup, see the\n",
      "    documentation: http://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    builder (package)\n",
      "    css\n",
      "    dammit\n",
      "    diagnose\n",
      "    element\n",
      "    formatter\n",
      "    tests (package)\n",
      "\n",
      "CLASSES\n",
      "    bs4.element.Tag(bs4.element.PageElement)\n",
      "        BeautifulSoup\n",
      "\n",
      "    class BeautifulSoup(bs4.element.Tag)\n",
      "     |  BeautifulSoup(markup='', features=None, builder=None, parse_only=None, from_encoding=None, exclude_encodings=None, element_classes=None, **kwargs)\n",
      "     |\n",
      "     |  A data structure representing a parsed HTML or XML document.\n",
      "     |\n",
      "     |  Most of the methods you'll call on a BeautifulSoup object are inherited from\n",
      "     |  PageElement or Tag.\n",
      "     |\n",
      "     |  Internally, this class defines the basic interface called by the\n",
      "     |  tree builders when converting an HTML/XML document into a data\n",
      "     |  structure. The interface abstracts away the differences between\n",
      "     |  parsers. To write a new tree builder, you'll need to understand\n",
      "     |  these methods as a whole.\n",
      "     |\n",
      "     |  These methods will be called by the BeautifulSoup constructor:\n",
      "     |    * reset()\n",
      "     |    * feed(markup)\n",
      "     |\n",
      "     |  The tree builder may call these methods from its feed() implementation:\n",
      "     |    * handle_starttag(name, attrs) # See note about return value\n",
      "     |    * handle_endtag(name)\n",
      "     |    * handle_data(data) # Appends to the current data node\n",
      "     |    * endData(containerClass) # Ends the current data node\n",
      "     |\n",
      "     |  No matter how complicated the underlying parser is, you should be\n",
      "     |  able to build a tree using 'start tag' events, 'end tag' events,\n",
      "     |  'data' events, and \"done with data\" events.\n",
      "     |\n",
      "     |  If you encounter an empty-element tag (aka a self-closing tag,\n",
      "     |  like HTML's <br> tag), call handle_starttag and then\n",
      "     |  handle_endtag.\n",
      "     |\n",
      "     |  Method resolution order:\n",
      "     |      BeautifulSoup\n",
      "     |      bs4.element.Tag\n",
      "     |      bs4.element.PageElement\n",
      "     |      builtins.object\n",
      "     |\n",
      "     |  Methods defined here:\n",
      "     |\n",
      "     |  __getstate__(self)\n",
      "     |      Helper for pickle.\n",
      "     |\n",
      "     |  __init__(self, markup='', features=None, builder=None, parse_only=None, from_encoding=None, exclude_encodings=None, element_classes=None, **kwargs)\n",
      "     |      Constructor.\n",
      "     |\n",
      "     |      :param markup: A string or a file-like object representing\n",
      "     |       markup to be parsed.\n",
      "     |\n",
      "     |      :param features: Desirable features of the parser to be\n",
      "     |       used. This may be the name of a specific parser (\"lxml\",\n",
      "     |       \"lxml-xml\", \"html.parser\", or \"html5lib\") or it may be the\n",
      "     |       type of markup to be used (\"html\", \"html5\", \"xml\"). It's\n",
      "     |       recommended that you name a specific parser, so that\n",
      "     |       Beautiful Soup gives you the same results across platforms\n",
      "     |       and virtual environments.\n",
      "     |\n",
      "     |      :param builder: A TreeBuilder subclass to instantiate (or\n",
      "     |       instance to use) instead of looking one up based on\n",
      "     |       `features`. You only need to use this if you've implemented a\n",
      "     |       custom TreeBuilder.\n",
      "     |\n",
      "     |      :param parse_only: A SoupStrainer. Only parts of the document\n",
      "     |       matching the SoupStrainer will be considered. This is useful\n",
      "     |       when parsing part of a document that would otherwise be too\n",
      "     |       large to fit into memory.\n",
      "     |\n",
      "     |      :param from_encoding: A string indicating the encoding of the\n",
      "     |       document to be parsed. Pass this in if Beautiful Soup is\n",
      "     |       guessing wrongly about the document's encoding.\n",
      "     |\n",
      "     |      :param exclude_encodings: A list of strings indicating\n",
      "     |       encodings known to be wrong. Pass this in if you don't know\n",
      "     |       the document's encoding but you know Beautiful Soup's guess is\n",
      "     |       wrong.\n",
      "     |\n",
      "     |      :param element_classes: A dictionary mapping BeautifulSoup\n",
      "     |       classes like Tag and NavigableString, to other classes you'd\n",
      "     |       like to be instantiated instead as the parse tree is\n",
      "     |       built. This is useful for subclassing Tag or NavigableString\n",
      "     |       to modify default behavior.\n",
      "     |\n",
      "     |      :param kwargs: For backwards compatibility purposes, the\n",
      "     |       constructor accepts certain keyword arguments used in\n",
      "     |       Beautiful Soup 3. None of these arguments do anything in\n",
      "     |       Beautiful Soup 4; they will result in a warning and then be\n",
      "     |       ignored.\n",
      "     |\n",
      "     |       Apart from this, any keyword arguments passed into the\n",
      "     |       BeautifulSoup constructor are propagated to the TreeBuilder\n",
      "     |       constructor. This makes it possible to configure a\n",
      "     |       TreeBuilder by passing in arguments, not just by saying which\n",
      "     |       one to use.\n",
      "     |\n",
      "     |  __setstate__(self, state)\n",
      "     |\n",
      "     |  decode(self, pretty_print=False, eventual_encoding='utf-8', formatter='minimal', iterator=None)\n",
      "     |      Returns a string or Unicode representation of the parse tree\n",
      "     |          as an HTML or XML document.\n",
      "     |\n",
      "     |      :param pretty_print: If this is True, indentation will be used to\n",
      "     |          make the document more readable.\n",
      "     |      :param eventual_encoding: The encoding of the final document.\n",
      "     |          If this is None, the document will be a Unicode string.\n",
      "     |\n",
      "     |  endData(self, containerClass=None)\n",
      "     |      Method called by the TreeBuilder when the end of a data segment\n",
      "     |      occurs.\n",
      "     |\n",
      "     |  handle_data(self, data)\n",
      "     |      Called by the tree builder when a chunk of textual data is encountered.\n",
      "     |\n",
      "     |  handle_endtag(self, name, nsprefix=None)\n",
      "     |      Called by the tree builder when an ending tag is encountered.\n",
      "     |\n",
      "     |      :param name: Name of the tag.\n",
      "     |      :param nsprefix: Namespace prefix for the tag.\n",
      "     |\n",
      "     |  handle_starttag(self, name, namespace, nsprefix, attrs, sourceline=None, sourcepos=None, namespaces=None)\n",
      "     |      Called by the tree builder when a new tag is encountered.\n",
      "     |\n",
      "     |      :param name: Name of the tag.\n",
      "     |      :param nsprefix: Namespace prefix for the tag.\n",
      "     |      :param attrs: A dictionary of attribute values.\n",
      "     |      :param sourceline: The line number where this tag was found in its\n",
      "     |          source document.\n",
      "     |      :param sourcepos: The character position within `sourceline` where this\n",
      "     |          tag was found.\n",
      "     |      :param namespaces: A dictionary of all namespace prefix mappings\n",
      "     |          currently in scope in the document.\n",
      "     |\n",
      "     |      If this method returns None, the tag was rejected by an active\n",
      "     |      SoupStrainer. You should proceed as if the tag had not occurred\n",
      "     |      in the document. For instance, if this was a self-closing tag,\n",
      "     |      don't call handle_endtag.\n",
      "     |\n",
      "     |  insert_after(self, *args)\n",
      "     |      This method is part of the PageElement API, but `BeautifulSoup` doesn't implement\n",
      "     |      it because there is nothing before or after it in the parse tree.\n",
      "     |\n",
      "     |  insert_before(self, *args)\n",
      "     |      This method is part of the PageElement API, but `BeautifulSoup` doesn't implement\n",
      "     |      it because there is nothing before or after it in the parse tree.\n",
      "     |\n",
      "     |  new_string(self, s, subclass=None)\n",
      "     |      Create a new NavigableString associated with this BeautifulSoup\n",
      "     |      object.\n",
      "     |\n",
      "     |  new_tag(self, name, namespace=None, nsprefix=None, attrs={}, sourceline=None, sourcepos=None, **kwattrs)\n",
      "     |      Create a new Tag associated with this BeautifulSoup object.\n",
      "     |\n",
      "     |      :param name: The name of the new Tag.\n",
      "     |      :param namespace: The URI of the new Tag's XML namespace, if any.\n",
      "     |      :param prefix: The prefix for the new Tag's XML namespace, if any.\n",
      "     |      :param attrs: A dictionary of this Tag's attribute values; can\n",
      "     |          be used instead of `kwattrs` for attributes like 'class'\n",
      "     |          that are reserved words in Python.\n",
      "     |      :param sourceline: The line number where this tag was\n",
      "     |          (purportedly) found in its source document.\n",
      "     |      :param sourcepos: The character position within `sourceline` where this\n",
      "     |          tag was (purportedly) found.\n",
      "     |      :param kwattrs: Keyword arguments for the new Tag's attribute values.\n",
      "     |\n",
      "     |  object_was_parsed(self, o, parent=None, most_recent_element=None)\n",
      "     |      Method called by the TreeBuilder to integrate an object into the parse tree.\n",
      "     |\n",
      "     |  popTag(self)\n",
      "     |      Internal method called by _popToTag when a tag is closed.\n",
      "     |\n",
      "     |  pushTag(self, tag)\n",
      "     |      Internal method called by handle_starttag when a tag is opened.\n",
      "     |\n",
      "     |  reset(self)\n",
      "     |      Reset this object to a state as though it had never parsed any\n",
      "     |      markup.\n",
      "     |\n",
      "     |  string_container(self, base_class=None)\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |\n",
      "     |  ASCII_SPACES = ' \\n\\t\\x0c\\r'\n",
      "     |\n",
      "     |  DEFAULT_BUILDER_FEATURES = ['html', 'fast']\n",
      "     |\n",
      "     |  NO_PARSER_SPECIFIED_WARNING = 'No parser was explicitly specified, so ...\n",
      "     |\n",
      "     |  ROOT_TAG_NAME = '[document]'\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from bs4.element.Tag:\n",
      "     |\n",
      "     |  __bool__(self)\n",
      "     |      A tag is non-None even if it has no contents.\n",
      "     |\n",
      "     |  __call__(self, *args, **kwargs)\n",
      "     |      Calling a Tag like a function is the same as calling its\n",
      "     |      find_all() method. Eg. tag('a') returns a list of all the A tags\n",
      "     |      found within this tag.\n",
      "     |\n",
      "     |  __contains__(self, x)\n",
      "     |\n",
      "     |  __copy__(self)\n",
      "     |      A copy of a Tag must always be a deep copy, because a Tag's\n",
      "     |      children can only have one parent at a time.\n",
      "     |\n",
      "     |  __deepcopy__(self, memo, recursive=True)\n",
      "     |      A deepcopy of a Tag is a new Tag, unconnected to the parse tree.\n",
      "     |      Its contents are a copy of the old Tag's contents.\n",
      "     |\n",
      "     |  __delitem__(self, key)\n",
      "     |      Deleting tag[key] deletes all 'key' attributes for the tag.\n",
      "     |\n",
      "     |  __eq__(self, other)\n",
      "     |      Returns true iff this Tag has the same name, the same attributes,\n",
      "     |      and the same contents (recursively) as `other`.\n",
      "     |\n",
      "     |  __getattr__(self, tag)\n",
      "     |      Calling tag.subtag is the same as calling tag.find(name=\"subtag\")\n",
      "     |\n",
      "     |  __getitem__(self, key)\n",
      "     |      tag[key] returns the value of the 'key' attribute for the Tag,\n",
      "     |      and throws an exception if it's not there.\n",
      "     |\n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |\n",
      "     |  __iter__(self)\n",
      "     |      Iterating over a Tag iterates over its contents.\n",
      "     |\n",
      "     |  __len__(self)\n",
      "     |      The length of a Tag is the length of its list of contents.\n",
      "     |\n",
      "     |  __ne__(self, other)\n",
      "     |      Returns true iff this Tag is not identical to `other`,\n",
      "     |      as defined in __eq__.\n",
      "     |\n",
      "     |  __repr__ = __unicode__(self)\n",
      "     |\n",
      "     |  __setitem__(self, key, value)\n",
      "     |      Setting tag[key] sets the value of the 'key' attribute for the\n",
      "     |      tag.\n",
      "     |\n",
      "     |  __str__ = __unicode__(self)\n",
      "     |\n",
      "     |  __unicode__(self)\n",
      "     |      Renders this PageElement as a Unicode string.\n",
      "     |\n",
      "     |  childGenerator(self)\n",
      "     |      Deprecated generator.\n",
      "     |\n",
      "     |  clear(self, decompose=False)\n",
      "     |      Wipe out all children of this PageElement by calling extract()\n",
      "     |         on them.\n",
      "     |\n",
      "     |      :param decompose: If this is True, decompose() (a more\n",
      "     |          destructive method) will be called instead of extract().\n",
      "     |\n",
      "     |  decode_contents(self, indent_level=None, eventual_encoding='utf-8', formatter='minimal')\n",
      "     |      Renders the contents of this tag as a Unicode string.\n",
      "     |\n",
      "     |      :param indent_level: Each line of the rendering will be\n",
      "     |         indented this many levels. (The formatter decides what a\n",
      "     |         'level' means in terms of spaces or other characters\n",
      "     |         output.) Used internally in recursive calls while\n",
      "     |         pretty-printing.\n",
      "     |\n",
      "     |      :param eventual_encoding: The tag is destined to be\n",
      "     |         encoded into this encoding. decode_contents() is _not_\n",
      "     |         responsible for performing that encoding. This information\n",
      "     |         is passed in so that it can be substituted in if the\n",
      "     |         document contains a <META> tag that mentions the document's\n",
      "     |         encoding.\n",
      "     |\n",
      "     |      :param formatter: A Formatter object, or a string naming one of\n",
      "     |          the standard Formatters.\n",
      "     |\n",
      "     |  decompose(self)\n",
      "     |      Recursively destroys this PageElement and its children.\n",
      "     |\n",
      "     |      This element will be removed from the tree and wiped out; so\n",
      "     |      will everything beneath it.\n",
      "     |\n",
      "     |      The behavior of a decomposed PageElement is undefined and you\n",
      "     |      should never use one for anything, but if you need to _check_\n",
      "     |      whether an element has been decomposed, you can use the\n",
      "     |      `decomposed` property.\n",
      "     |\n",
      "     |  encode(self, encoding='utf-8', indent_level=None, formatter='minimal', errors='xmlcharrefreplace')\n",
      "     |      Render a bytestring representation of this PageElement and its\n",
      "     |      contents.\n",
      "     |\n",
      "     |      :param encoding: The destination encoding.\n",
      "     |      :param indent_level: Each line of the rendering will be\n",
      "     |         indented this many levels. (The formatter decides what a\n",
      "     |         'level' means in terms of spaces or other characters\n",
      "     |         output.) Used internally in recursive calls while\n",
      "     |         pretty-printing.\n",
      "     |      :param formatter: A Formatter object, or a string naming one of\n",
      "     |          the standard formatters.\n",
      "     |      :param errors: An error handling strategy such as\n",
      "     |          'xmlcharrefreplace'. This value is passed along into\n",
      "     |          encode() and its value should be one of the constants\n",
      "     |          defined by Python.\n",
      "     |      :return: A bytestring.\n",
      "     |\n",
      "     |  encode_contents(self, indent_level=None, encoding='utf-8', formatter='minimal')\n",
      "     |      Renders the contents of this PageElement as a bytestring.\n",
      "     |\n",
      "     |      :param indent_level: Each line of the rendering will be\n",
      "     |         indented this many levels. (The formatter decides what a\n",
      "     |         'level' means in terms of spaces or other characters\n",
      "     |         output.) Used internally in recursive calls while\n",
      "     |         pretty-printing.\n",
      "     |\n",
      "     |      :param eventual_encoding: The bytestring will be in this encoding.\n",
      "     |\n",
      "     |      :param formatter: A Formatter object, or a string naming one of\n",
      "     |          the standard Formatters.\n",
      "     |\n",
      "     |      :return: A bytestring.\n",
      "     |\n",
      "     |  find(self, name=None, attrs={}, recursive=True, string=None, **kwargs)\n",
      "     |      Look in the children of this PageElement and find the first\n",
      "     |      PageElement that matches the given criteria.\n",
      "     |\n",
      "     |      All find_* methods take a common set of arguments. See the online\n",
      "     |      documentation for detailed explanations.\n",
      "     |\n",
      "     |      :param name: A filter on tag name.\n",
      "     |      :param attrs: A dictionary of filters on attribute values.\n",
      "     |      :param recursive: If this is True, find() will perform a\n",
      "     |          recursive search of this PageElement's children. Otherwise,\n",
      "     |          only the direct children will be considered.\n",
      "     |      :param limit: Stop looking after finding this many results.\n",
      "     |      :kwargs: A dictionary of filters on attribute values.\n",
      "     |      :return: A PageElement.\n",
      "     |      :rtype: bs4.element.Tag | bs4.element.NavigableString\n",
      "     |\n",
      "     |  findAll = find_all(self, name=None, attrs={}, recursive=True, string=None, limit=None, **kwargs)\n",
      "     |\n",
      "     |  findChild = find(self, name=None, attrs={}, recursive=True, string=None, **kwargs)\n",
      "     |\n",
      "     |  findChildren = find_all(self, name=None, attrs={}, recursive=True, string=None, limit=None, **kwargs)\n",
      "     |\n",
      "     |  find_all(self, name=None, attrs={}, recursive=True, string=None, limit=None, **kwargs)\n",
      "     |      Look in the children of this PageElement and find all\n",
      "     |      PageElements that match the given criteria.\n",
      "     |\n",
      "     |      All find_* methods take a common set of arguments. See the online\n",
      "     |      documentation for detailed explanations.\n",
      "     |\n",
      "     |      :param name: A filter on tag name.\n",
      "     |      :param attrs: A dictionary of filters on attribute values.\n",
      "     |      :param recursive: If this is True, find_all() will perform a\n",
      "     |          recursive search of this PageElement's children. Otherwise,\n",
      "     |          only the direct children will be considered.\n",
      "     |      :param limit: Stop looking after finding this many results.\n",
      "     |      :kwargs: A dictionary of filters on attribute values.\n",
      "     |      :return: A ResultSet of PageElements.\n",
      "     |      :rtype: bs4.element.ResultSet\n",
      "     |\n",
      "     |  get(self, key, default=None)\n",
      "     |      Returns the value of the 'key' attribute for the tag, or\n",
      "     |      the value given for 'default' if it doesn't have that\n",
      "     |      attribute.\n",
      "     |\n",
      "     |  get_attribute_list(self, key, default=None)\n",
      "     |      The same as get(), but always returns a list.\n",
      "     |\n",
      "     |      :param key: The attribute to look for.\n",
      "     |      :param default: Use this value if the attribute is not present\n",
      "     |          on this PageElement.\n",
      "     |      :return: A list of values, probably containing only a single\n",
      "     |          value.\n",
      "     |\n",
      "     |  has_attr(self, key)\n",
      "     |      Does this PageElement have an attribute with the given name?\n",
      "     |\n",
      "     |  has_key(self, key)\n",
      "     |      Deprecated method. This was kind of misleading because has_key()\n",
      "     |      (attributes) was different from __in__ (contents).\n",
      "     |\n",
      "     |      has_key() is gone in Python 3, anyway.\n",
      "     |\n",
      "     |  index(self, element)\n",
      "     |      Find the index of a child by identity, not value.\n",
      "     |\n",
      "     |      Avoids issues with tag.contents.index(element) getting the\n",
      "     |      index of equal elements.\n",
      "     |\n",
      "     |      :param element: Look for this PageElement in `self.contents`.\n",
      "     |\n",
      "     |  prettify(self, encoding=None, formatter='minimal')\n",
      "     |      Pretty-print this PageElement as a string.\n",
      "     |\n",
      "     |      :param encoding: The eventual encoding of the string. If this is None,\n",
      "     |          a Unicode string will be returned.\n",
      "     |      :param formatter: A Formatter object, or a string naming one of\n",
      "     |          the standard formatters.\n",
      "     |      :return: A Unicode string (if encoding==None) or a bytestring\n",
      "     |          (otherwise).\n",
      "     |\n",
      "     |  recursiveChildGenerator(self)\n",
      "     |      Deprecated generator.\n",
      "     |\n",
      "     |  renderContents(self, encoding='utf-8', prettyPrint=False, indentLevel=0)\n",
      "     |      Deprecated method for BS3 compatibility.\n",
      "     |\n",
      "     |  select(self, selector, namespaces=None, limit=None, **kwargs)\n",
      "     |      Perform a CSS selection operation on the current element.\n",
      "     |\n",
      "     |      This uses the SoupSieve library.\n",
      "     |\n",
      "     |      :param selector: A string containing a CSS selector.\n",
      "     |\n",
      "     |      :param namespaces: A dictionary mapping namespace prefixes\n",
      "     |         used in the CSS selector to namespace URIs. By default,\n",
      "     |         Beautiful Soup will use the prefixes it encountered while\n",
      "     |         parsing the document.\n",
      "     |\n",
      "     |      :param limit: After finding this number of results, stop looking.\n",
      "     |\n",
      "     |      :param kwargs: Keyword arguments to be passed into SoupSieve's\n",
      "     |         soupsieve.select() method.\n",
      "     |\n",
      "     |      :return: A ResultSet of Tags.\n",
      "     |      :rtype: bs4.element.ResultSet\n",
      "     |\n",
      "     |  select_one(self, selector, namespaces=None, **kwargs)\n",
      "     |      Perform a CSS selection operation on the current element.\n",
      "     |\n",
      "     |      :param selector: A CSS selector.\n",
      "     |\n",
      "     |      :param namespaces: A dictionary mapping namespace prefixes\n",
      "     |         used in the CSS selector to namespace URIs. By default,\n",
      "     |         Beautiful Soup will use the prefixes it encountered while\n",
      "     |         parsing the document.\n",
      "     |\n",
      "     |      :param kwargs: Keyword arguments to be passed into Soup Sieve's\n",
      "     |         soupsieve.select() method.\n",
      "     |\n",
      "     |      :return: A Tag.\n",
      "     |      :rtype: bs4.element.Tag\n",
      "     |\n",
      "     |  smooth(self)\n",
      "     |      Smooth out this element's children by consolidating consecutive\n",
      "     |      strings.\n",
      "     |\n",
      "     |      This makes pretty-printed output look more natural following a\n",
      "     |      lot of operations that modified the tree.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from bs4.element.Tag:\n",
      "     |\n",
      "     |  children\n",
      "     |      Iterate over all direct children of this PageElement.\n",
      "     |\n",
      "     |      :yield: A sequence of PageElements.\n",
      "     |\n",
      "     |  css\n",
      "     |      Return an interface to the CSS selector API.\n",
      "     |\n",
      "     |  descendants\n",
      "     |      Iterate over all children of this PageElement in a\n",
      "     |      breadth-first sequence.\n",
      "     |\n",
      "     |      :yield: A sequence of PageElements.\n",
      "     |\n",
      "     |  isSelfClosing\n",
      "     |      Is this tag an empty-element tag? (aka a self-closing tag)\n",
      "     |\n",
      "     |      A tag that has contents is never an empty-element tag.\n",
      "     |\n",
      "     |      A tag that has no contents may or may not be an empty-element\n",
      "     |      tag. It depends on the builder used to create the tag. If the\n",
      "     |      builder has a designated list of empty-element tags, then only\n",
      "     |      a tag whose name shows up in that list is considered an\n",
      "     |      empty-element tag.\n",
      "     |\n",
      "     |      If the builder has no designated list of empty-element tags,\n",
      "     |      then any tag with no contents is an empty-element tag.\n",
      "     |\n",
      "     |  is_empty_element\n",
      "     |      Is this tag an empty-element tag? (aka a self-closing tag)\n",
      "     |\n",
      "     |      A tag that has contents is never an empty-element tag.\n",
      "     |\n",
      "     |      A tag that has no contents may or may not be an empty-element\n",
      "     |      tag. It depends on the builder used to create the tag. If the\n",
      "     |      builder has a designated list of empty-element tags, then only\n",
      "     |      a tag whose name shows up in that list is considered an\n",
      "     |      empty-element tag.\n",
      "     |\n",
      "     |      If the builder has no designated list of empty-element tags,\n",
      "     |      then any tag with no contents is an empty-element tag.\n",
      "     |\n",
      "     |  self_and_descendants\n",
      "     |      Iterate over this PageElement and its children in a\n",
      "     |      breadth-first sequence.\n",
      "     |\n",
      "     |      :yield: A sequence of PageElements.\n",
      "     |\n",
      "     |  strings\n",
      "     |      Yield all strings of certain classes, possibly stripping them.\n",
      "     |\n",
      "     |      :param strip: If True, all strings will be stripped before being\n",
      "     |          yielded.\n",
      "     |\n",
      "     |      :param types: A tuple of NavigableString subclasses. Any strings of\n",
      "     |          a subclass not found in this list will be ignored. By\n",
      "     |          default, the subclasses considered are the ones found in\n",
      "     |          self.interesting_string_types. If that's not specified,\n",
      "     |          only NavigableString and CData objects will be\n",
      "     |          considered. That means no comments, processing\n",
      "     |          instructions, etc.\n",
      "     |\n",
      "     |      :yield: A sequence of strings.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from bs4.element.Tag:\n",
      "     |\n",
      "     |  parserClass\n",
      "     |\n",
      "     |  string\n",
      "     |      Convenience property to get the single string within this\n",
      "     |      PageElement.\n",
      "     |\n",
      "     |      TODO It might make sense to have NavigableString.string return\n",
      "     |      itself.\n",
      "     |\n",
      "     |      :return: If this element has a single string child, return\n",
      "     |       value is that string. If this element has one child tag,\n",
      "     |       return value is the 'string' attribute of the child tag,\n",
      "     |       recursively. If this element is itself a string, has no\n",
      "     |       children, or has more than one child, return value is None.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from bs4.element.Tag:\n",
      "     |\n",
      "     |  DEFAULT_INTERESTING_STRING_TYPES = (<class 'bs4.element.NavigableStrin...\n",
      "     |\n",
      "     |  EMPTY_ELEMENT_EVENT = <object object>\n",
      "     |\n",
      "     |  END_ELEMENT_EVENT = <object object>\n",
      "     |\n",
      "     |  START_ELEMENT_EVENT = <object object>\n",
      "     |\n",
      "     |  STRING_ELEMENT_EVENT = <object object>\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from bs4.element.PageElement:\n",
      "     |\n",
      "     |  append(self, tag)\n",
      "     |      Appends the given PageElement to the contents of this one.\n",
      "     |\n",
      "     |      :param tag: A PageElement.\n",
      "     |\n",
      "     |  extend(self, tags)\n",
      "     |      Appends the given PageElements to this one's contents.\n",
      "     |\n",
      "     |      :param tags: A list of PageElements. If a single Tag is\n",
      "     |          provided instead, this PageElement's contents will be extended\n",
      "     |          with that Tag's contents.\n",
      "     |\n",
      "     |  extract(self, _self_index=None)\n",
      "     |      Destructively rips this element out of the tree.\n",
      "     |\n",
      "     |      :param _self_index: The location of this element in its parent's\n",
      "     |         .contents, if known. Passing this in allows for a performance\n",
      "     |         optimization.\n",
      "     |\n",
      "     |      :return: `self`, no longer part of the tree.\n",
      "     |\n",
      "     |  fetchNextSiblings = find_next_siblings(self, name=None, attrs={}, string=None, limit=None, **kwargs)\n",
      "     |\n",
      "     |  fetchParents = find_parents(self, name=None, attrs={}, limit=None, **kwargs)\n",
      "     |\n",
      "     |  fetchPrevious = find_all_previous(self, name=None, attrs={}, string=None, limit=None, **kwargs)\n",
      "     |\n",
      "     |  fetchPreviousSiblings = find_previous_siblings(self, name=None, attrs={}, string=None, limit=None, **kwargs)\n",
      "     |\n",
      "     |  findAllNext = find_all_next(self, name=None, attrs={}, string=None, limit=None, **kwargs)\n",
      "     |\n",
      "     |  findAllPrevious = find_all_previous(self, name=None, attrs={}, string=None, limit=None, **kwargs)\n",
      "     |\n",
      "     |  findNext = find_next(self, name=None, attrs={}, string=None, **kwargs)\n",
      "     |\n",
      "     |  findNextSibling = find_next_sibling(self, name=None, attrs={}, string=None, **kwargs)\n",
      "     |\n",
      "     |  findNextSiblings = find_next_siblings(self, name=None, attrs={}, string=None, limit=None, **kwargs)\n",
      "     |\n",
      "     |  findParent = find_parent(self, name=None, attrs={}, **kwargs)\n",
      "     |\n",
      "     |  findParents = find_parents(self, name=None, attrs={}, limit=None, **kwargs)\n",
      "     |\n",
      "     |  findPrevious = find_previous(self, name=None, attrs={}, string=None, **kwargs)\n",
      "     |\n",
      "     |  findPreviousSibling = find_previous_sibling(self, name=None, attrs={}, string=None, **kwargs)\n",
      "     |\n",
      "     |  findPreviousSiblings = find_previous_siblings(self, name=None, attrs={}, string=None, limit=None, **kwargs)\n",
      "     |\n",
      "     |  find_all_next(self, name=None, attrs={}, string=None, limit=None, **kwargs)\n",
      "     |      Find all PageElements that match the given criteria and appear\n",
      "     |      later in the document than this PageElement.\n",
      "     |\n",
      "     |      All find_* methods take a common set of arguments. See the online\n",
      "     |      documentation for detailed explanations.\n",
      "     |\n",
      "     |      :param name: A filter on tag name.\n",
      "     |      :param attrs: A dictionary of filters on attribute values.\n",
      "     |      :param string: A filter for a NavigableString with specific text.\n",
      "     |      :param limit: Stop looking after finding this many results.\n",
      "     |      :kwargs: A dictionary of filters on attribute values.\n",
      "     |      :return: A ResultSet containing PageElements.\n",
      "     |\n",
      "     |  find_all_previous(self, name=None, attrs={}, string=None, limit=None, **kwargs)\n",
      "     |      Look backwards in the document from this PageElement and find all\n",
      "     |      PageElements that match the given criteria.\n",
      "     |\n",
      "     |      All find_* methods take a common set of arguments. See the online\n",
      "     |      documentation for detailed explanations.\n",
      "     |\n",
      "     |      :param name: A filter on tag name.\n",
      "     |      :param attrs: A dictionary of filters on attribute values.\n",
      "     |      :param string: A filter for a NavigableString with specific text.\n",
      "     |      :param limit: Stop looking after finding this many results.\n",
      "     |      :kwargs: A dictionary of filters on attribute values.\n",
      "     |      :return: A ResultSet of PageElements.\n",
      "     |      :rtype: bs4.element.ResultSet\n",
      "     |\n",
      "     |  find_next(self, name=None, attrs={}, string=None, **kwargs)\n",
      "     |      Find the first PageElement that matches the given criteria and\n",
      "     |      appears later in the document than this PageElement.\n",
      "     |\n",
      "     |      All find_* methods take a common set of arguments. See the online\n",
      "     |      documentation for detailed explanations.\n",
      "     |\n",
      "     |      :param name: A filter on tag name.\n",
      "     |      :param attrs: A dictionary of filters on attribute values.\n",
      "     |      :param string: A filter for a NavigableString with specific text.\n",
      "     |      :kwargs: A dictionary of filters on attribute values.\n",
      "     |      :return: A PageElement.\n",
      "     |      :rtype: bs4.element.Tag | bs4.element.NavigableString\n",
      "     |\n",
      "     |  find_next_sibling(self, name=None, attrs={}, string=None, **kwargs)\n",
      "     |      Find the closest sibling to this PageElement that matches the\n",
      "     |      given criteria and appears later in the document.\n",
      "     |\n",
      "     |      All find_* methods take a common set of arguments. See the\n",
      "     |      online documentation for detailed explanations.\n",
      "     |\n",
      "     |      :param name: A filter on tag name.\n",
      "     |      :param attrs: A dictionary of filters on attribute values.\n",
      "     |      :param string: A filter for a NavigableString with specific text.\n",
      "     |      :kwargs: A dictionary of filters on attribute values.\n",
      "     |      :return: A PageElement.\n",
      "     |      :rtype: bs4.element.Tag | bs4.element.NavigableString\n",
      "     |\n",
      "     |  find_next_siblings(self, name=None, attrs={}, string=None, limit=None, **kwargs)\n",
      "     |      Find all siblings of this PageElement that match the given criteria\n",
      "     |      and appear later in the document.\n",
      "     |\n",
      "     |      All find_* methods take a common set of arguments. See the online\n",
      "     |      documentation for detailed explanations.\n",
      "     |\n",
      "     |      :param name: A filter on tag name.\n",
      "     |      :param attrs: A dictionary of filters on attribute values.\n",
      "     |      :param string: A filter for a NavigableString with specific text.\n",
      "     |      :param limit: Stop looking after finding this many results.\n",
      "     |      :kwargs: A dictionary of filters on attribute values.\n",
      "     |      :return: A ResultSet of PageElements.\n",
      "     |      :rtype: bs4.element.ResultSet\n",
      "     |\n",
      "     |  find_parent(self, name=None, attrs={}, **kwargs)\n",
      "     |      Find the closest parent of this PageElement that matches the given\n",
      "     |      criteria.\n",
      "     |\n",
      "     |      All find_* methods take a common set of arguments. See the online\n",
      "     |      documentation for detailed explanations.\n",
      "     |\n",
      "     |      :param name: A filter on tag name.\n",
      "     |      :param attrs: A dictionary of filters on attribute values.\n",
      "     |      :kwargs: A dictionary of filters on attribute values.\n",
      "     |\n",
      "     |      :return: A PageElement.\n",
      "     |      :rtype: bs4.element.Tag | bs4.element.NavigableString\n",
      "     |\n",
      "     |  find_parents(self, name=None, attrs={}, limit=None, **kwargs)\n",
      "     |      Find all parents of this PageElement that match the given criteria.\n",
      "     |\n",
      "     |      All find_* methods take a common set of arguments. See the online\n",
      "     |      documentation for detailed explanations.\n",
      "     |\n",
      "     |      :param name: A filter on tag name.\n",
      "     |      :param attrs: A dictionary of filters on attribute values.\n",
      "     |      :param limit: Stop looking after finding this many results.\n",
      "     |      :kwargs: A dictionary of filters on attribute values.\n",
      "     |\n",
      "     |      :return: A PageElement.\n",
      "     |      :rtype: bs4.element.Tag | bs4.element.NavigableString\n",
      "     |\n",
      "     |  find_previous(self, name=None, attrs={}, string=None, **kwargs)\n",
      "     |      Look backwards in the document from this PageElement and find the\n",
      "     |      first PageElement that matches the given criteria.\n",
      "     |\n",
      "     |      All find_* methods take a common set of arguments. See the online\n",
      "     |      documentation for detailed explanations.\n",
      "     |\n",
      "     |      :param name: A filter on tag name.\n",
      "     |      :param attrs: A dictionary of filters on attribute values.\n",
      "     |      :param string: A filter for a NavigableString with specific text.\n",
      "     |      :kwargs: A dictionary of filters on attribute values.\n",
      "     |      :return: A PageElement.\n",
      "     |      :rtype: bs4.element.Tag | bs4.element.NavigableString\n",
      "     |\n",
      "     |  find_previous_sibling(self, name=None, attrs={}, string=None, **kwargs)\n",
      "     |      Returns the closest sibling to this PageElement that matches the\n",
      "     |      given criteria and appears earlier in the document.\n",
      "     |\n",
      "     |      All find_* methods take a common set of arguments. See the online\n",
      "     |      documentation for detailed explanations.\n",
      "     |\n",
      "     |      :param name: A filter on tag name.\n",
      "     |      :param attrs: A dictionary of filters on attribute values.\n",
      "     |      :param string: A filter for a NavigableString with specific text.\n",
      "     |      :kwargs: A dictionary of filters on attribute values.\n",
      "     |      :return: A PageElement.\n",
      "     |      :rtype: bs4.element.Tag | bs4.element.NavigableString\n",
      "     |\n",
      "     |  find_previous_siblings(self, name=None, attrs={}, string=None, limit=None, **kwargs)\n",
      "     |      Returns all siblings to this PageElement that match the\n",
      "     |      given criteria and appear earlier in the document.\n",
      "     |\n",
      "     |      All find_* methods take a common set of arguments. See the online\n",
      "     |      documentation for detailed explanations.\n",
      "     |\n",
      "     |      :param name: A filter on tag name.\n",
      "     |      :param attrs: A dictionary of filters on attribute values.\n",
      "     |      :param string: A filter for a NavigableString with specific text.\n",
      "     |      :param limit: Stop looking after finding this many results.\n",
      "     |      :kwargs: A dictionary of filters on attribute values.\n",
      "     |      :return: A ResultSet of PageElements.\n",
      "     |      :rtype: bs4.element.ResultSet\n",
      "     |\n",
      "     |  format_string(self, s, formatter)\n",
      "     |      Format the given string using the given formatter.\n",
      "     |\n",
      "     |      :param s: A string.\n",
      "     |      :param formatter: A Formatter object, or a string naming one of the standard formatters.\n",
      "     |\n",
      "     |  formatter_for_name(self, formatter)\n",
      "     |      Look up or create a Formatter for the given identifier,\n",
      "     |      if necessary.\n",
      "     |\n",
      "     |      :param formatter: Can be a Formatter object (used as-is), a\n",
      "     |          function (used as the entity substitution hook for an\n",
      "     |          XMLFormatter or HTMLFormatter), or a string (used to look\n",
      "     |          up an XMLFormatter or HTMLFormatter in the appropriate\n",
      "     |          registry.\n",
      "     |\n",
      "     |  getText = get_text(self, separator='', strip=False, types=<object object at 0x7ff48df5d490>)\n",
      "     |\n",
      "     |  get_text(self, separator='', strip=False, types=<object object at 0x7ff48df5d490>)\n",
      "     |      Get all child strings of this PageElement, concatenated using the\n",
      "     |      given separator.\n",
      "     |\n",
      "     |      :param separator: Strings will be concatenated using this separator.\n",
      "     |\n",
      "     |      :param strip: If True, strings will be stripped before being\n",
      "     |          concatenated.\n",
      "     |\n",
      "     |      :param types: A tuple of NavigableString subclasses. Any\n",
      "     |          strings of a subclass not found in this list will be\n",
      "     |          ignored. Although there are exceptions, the default\n",
      "     |          behavior in most cases is to consider only NavigableString\n",
      "     |          and CData objects. That means no comments, processing\n",
      "     |          instructions, etc.\n",
      "     |\n",
      "     |      :return: A string.\n",
      "     |\n",
      "     |  insert(self, position, new_child)\n",
      "     |      Insert a new PageElement in the list of this PageElement's children.\n",
      "     |\n",
      "     |      This works the same way as `list.insert`.\n",
      "     |\n",
      "     |      :param position: The numeric position that should be occupied\n",
      "     |         in `self.children` by the new PageElement.\n",
      "     |      :param new_child: A PageElement.\n",
      "     |\n",
      "     |  nextGenerator(self)\n",
      "     |      # Old non-property versions of the generators, for backwards\n",
      "     |      # compatibility with BS3.\n",
      "     |\n",
      "     |  nextSiblingGenerator(self)\n",
      "     |\n",
      "     |  parentGenerator(self)\n",
      "     |\n",
      "     |  previousGenerator(self)\n",
      "     |\n",
      "     |  previousSiblingGenerator(self)\n",
      "     |\n",
      "     |  replaceWith = replace_with(self, *args)\n",
      "     |\n",
      "     |  replaceWithChildren = unwrap(self)\n",
      "     |\n",
      "     |  replace_with(self, *args)\n",
      "     |      Replace this PageElement with one or more PageElements, keeping the\n",
      "     |      rest of the tree the same.\n",
      "     |\n",
      "     |      :param args: One or more PageElements.\n",
      "     |      :return: `self`, no longer part of the tree.\n",
      "     |\n",
      "     |  replace_with_children = unwrap(self)\n",
      "     |\n",
      "     |  setup(self, parent=None, previous_element=None, next_element=None, previous_sibling=None, next_sibling=None)\n",
      "     |      Sets up the initial relations between this element and\n",
      "     |      other elements.\n",
      "     |\n",
      "     |      :param parent: The parent of this element.\n",
      "     |\n",
      "     |      :param previous_element: The element parsed immediately before\n",
      "     |          this one.\n",
      "     |\n",
      "     |      :param next_element: The element parsed immediately before\n",
      "     |          this one.\n",
      "     |\n",
      "     |      :param previous_sibling: The most recently encountered element\n",
      "     |          on the same level of the parse tree as this one.\n",
      "     |\n",
      "     |      :param previous_sibling: The next element to be encountered\n",
      "     |          on the same level of the parse tree as this one.\n",
      "     |\n",
      "     |  unwrap(self)\n",
      "     |      Replace this PageElement with its contents.\n",
      "     |\n",
      "     |      :return: `self`, no longer part of the tree.\n",
      "     |\n",
      "     |  wrap(self, wrap_inside)\n",
      "     |      Wrap this PageElement inside another one.\n",
      "     |\n",
      "     |      :param wrap_inside: A PageElement.\n",
      "     |      :return: `wrap_inside`, occupying the position in the tree that used\n",
      "     |         to be occupied by `self`, and with `self` inside it.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from bs4.element.PageElement:\n",
      "     |\n",
      "     |  decomposed\n",
      "     |      Check whether a PageElement has been decomposed.\n",
      "     |\n",
      "     |      :rtype: bool\n",
      "     |\n",
      "     |  next\n",
      "     |      The PageElement, if any, that was parsed just after this one.\n",
      "     |\n",
      "     |      :return: A PageElement.\n",
      "     |      :rtype: bs4.element.Tag | bs4.element.NavigableString\n",
      "     |\n",
      "     |  next_elements\n",
      "     |      All PageElements that were parsed after this one.\n",
      "     |\n",
      "     |      :yield: A sequence of PageElements.\n",
      "     |\n",
      "     |  next_siblings\n",
      "     |      All PageElements that are siblings of this one but were parsed\n",
      "     |      later.\n",
      "     |\n",
      "     |      :yield: A sequence of PageElements.\n",
      "     |\n",
      "     |  parents\n",
      "     |      All PageElements that are parents of this PageElement.\n",
      "     |\n",
      "     |      :yield: A sequence of PageElements.\n",
      "     |\n",
      "     |  previous\n",
      "     |      The PageElement, if any, that was parsed just before this one.\n",
      "     |\n",
      "     |      :return: A PageElement.\n",
      "     |      :rtype: bs4.element.Tag | bs4.element.NavigableString\n",
      "     |\n",
      "     |  previous_elements\n",
      "     |      All PageElements that were parsed before this one.\n",
      "     |\n",
      "     |      :yield: A sequence of PageElements.\n",
      "     |\n",
      "     |  previous_siblings\n",
      "     |      All PageElements that are siblings of this one but were parsed\n",
      "     |      earlier.\n",
      "     |\n",
      "     |      :yield: A sequence of PageElements.\n",
      "     |\n",
      "     |  stripped_strings\n",
      "     |      Yield all strings in this PageElement, stripping them first.\n",
      "     |\n",
      "     |      :yield: A sequence of stripped strings.\n",
      "     |\n",
      "     |  text\n",
      "     |      Get all child strings of this PageElement, concatenated using the\n",
      "     |      given separator.\n",
      "     |\n",
      "     |      :param separator: Strings will be concatenated using this separator.\n",
      "     |\n",
      "     |      :param strip: If True, strings will be stripped before being\n",
      "     |          concatenated.\n",
      "     |\n",
      "     |      :param types: A tuple of NavigableString subclasses. Any\n",
      "     |          strings of a subclass not found in this list will be\n",
      "     |          ignored. Although there are exceptions, the default\n",
      "     |          behavior in most cases is to consider only NavigableString\n",
      "     |          and CData objects. That means no comments, processing\n",
      "     |          instructions, etc.\n",
      "     |\n",
      "     |      :return: A string.\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from bs4.element.PageElement:\n",
      "     |\n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables\n",
      "     |\n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object\n",
      "     |\n",
      "     |  nextSibling\n",
      "     |\n",
      "     |  previousSibling\n",
      "     |\n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from bs4.element.PageElement:\n",
      "     |\n",
      "     |  default = <object object>\n",
      "     |\n",
      "     |  known_xml = None\n",
      "\n",
      "DATA\n",
      "    __all__ = ['BeautifulSoup']\n",
      "    __copyright__ = 'Copyright (c) 2004-2024 Leonard Richardson'\n",
      "    __license__ = 'MIT'\n",
      "\n",
      "VERSION\n",
      "    4.12.3\n",
      "\n",
      "AUTHOR\n",
      "    Leonard Richardson (leonardr@segfault.org)\n",
      "\n",
      "FILE\n",
      "    /home/commi/venv/venv3.12/lib/python3.12/site-packages/bs4/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "\n",
    "help(bs4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba42e4c-fd55-4da4-8e86-362824721564",
   "metadata": {},
   "source": [
    "## Running BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666298a8-4134-4434-8912-8464e6c11456",
   "metadata": {},
   "source": [
    "### `html.parser`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d068026-47d2-4ffa-948b-51ec1f6dabf2",
   "metadata": {},
   "source": [
    "The most commonly used object in the BeautifulSoup library is, appropriately, the `BeautifulSoup object`. Let’s take a look at it in action, modifying the example found in the beginning of this chapter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67b87c7e-95de-4797-909c-5b5463f6a71b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T10:45:53.489819Z",
     "iopub.status.busy": "2024-01-18T10:45:53.489459Z",
     "iopub.status.idle": "2024-01-18T10:45:54.790031Z",
     "shell.execute_reply": "2024-01-18T10:45:54.789002Z",
     "shell.execute_reply.started": "2024-01-18T10:45:53.489792Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com/pages/page1.html')\n",
    "bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "print(bs.h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50d52a0-e4b3-4b0e-bfcc-8853efaa3d98",
   "metadata": {},
   "source": [
    "Note that this returns only the first instance of the `h1` tag found on the page. By convention, only one `h1` tag should be used on a single page, but conventions are often broken on the web, so you should be aware that this will retrieve the first instance of the tag only, and not necessarily the one that you’re looking for.\n",
    "\n",
    "As in previous web scraping examples, you are importing the `urlopen` function and calling `html.read()` in order to get the HTML content of the page. In addition to the text string, BeautifulSoup can also use the file object directly returned by `urlopen`, without needing to call `.read()` first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6d5c78c-16b1-450a-990a-0ef8692995db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T10:47:41.369952Z",
     "iopub.status.busy": "2024-01-18T10:47:41.367552Z",
     "iopub.status.idle": "2024-01-18T10:47:42.399272Z",
     "shell.execute_reply": "2024-01-18T10:47:42.396644Z",
     "shell.execute_reply.started": "2024-01-18T10:47:41.369834Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com/pages/page1.html')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "print(bs.h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d58595d-3a8c-446b-b6d4-c4dc76f3af05",
   "metadata": {},
   "source": [
    "This HTML content is then transformed into a BeautifulSoup object, with the following structure:\n",
    "```html\n",
    "html → <html><head>...</head><body>...</body></html>\n",
    "    head → <head><title>A Useful Page</title></head>\n",
    "        title → <title>A Useful Page</title>\n",
    "    body → <body><h1>An Int...</h1><div>Lorem ip...</div></body>\n",
    "        h1 → <h1>An Interesting Title</h1>\n",
    "        div → <div>Lorem Ipsum dolor...</div>\n",
    "```\n",
    "\n",
    "Note that the `h1` tag that you extract from the page is nested two layers deep into your BeautifulSoup object structure (`html → body → h1`). However, when you actually fetch it from the object, you call the `h1` tag directly:\n",
    "\n",
    "```python\n",
    "bs.h1\n",
    "```\n",
    "\n",
    "In fact, any of the following function calls would produce the same output:\n",
    "\n",
    "```python\n",
    "bs.html.body.h1\n",
    "bs.body.h1\n",
    "bs.html.h1\n",
    "```\n",
    "\n",
    "When you create a BeautifulSoup object, two arguments are passed in:\n",
    "\n",
    "```python\n",
    "bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "```\n",
    "\n",
    "The first is the HTML text the object is based on, and the second specifies the parser that you want BeautifulSoup to use in order to create that object. In the majority of cases, it makes no difference which parser you choose.\n",
    "\n",
    "- `html.parser` is a parser that is included with Python 3 and requires no extra installations in order to use. Except where required, we will use this parser throughout the book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f204edfe-4014-4934-84aa-f9033145810c",
   "metadata": {},
   "source": [
    "### `lxml`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254ae385-ee04-4c96-bb4c-c466d7cc48e0",
   "metadata": {},
   "source": [
    "Another popular parser is `lxml`. This can be installed through pip:\n",
    "\n",
    "```sh\n",
    "$ pip3 install lxml\n",
    "```\n",
    "\n",
    "`lxml` can be used with BeautifulSoup by changing the parser string provided:\n",
    "\n",
    "```python\n",
    "bs = BeautifulSoup(html.read(), 'lxml')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e537c93b-6e5f-4fc0-93be-89cdef3bfba0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T10:54:08.346019Z",
     "iopub.status.busy": "2024-01-18T10:54:08.345070Z",
     "iopub.status.idle": "2024-01-18T10:54:09.378552Z",
     "shell.execute_reply": "2024-01-18T10:54:09.375877Z",
     "shell.execute_reply.started": "2024-01-18T10:54:08.345934Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com/pages/page1.html')\n",
    "bs = BeautifulSoup(html.read(), 'lxml')\n",
    "print(bs.h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a57941-1f99-42de-aeea-9601f8523995",
   "metadata": {},
   "source": [
    "> `lxml` has some advantages over `html.parser` in that it is generally better at parsing “messy” or malformed HTML code. \n",
    "\n",
    "It is forgiving and fixes problems like unclosed tags, tags that are improperly nested, and missing head or body tags. It is also somewhat faster than `html.parser`, although speed is not necessarily an advantage in web scraping, given that the speed of the network itself will almost always be your largest bottleneck.\n",
    "\n",
    "One of the disadvantages of `lxml` is that it has to be installed separately and depends on third-party C libraries to function. This can cause problems for portability and ease of use, compared to `html.parser`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7810da38-3389-433b-9eda-3a28db863edf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `html5lib`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e25e171-3d19-45e9-b9bc-d3c8c33d1b42",
   "metadata": {},
   "source": [
    "Another popular HTML parser is `html5lib`. \n",
    "\n",
    "Like `lxml`, `html5lib` is an extremely forgiving parser that takes even more initiative correcting broken HTML. It also depends on an external dependency, and is slower than both `lxml` and `html.parser`. Despite this, it may be a good choice if you are working with messy or handwritten HTML sites.\n",
    "\n",
    "It can be used by installing and passing the string `html5lib` to the BeautifulSoup object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3782614b-48fe-4365-9642-ed745b44bd5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T10:57:56.587036Z",
     "iopub.status.busy": "2024-01-18T10:57:56.586618Z",
     "iopub.status.idle": "2024-01-18T10:57:57.797985Z",
     "shell.execute_reply": "2024-01-18T10:57:57.796382Z",
     "shell.execute_reply.started": "2024-01-18T10:57:56.586991Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com/pages/page1.html')\n",
    "bs = BeautifulSoup(html.read(), 'html5lib')\n",
    "print(bs.h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d21ccb-e19a-4c15-917d-7b0c1831a69d",
   "metadata": {},
   "source": [
    "Virtually any information can be extracted from any HTML (or XML) file, as long as it has an identifying tag surrounding it or near it. Chapter 2 delves more deeply into more-complex BeautifulSoup function calls, and presents regular expressions and how they can be used with BeautifulSoup in order to extract information from websites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aa20e2-8406-4e54-85d3-e2f680ad6494",
   "metadata": {},
   "source": [
    "### `html` or `html.read()`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690e8481-08d7-47bc-a99d-fec61fdaf051",
   "metadata": {},
   "source": [
    "ChatGPT:\n",
    "\n",
    "When you use `html = urlopen(url)`, it returns a response object which contains the HTML content. You can directly pass this response object to the BeautifulSoup constructor to parse the HTML content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5571af12-8e00-48d3-b19a-aecd21f894bd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1.3 Connecting Reliably and Handling Exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59c18a8-cb5e-406a-a666-03639dac3966",
   "metadata": {},
   "source": [
    "## Server error Exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7c07a8-0d2f-4e74-a5aa-9b46951c42c9",
   "metadata": {},
   "source": [
    "Let’s take a look at the first line of our scraper, after the import statements, and figure out how to handle any exceptions this might throw:\n",
    "\n",
    "```python\n",
    "html = urlopen('http://www.pythonscraping.com/pages/page1.html')\n",
    "```\n",
    "\n",
    "Two main things can go wrong in this line:\n",
    "- The page is not found on the server (or there was an error in retrieving it):\n",
    "    - an `HTTPError` will be returned (404 Page Not Found,” “500 Internal Server Error,” and so forth);\n",
    "- The server is not found:\n",
    "    - urlopen will throw an `URLError` - no server could be reached at all, and, because the remote server is responsible for returning HTTP status codes, an `HTTPError` cannot be thrown, and the more serious URLError must be caught."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbb72b2-fa06-4f1a-8b9c-dad30252cb59",
   "metadata": {},
   "source": [
    "Author's solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9774b147-b967-44a4-b4e0-d297674d4ff4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T11:12:44.537654Z",
     "iopub.status.busy": "2024-01-18T11:12:44.534762Z",
     "iopub.status.idle": "2024-01-18T11:12:46.957442Z",
     "shell.execute_reply": "2024-01-18T11:12:46.955070Z",
     "shell.execute_reply.started": "2024-01-18T11:12:44.537543Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: Not Found\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "try:\n",
    "    html = urlopen('http://www.pythonscraping.com/page/page1.html')\n",
    "except HTTPError as e:\n",
    "    print(e)\n",
    "    # return null, break, or do some other \"Plan B\"\n",
    "else:\n",
    "    # program continues. Note: If you return or break in the\n",
    "    # exception catch, you do not need to use the \"else\" statement\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29d81881-4c96-4a9c-acd3-4367bf937ebb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T10:15:31.252027Z",
     "iopub.status.busy": "2024-01-21T10:15:31.250503Z",
     "iopub.status.idle": "2024-01-21T10:15:34.117165Z",
     "shell.execute_reply": "2024-01-21T10:15:34.113585Z",
     "shell.execute_reply.started": "2024-01-21T10:15:31.251895Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: Not Found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# my simple solution\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def main():\n",
    "    bad_url = 'http://www.pythonscraping.com/page/page1.html'\n",
    "    try:\n",
    "        html = urlopen(bad_url)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 1\n",
    "\n",
    "    bs = BeautifulSoup(html.read(), 'html5lib')\n",
    "    print(bs.h1)\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e1d4d17-e196-4eed-8ca1-4f4786b46a50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T11:14:31.487689Z",
     "iopub.status.busy": "2024-01-18T11:14:31.486629Z",
     "iopub.status.idle": "2024-01-18T11:14:31.558862Z",
     "shell.execute_reply": "2024-01-18T11:14:31.556807Z",
     "shell.execute_reply.started": "2024-01-18T11:14:31.487587Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server could not be found!\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "\n",
    "try:\n",
    "    html = urlopen('https://pythonscrapingthisurldoesnotexist.com')\n",
    "except HTTPError as e:\n",
    "    print(e)\n",
    "except URLError as e:\n",
    "    print('The server could not be found!')\n",
    "else:\n",
    "    print('It Worked!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af772ca-62d3-485c-980d-398a3d747a57",
   "metadata": {},
   "source": [
    "## Tag errors - `None`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97fb958-fc28-40bc-9f5a-bb83113f73d2",
   "metadata": {},
   "source": [
    "Every time you access a tag in a BeautifulSoup object, it’s smart to add a check to make sure the tag actually exists. If you attempt to access a tag that does not exist, BeautifulSoup will return a `None` object. The problem is, attempting to access a tag on a `None` object itself will result in an `AttributeError` being thrown:\n",
    "\n",
    "```python\n",
    "AttributeError: 'NoneType' object has no attribute 'someTag'\n",
    "```\n",
    "\n",
    "The easiest way is to explicitly check for both situations:\n",
    "\n",
    "```python\n",
    "try:\n",
    "    badContent = bs.nonExistingTag.anotherTag\n",
    "except AttributeError as e:\n",
    "    print('Tag was not found')\n",
    "else:\n",
    "    if badContent == None:\n",
    "        print ('Tag was not found')\n",
    "    else:\n",
    "        print(badContent)\n",
    "```\n",
    "\n",
    "This checking and handling of every error does seem laborious at first, but it’s easy to add a little reorganization to this code to make it less difficult to write (and, more important, much less difficult to read). This code, for example, is our same scraper written in a slightly different way:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768c9e5c-3ed9-449a-89aa-f30ad0394b7d",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49e8cf6-1e60-496b-8257-cf6a8c4f46b2",
   "metadata": {},
   "source": [
    "1. Working url:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac902d28-6a70-4464-abf4-6a5c9b10b220",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T10:30:37.932790Z",
     "iopub.status.busy": "2024-01-21T10:30:37.931360Z",
     "iopub.status.idle": "2024-01-21T10:30:38.927340Z",
     "shell.execute_reply": "2024-01-21T10:30:38.926418Z",
     "shell.execute_reply.started": "2024-01-21T10:30:37.932740Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    working_url = 'http://www.pythonscraping.com/pages/page1.html'\n",
    "\n",
    "    title = get_title(working_url)\n",
    "    if title == None:\n",
    "        print('Title could not be found')\n",
    "    else:\n",
    "        print(title)\n",
    "\n",
    "\n",
    "def get_title(url):\n",
    "    try:\n",
    "        html = urlopen(url)\n",
    "    except HTTPError as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    try:\n",
    "        bs = BeautifulSoup(html, 'html.parser')\n",
    "        title = bs.body.h1\n",
    "    except AttributeError as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    return title\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1abc4a-9dfd-4c20-92ae-aa5fe075aca6",
   "metadata": {},
   "source": [
    "2. Bad url:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b09b5b1-5e30-4eec-add5-9f1ccd1ef700",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T10:31:29.952273Z",
     "iopub.status.busy": "2024-01-21T10:31:29.951627Z",
     "iopub.status.idle": "2024-01-21T10:31:32.168740Z",
     "shell.execute_reply": "2024-01-21T10:31:32.165272Z",
     "shell.execute_reply.started": "2024-01-21T10:31:29.952213Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: Not Found\n",
      "Title could not be found\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    bad_url = 'http://www.pythonscraping.com/page/page1.html'\n",
    "\n",
    "    title = get_title(bad_url)    # modified\n",
    "    if title == None:\n",
    "        print('Title could not be found')\n",
    "    else:\n",
    "        print(title)\n",
    "\n",
    "\n",
    "def get_title(url):\n",
    "    try:\n",
    "        html = urlopen(url)\n",
    "    except HTTPError as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    try:\n",
    "        bs = BeautifulSoup(html, 'html.parser')\n",
    "        title = bs.body.h1\n",
    "    except AttributeError as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    return title\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f47952f-dc17-4813-9bdf-af8d88400666",
   "metadata": {},
   "source": [
    "3. Pass `None` to the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c10b1224-7d76-435f-bdad-996ba959dc12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T10:31:52.998975Z",
     "iopub.status.busy": "2024-01-21T10:31:52.996958Z",
     "iopub.status.idle": "2024-01-21T10:31:53.984565Z",
     "shell.execute_reply": "2024-01-21T10:31:53.981535Z",
     "shell.execute_reply.started": "2024-01-21T10:31:52.998896Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'p'\n",
      "Title could not be found\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    working_url = 'http://www.pythonscraping.com/pages/page1.html'\n",
    "\n",
    "    title = get_title(working_url)\n",
    "    if title == None:\n",
    "        print('Title could not be found')\n",
    "    else:\n",
    "        print(title)\n",
    "\n",
    "\n",
    "def get_title(url):\n",
    "    try:\n",
    "        html = urlopen(url)\n",
    "    except HTTPError as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    try:\n",
    "        bs = BeautifulSoup(html, 'html.parser')\n",
    "        title = bs.body.h2.p    # modified: there is no h2 tag and we try to get h2.p\n",
    "    except AttributeError as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    return title\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe64b6b-8380-42ab-9b56-00df866a00c4",
   "metadata": {},
   "source": [
    "4. Or even more simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac7a695a-2e8e-4a92-a2dd-2026be216a8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T10:32:31.410089Z",
     "iopub.status.busy": "2024-01-21T10:32:31.409368Z",
     "iopub.status.idle": "2024-01-21T10:32:32.458269Z",
     "shell.execute_reply": "2024-01-21T10:32:32.455439Z",
     "shell.execute_reply.started": "2024-01-21T10:32:31.410052Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'p'\n",
      "Title could not be found\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    working_url = 'http://www.pythonscraping.com/pages/page1.html'\n",
    "\n",
    "    title = get_title(working_url)\n",
    "    if title == None:\n",
    "        print('Title could not be found')\n",
    "    else:\n",
    "        print(title)\n",
    "\n",
    "\n",
    "def get_title(url):\n",
    "    try:\n",
    "        html = urlopen(url)\n",
    "    except Exception as e:    # modified\n",
    "        print(e)\n",
    "        return None\n",
    "    try:\n",
    "        bs = BeautifulSoup(html, 'html.parser')\n",
    "        title = bs.body.h2.p\n",
    "    except Exception as e:    # modified\n",
    "        print(e)\n",
    "        return None\n",
    "    return title\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e8e5ff-558f-4958-b80b-7b474e872605",
   "metadata": {},
   "source": [
    "When writing scrapers, it’s important to think about the overall pattern of your code in order to handle exceptions and make it readable at the same time. \n",
    "\n",
    "You’ll also likely want to heavily reuse code. Having generic functions such as `get_site_html` and `get_title` (complete with thorough exception handling) makes it easy to quickly — and reliably — scrape the web."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2ff299-3cdc-4adf-8f6c-c91a2142f352",
   "metadata": {},
   "source": [
    "# <b>2. Advanced HTML Parsing</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a50aec0-865e-4702-a526-83d96c16a395",
   "metadata": {},
   "source": [
    "In this section, we’ll discuss \n",
    "- searching for tags by attributes, \n",
    "- working with lists of tags, and \n",
    "- navigating parse trees.\n",
    "\n",
    "Keep in mind that layering the techniques used in this section with reckless abandon can lead to code that is difficult to debug, fragile, or both. Before getting started, let’s take a look at some of the ways you can avoid altogether the need for advanced HTML parsing!\n",
    "\n",
    "- Look for a “Print This Page” link, or perhaps a mobile version of the site that has better-formatted HTML (more on presenting yourself as a mobile device — and receiving mobile site versions — in **Chapter 14**).\n",
    "- Look for the information hidden in a JavaScript file. Remember, you might need to examine the imported JavaScript files in order to do this. For example, I once collected street addresses (along with latitude and longitude) off a website in a neatly formatted array by looking at the JavaScript for the embedded Google Map that displayed a pinpoint over each address.\n",
    "- This is more common for page titles, but the information might be available in the URL of the page itself.\n",
    "- If the information you are looking for is unique to this website for some reason, you’re out of luck. If not, try to think of other sources you could get this information from. Is there another website with the same data? Is this website displaying data that it scraped or aggregated from another website?\n",
    "\n",
    "Especially when faced with buried or poorly formatted data, it’s important not to just start digging and write yourself into a hole that you might not be able to get out of. \n",
    "\n",
    "> Take a deep breath and think of alternatives.\n",
    "\n",
    "If you’re certain no alternatives exist, the rest of this chapter explains standard and creative ways of selecting tags based on their position, context, attributes, and contents. The techniques presented here, when used correctly, will go a long way toward writing more stable and reliable web crawlers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608271f6-6c8f-4628-94ca-bed20d79ba81",
   "metadata": {},
   "source": [
    "# 2.1 CSS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ffd3ed-b72b-465f-8cd2-4bd3fc2f9b77",
   "metadata": {},
   "source": [
    "CSS relies on the differentiation of HTML elements that might otherwise have the exact same markup in order to style them differently. Some tags might look like this:\n",
    "\n",
    "```html\n",
    "<span class=\"green\"></span>\n",
    "```\n",
    "\n",
    "Others look like this:\n",
    "\n",
    "```html\n",
    "<span class=\"red\"></span>\n",
    "```\n",
    "\n",
    "Web scrapers can easily separate these two tags based on their class; for example, they might use BeautifulSoup to grab all the red text but none of the green text. Because CSS relies on these identifying attributes to style sites appropriately, you are almost guaranteed that these class and ID attributes will be plentiful on most modern websites.\n",
    "\n",
    "Let’s create an example web scraper that scrapes the page located at http://www.pythonscraping.com/pages/warandpeace.html.\n",
    "\n",
    "On this page, the lines spoken by characters in the story are in red, whereas the names of characters are in green. You can see the span tags, which reference the appropriate CSS classes, in the following sample of the page’s source code:\n",
    "\n",
    "```html\n",
    "<span class=\"red\">Heavens! what a virulent attack!</span> replied\n",
    "<span class=\"green\">the prince</span>, not in the least disconcerted\n",
    "by this reception.\n",
    "```\n",
    "\n",
    "You can grab the entire page and create a BeautifulSoup object with it by using a program similar to the one used in **Chapter 1**. Using this BeautifulSoup object, you can use the `find_all` function to extract a Python list of proper nouns found by selecting only the text within `<span class=\"green\"></span>` tags (`find_all` is an extremely flexible function you’ll be using a lot later in this book):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9276d428-068a-4b8e-b71e-743cf65d7b46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T14:27:35.381270Z",
     "iopub.status.busy": "2024-01-18T14:27:35.380375Z",
     "iopub.status.idle": "2024-01-18T14:27:36.381125Z",
     "shell.execute_reply": "2024-01-18T14:27:36.378973Z",
     "shell.execute_reply.started": "2024-01-18T14:27:35.381220Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna\n",
      "Pavlovna Scherer\n",
      "Empress Marya\n",
      "Fedorovna\n",
      "Prince Vasili Kuragin\n",
      "Anna Pavlovna\n",
      "St. Petersburg\n",
      "the prince\n",
      "Anna Pavlovna\n",
      "Anna Pavlovna\n",
      "the prince\n",
      "the prince\n",
      "the prince\n",
      "Prince Vasili\n",
      "Anna Pavlovna\n",
      "Anna Pavlovna\n",
      "the prince\n",
      "Wintzingerode\n",
      "King of Prussia\n",
      "le Vicomte de Mortemart\n",
      "Montmorencys\n",
      "Rohans\n",
      "Abbe Morio\n",
      "the Emperor\n",
      "the prince\n",
      "Prince Vasili\n",
      "Dowager Empress Marya Fedorovna\n",
      "the baron\n",
      "Anna Pavlovna\n",
      "the Empress\n",
      "the Empress\n",
      "Anna Pavlovna's\n",
      "Her Majesty\n",
      "Baron\n",
      "Funke\n",
      "The prince\n",
      "Anna\n",
      "Pavlovna\n",
      "the Empress\n",
      "The prince\n",
      "Anatole\n",
      "the prince\n",
      "The prince\n",
      "Anna\n",
      "Pavlovna\n",
      "Anna Pavlovna\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('http://www.pythonscraping.com/pages/warandpeace.html')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "name_list = bs.find_all('span', {'class': 'green'})\n",
    "for name in name_list:\n",
    "    print(name.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbd0a25-115e-464b-bcdb-0cd98cc2a36f",
   "metadata": {},
   "source": [
    "When run, it should list all the proper nouns in the text, in the order they appear in **\"War and Peace\"**. So what’s going on here? \n",
    "\n",
    "Previously, you’ve called `bs.tagName` to get the first occurrence of that tag on the page. Now, you’re calling `bs.find_all(tagName, tagAttributes)` to get a list of all of the tags on the page, rather than just the first.\n",
    "\n",
    "After getting a list of names, the program iterates through all names in the list, and prints `name.get_text()` in order to separate the content from the tags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fcae6a-2213-43c8-b313-bf2457e5a8a5",
   "metadata": {},
   "source": [
    "## When To `get_text()` And When To Preserve Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ad2fbb-574e-4bcf-9a4b-c4e0fcee6439",
   "metadata": {},
   "source": [
    "`.get_text()` strips all tags from the document you are working with and returns a Unicode string containing the text only. For example, if you are working with a large block of text that contains many hyperlinks, paragraphs, and other tags, all those will be stripped away, and you’ll be left with a tagless block of text.\n",
    "\n",
    "Keep in mind that it’s much easier to find what you’re looking for in a BeautifulSoup object than in a block of text. Calling `.get_text()` should always be the last thing you do, immediately before you print, store, or manipulate your final data. In general, \n",
    "\n",
    "> you should try to preserve the tag structure of a document as long as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8de5ccf-020d-43c5-a523-d004988d09ea",
   "metadata": {},
   "source": [
    "## `find()` and `find_all()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66725378-1432-4121-a063-a629519aefcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T14:59:36.404319Z",
     "iopub.status.busy": "2024-01-18T14:59:36.401828Z",
     "iopub.status.idle": "2024-01-18T14:59:36.415287Z",
     "shell.execute_reply": "2024-01-18T14:59:36.413041Z",
     "shell.execute_reply.started": "2024-01-18T14:59:36.404193Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method find_all in module bs4.element:\n",
      "\n",
      "find_all(name=None, attrs={}, recursive=True, string=None, limit=None, **kwargs) method of bs4.BeautifulSoup instance\n",
      "    Look in the children of this PageElement and find all\n",
      "    PageElements that match the given criteria.\n",
      "\n",
      "    All find_* methods take a common set of arguments. See the online\n",
      "    documentation for detailed explanations.\n",
      "\n",
      "    :param name: A filter on tag name.\n",
      "    :param attrs: A dictionary of filters on attribute values.\n",
      "    :param recursive: If this is True, find_all() will perform a\n",
      "        recursive search of this PageElement's children. Otherwise,\n",
      "        only the direct children will be considered.\n",
      "    :param limit: Stop looking after finding this many results.\n",
      "    :kwargs: A dictionary of filters on attribute values.\n",
      "    :return: A ResultSet of PageElements.\n",
      "    :rtype: bs4.element.ResultSet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(bs.find_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e3b8f87-4082-4683-a566-4d912b8611e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T15:00:33.066489Z",
     "iopub.status.busy": "2024-01-18T15:00:33.065021Z",
     "iopub.status.idle": "2024-01-18T15:00:33.076173Z",
     "shell.execute_reply": "2024-01-18T15:00:33.073717Z",
     "shell.execute_reply.started": "2024-01-18T15:00:33.066427Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method find in module bs4.element:\n",
      "\n",
      "find(name=None, attrs={}, recursive=True, string=None, **kwargs) method of bs4.BeautifulSoup instance\n",
      "    Look in the children of this PageElement and find the first\n",
      "    PageElement that matches the given criteria.\n",
      "\n",
      "    All find_* methods take a common set of arguments. See the online\n",
      "    documentation for detailed explanations.\n",
      "\n",
      "    :param name: A filter on tag name.\n",
      "    :param attrs: A dictionary of filters on attribute values.\n",
      "    :param recursive: If this is True, find() will perform a\n",
      "        recursive search of this PageElement's children. Otherwise,\n",
      "        only the direct children will be considered.\n",
      "    :param limit: Stop looking after finding this many results.\n",
      "    :kwargs: A dictionary of filters on attribute values.\n",
      "    :return: A PageElement.\n",
      "    :rtype: bs4.element.Tag | bs4.element.NavigableString\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(bs.find)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948b3758-7cc9-42a8-9149-73d6d83cf318",
   "metadata": {},
   "source": [
    "```python\n",
    "find_all(name=None, attrs={}, recursive=True, string=None, limit=None, **kwargs)\n",
    "find(name=None, attrs={}, recursive=True, string=None, **kwargs)\n",
    "```\n",
    "\n",
    "1. The `name` argument is one that you’ve seen before; you can pass a string name of a tag or even a Python list of string tag names. For example, the following returns a list of all the header tags in a document:\n",
    "\n",
    "```python\n",
    ".find_all(['h1','h2','h3','h4','h5','h6'])\n",
    "```\n",
    "\n",
    "- The `attributes` argument takes a Python dictionary of attributes and matches tags that contain any one of those attributes. For example, the following function would return both the green and red span tags in the HTML document:\n",
    "\n",
    "```python\n",
    ".find_all('span', {'class':{'green', 'red'}})\n",
    "```\n",
    "\n",
    "- The `recursive` argument is a boolean. How deeply into the document do you want to go? \n",
    "    - If recursive is set to `True`, the `find_all` function looks into children, and children’s children, for tags that match your parameters. \n",
    "    - If it is `False`, it will look only at the top-level tags in your document.\n",
    "<br>By default, `find_all` works recursively (`recursive` is set to `True`); it’s generally a good idea to leave this as is, unless you really know what you need to do and performance is an issue.<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a1064e-c14a-4f11-a208-5840b532f3ab",
   "metadata": {},
   "source": [
    "### `string` argument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638046b7-30d1-4c30-b013-568fd378bc37",
   "metadata": {},
   "source": [
    "- The `string` argument is unusual in that it matches based on the text content of the tags, rather than properties of the tags themselves. For instance, if you want to find the number of times “the prince” is surrounded by tags on the example page, you could replace your `.find_all()` function in the previous example with the following lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e24c8db5-491f-4b4b-8c2f-bd216e48bd6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T15:09:53.561174Z",
     "iopub.status.busy": "2024-01-18T15:09:53.559960Z",
     "iopub.status.idle": "2024-01-18T15:09:54.382482Z",
     "shell.execute_reply": "2024-01-18T15:09:54.381072Z",
     "shell.execute_reply.started": "2024-01-18T15:09:53.561054Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    html = urlopen('http://www.pythonscraping.com/pages/warandpeace.html')\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    name_list = bs.find_all(string='the prince')\n",
    "    print(len(name_list))\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3b2c30-a997-448d-9dbb-117d263484e1",
   "metadata": {},
   "source": [
    "- The `limit` argument, of course, is used only in the `find_all` method; `find` is equivalent to the same `find_all` call, with a limit of `1`. You might set this if you’re interested only in retrieving the first `x` items from the page. Be aware, however, that this gives you the first items on the page in the order that they occur, not necessarily the first ones that you want.\n",
    "\n",
    "- The `**kwargs` argument allows you to select tags that contain a particular attribute or set of attributes. For example:\n",
    "\n",
    "```python\n",
    "title = bs.find_all(id='title', class_='text')\n",
    "```\n",
    "\n",
    "This returns the first tag with the word “text” in the `class_` attribute and “title” in the `id` attribute. Note that, by convention, each value for an `id` should be used only once on the page. Therefore, in practice, a line like this may not be particularly useful, and should be equivalent to the following:\n",
    "\n",
    "```python\n",
    "title = bs.find(id='title')\n",
    "```\n",
    "\n",
    "_ChatGPT:_  \n",
    "The `id` and `class_` attributes are commonly used in HTML to uniquely identify an element (`id`) or to categorize elements (`class`). These attributes come from the HTML code of the page being parsed.\n",
    "\n",
    "The `id` attribute should be unique for each element on a page, and it's typically used to identify a specific element. The `class` attribute can be used to apply similar styling or behavior to multiple elements.\n",
    "\n",
    "In practice, using `find_all` with both `id` and `class_` attributes may not be particularly useful since the `id` should be unique. \n",
    "\n",
    "Recall that passing a list of tags to `.find_all()` via the attributes list acts as an “or” filter (it selects a list of all tags that have `tag1, tag2, or tag3...`). If you have a lengthy list of tags, you can end up with a lot of stuff you don’t want. The `keyword argument` allows you to add an additional “and” filter to this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9319fc8d-4f10-412d-82e9-40d3006d1481",
   "metadata": {},
   "source": [
    "## Keyword arguments and “class”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82784df-d5c4-49b7-9180-8d29a7223cef",
   "metadata": {},
   "source": [
    "The `keyword argument` can be helpful in some situations. However, it is technically redundant as a BeautifulSoup feature. Keep in mind that anything that can be done with keyword can also be accomplished using techniques covered later in this chapter (see `regular_express` and `lambda_express`).\n",
    "\n",
    "For instance, the following two lines are identical:\n",
    "\n",
    "```python\n",
    "bs.find_all(id='text')\n",
    "bs.find_all('', {'id':'text'})\n",
    "```\n",
    "\n",
    "_VR:_  \n",
    "Not true! You cannot leave `name=''`, you have to put the name of the tag: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3173efcf-c5dd-4306-8b2a-ea62ed27ad98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T15:16:15.262542Z",
     "iopub.status.busy": "2024-01-21T15:16:15.261981Z",
     "iopub.status.idle": "2024-01-21T15:16:16.089285Z",
     "shell.execute_reply": "2024-01-21T15:16:16.086287Z",
     "shell.execute_reply.started": "2024-01-21T15:16:15.262515Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    html = urlopen('http://www.pythonscraping.com/pages/warandpeace.html')\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    test2 = bs.find('', {'class': 'red'})\n",
    "\n",
    "    print(test2)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db931d4c-d0f8-4863-8200-669fa3d2489d",
   "metadata": {},
   "source": [
    "And this thing works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "291b2a98-a0d4-4a86-ae8f-cbb2a2cc8987",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T15:14:09.676559Z",
     "iopub.status.busy": "2024-01-21T15:14:09.674683Z",
     "iopub.status.idle": "2024-01-21T15:14:10.517722Z",
     "shell.execute_reply": "2024-01-21T15:14:10.512628Z",
     "shell.execute_reply.started": "2024-01-21T15:14:09.676438Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identical\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    html = urlopen('http://www.pythonscraping.com/pages/warandpeace.html')\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    test1 = bs.find_all(class_='red')\n",
    "    test2 = bs.find_all('span', {'class': 'red'})\n",
    "\n",
    "    for i, j in zip(test1, test2):\n",
    "        if i == j:\n",
    "            print(\"Identical\")\n",
    "        else:\n",
    "            print(\"Different\")\n",
    "        break\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85debb0-bbda-4198-ac8e-573778f130c9",
   "metadata": {},
   "source": [
    "In addition, you might occasionally run into problems using keyword, most notably when searching for elements by their `class` attribute, because `class` is a protected keyword in Python. That is, `class` is a reserved word in Python that cannot be used as a variable or argument name (no relation to the `BeautifulSoup.find_all()` keyword argument, previously discussed). For example, if you try the following call, you’ll get a syntax error due to the nonstandard use of class:\n",
    "\n",
    "```python\n",
    "bs.find_all(class='green')\n",
    "```\n",
    "\n",
    "Instead, you can use BeautifulSoup’s somewhat clumsy solution, which involves adding an underscore:\n",
    "```python\n",
    "bs.find_all(class_='green')\n",
    "```\n",
    "\n",
    "Alternatively, you can enclose class in quotes (_no, you cannot! you will have to provide `name='tagname'`, which is not needed in the first case._ - VR) :\n",
    "```python\n",
    "bs.find_all('', {'class':'green'})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4384e186-ce75-4398-a2a8-d16accc4c85a",
   "metadata": {},
   "source": [
    "# 2.2 Other `BeautifulSoup` Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3535652-bb47-4d32-802e-170fac72f18e",
   "metadata": {},
   "source": [
    "So far in the book, you’ve seen two types of objects in the BeautifulSoup library:\n",
    "- **BeautifulSoup objects** - `bs`, and\n",
    "- **Tag objects** - \n",
    "    - `bs.div.h1`, retrieved in lists, or \n",
    "    - retrieved individually by calling `find` and `find_all` on a BeautifulSoup object, or drilling down.\n",
    "\n",
    "However, there are two more objects in the library that, although less commonly used, are still important to know about:\n",
    "- **NavigableString objects** (\"NUH-vi-guh-buhl-string\") - used to represent text within tags, rather than the tags themselves (some functions operate on and produce `NavigableStrings`, rather than tag objects).\n",
    "- **Comment object** - used to find HTML comments in comment tags, `<!--like this one-->`.\n",
    "\n",
    "These four objects are the only objects you will ever encounter in the BeautifulSoup library (at the time of this writing).\n",
    "\n",
    "See details [here](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#kinds-of-objects)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a175350e-300d-4f00-bc19-56d842734b2f",
   "metadata": {},
   "source": [
    "# 2.3 Navigating Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e935c5b-a5ec-4789-86d4-fd7a996f84dd",
   "metadata": {},
   "source": [
    "The `find_all` function is responsible for finding tags based on their name and attributes. But \n",
    "\n",
    "> what if you need to find a tag based on its location in a document? \n",
    "\n",
    "That’s where tree navigation comes in handy. In **Chapter 1**, you looked at navigating a BeautifulSoup tree in a single direction:\n",
    "\n",
    "```html\n",
    "bs.tag.subTag.anotherSubTag\n",
    "```\n",
    "\n",
    "Now let’s look at navigating up, across, and diagonally through HTML trees. You’ll use our highly questionable online shopping site at http://www.pythonscraping.com/pages/page3.html.\n",
    "\n",
    "The HTML for this page, mapped out as a tree (with some tags omitted for brevity), looks like this:\n",
    "\n",
    "- HTML\n",
    "    - body\n",
    "        - div.wrapper\n",
    "            - h1\n",
    "            - div.content\n",
    "        - table#giftList\n",
    "            - tr\n",
    "                - th\n",
    "                - th\n",
    "                - th\n",
    "                - th\n",
    "            - tr.gift#gift1\n",
    "                - td\n",
    "                - td\n",
    "                    - span.excitingNote\n",
    "                - td\n",
    "                - td\n",
    "                    - img\n",
    "            - ...table rows continue...\n",
    "        - div.footer\n",
    "        \n",
    "You will use this same HTML structure as an example in the next few sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869fc79a-f033-48a9-a76d-9a02da3c9058",
   "metadata": {},
   "source": [
    "## Children and other descendants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0b74da-f0b4-4398-91b7-f2d199a64288",
   "metadata": {},
   "source": [
    "In the BeautifulSoup library, as well as many other libraries, there is a distinction drawn between **children** and **descendants**: \n",
    "- much like in a human family tree, children are always exactly one tag below a parent, whereas \n",
    "- descendants can be at any level in the tree below a parent. \n",
    "\n",
    "For example, the `tr` tags are children of the `table` tag, whereas `tr`, `th`, `td`, `img`, and `span` are all descendants of the `table` tag (at least in our example page). All children are descendants, but not all descendants are children.\n",
    "\n",
    "In general, BeautifulSoup functions always deal with the descendants of the current tag selected. For instance, `bs.body.h1` selects the first `h1` tag that is a descendant of the `body` tag. It will not find tags located outside the `body`.\n",
    "\n",
    "Similarly, `bs.div.find_all('img')` will find the first `div` tag in the document, and then retrieve a list of all `img` tags that are descendants of that `div` tag.\n",
    "\n",
    "If you want to find only descendants that are children, you can use the `.children` tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e54c2510-20eb-45e8-bbe7-fa05620115f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T19:17:32.468066Z",
     "iopub.status.busy": "2024-01-21T19:17:32.466709Z",
     "iopub.status.idle": "2024-01-21T19:17:33.477774Z",
     "shell.execute_reply": "2024-01-21T19:17:33.476503Z",
     "shell.execute_reply.started": "2024-01-21T19:17:32.468010Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<tr><th>\n",
      "Item Title\n",
      "</th><th>\n",
      "Description\n",
      "</th><th>\n",
      "Cost\n",
      "</th><th>\n",
      "Image\n",
      "</th></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift1\"><td>\n",
      "Vegetable Basket\n",
      "</td><td>\n",
      "This vegetable basket is the perfect gift for your health conscious (or overweight) friends!\n",
      "<span class=\"excitingNote\">Now with super-colorful bell peppers!</span>\n",
      "</td><td>\n",
      "$15.00\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img1.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift2\"><td>\n",
      "Russian Nesting Dolls\n",
      "</td><td>\n",
      "Hand-painted by trained monkeys, these exquisite dolls are priceless! And by \"priceless,\" we mean \"extremely expensive\"! <span class=\"excitingNote\">8 entire dolls per set! Octuple the presents!</span>\n",
      "</td><td>\n",
      "$10,000.52\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img2.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift3\"><td>\n",
      "Fish Painting\n",
      "</td><td>\n",
      "If something seems fishy about this painting, it's because it's a fish! <span class=\"excitingNote\">Also hand-painted by trained monkeys!</span>\n",
      "</td><td>\n",
      "$10,005.00\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img3.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift4\"><td>\n",
      "Dead Parrot\n",
      "</td><td>\n",
      "This is an ex-parrot! <span class=\"excitingNote\">Or maybe he's only resting?</span>\n",
      "</td><td>\n",
      "$0.50\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img4.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift5\"><td>\n",
      "Mystery Box\n",
      "</td><td>\n",
      "If you love suprises, this mystery box is for you! Do not place on light-colored surfaces. May cause oil staining. <span class=\"excitingNote\">Keep your friends guessing!</span>\n",
      "</td><td>\n",
      "$1.50\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img6.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def main():\n",
    "    html = urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    count = 0\n",
    "    for child in bs.find('table', {'id': 'giftList'}).children:\n",
    "        count += 1\n",
    "        print(child)\n",
    "\n",
    "    print(count)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f90e55d-9062-473c-8ba7-8f6dbe6b8c2c",
   "metadata": {},
   "source": [
    "This code prints the list of product rows in the `giftList` table, including the initial row of column labels. If you were to write it using the `descendants()` function instead of the `children()` function, about two dozen tags would be found within the table and printed, including `img` tags, `span` tags, and individual `td` tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf52f5d2-163f-4dad-90f1-abbaa0c4ee71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T19:18:21.694695Z",
     "iopub.status.busy": "2024-01-21T19:18:21.694115Z",
     "iopub.status.idle": "2024-01-21T19:18:22.550516Z",
     "shell.execute_reply": "2024-01-21T19:18:22.549356Z",
     "shell.execute_reply.started": "2024-01-21T19:18:21.694667Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def main():\n",
    "    html = urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    count = 0\n",
    "    for child in bs.find('table', {'id': 'giftList'}).descendants:\n",
    "        count += 1\n",
    "        # print(child)\n",
    "\n",
    "    print(count)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbab9e2-d895-4f9d-be8a-956d0d312237",
   "metadata": {},
   "source": [
    "## Siblings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd87407-ea0e-489b-9f7c-499842c02b57",
   "metadata": {},
   "source": [
    "### `next_siblings()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041e66ab-a75a-462c-bc80-b4013c7a8701",
   "metadata": {},
   "source": [
    "The BeautifulSoup `next_siblings()` function makes it trivial to collect data from tables, especially ones with title rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0382cc8-5d1c-401f-b3d2-59b7db3d792b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T19:22:11.849983Z",
     "iopub.status.busy": "2024-01-21T19:22:11.848576Z",
     "iopub.status.idle": "2024-01-21T19:22:12.660628Z",
     "shell.execute_reply": "2024-01-21T19:22:12.658338Z",
     "shell.execute_reply.started": "2024-01-21T19:22:11.849900Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift1\"><td>\n",
      "Vegetable Basket\n",
      "</td><td>\n",
      "This vegetable basket is the perfect gift for your health conscious (or overweight) friends!\n",
      "<span class=\"excitingNote\">Now with super-colorful bell peppers!</span>\n",
      "</td><td>\n",
      "$15.00\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img1.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift2\"><td>\n",
      "Russian Nesting Dolls\n",
      "</td><td>\n",
      "Hand-painted by trained monkeys, these exquisite dolls are priceless! And by \"priceless,\" we mean \"extremely expensive\"! <span class=\"excitingNote\">8 entire dolls per set! Octuple the presents!</span>\n",
      "</td><td>\n",
      "$10,000.52\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img2.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift3\"><td>\n",
      "Fish Painting\n",
      "</td><td>\n",
      "If something seems fishy about this painting, it's because it's a fish! <span class=\"excitingNote\">Also hand-painted by trained monkeys!</span>\n",
      "</td><td>\n",
      "$10,005.00\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img3.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift4\"><td>\n",
      "Dead Parrot\n",
      "</td><td>\n",
      "This is an ex-parrot! <span class=\"excitingNote\">Or maybe he's only resting?</span>\n",
      "</td><td>\n",
      "$0.50\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img4.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift5\"><td>\n",
      "Mystery Box\n",
      "</td><td>\n",
      "If you love suprises, this mystery box is for you! Do not place on light-colored surfaces. May cause oil staining. <span class=\"excitingNote\">Keep your friends guessing!</span>\n",
      "</td><td>\n",
      "$1.50\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img6.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def main():\n",
    "    html = urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
    "    bs = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    for sibling in bs.find('table', {'id': 'giftList'}).tr.next_siblings:\n",
    "        print(sibling)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c776d3c-f382-4d29-9539-bb3ea4fdf778",
   "metadata": {},
   "source": [
    "The output of this code is to print all rows of products from the product table, except for the first title row. \n",
    "\n",
    "> Why does the title row get skipped? \n",
    "\n",
    "Objects cannot be siblings with themselves. Anytime you get siblings of an object, the object itself will not be included in the list. As the name of the function implies, it calls next siblings only. If you were to select a row in the middle of the list, for example, and call `next_siblings` on it, only the subsequent siblings would be returned. So, by selecting the title row and calling `next_siblings`, you can select all the rows in the table, without selecting the title row itself.\n",
    "\n",
    "> MAKE SELECTIONS SPECIFIC</br>\n",
    "</br>The preceding code will work just as well, if you select bs.table.tr or even just bs.tr in order to select the first row of the table. However, in the code, I go through all of the trouble of writing everything out in a longer form:</br>\n",
    "</br>`bs.find('table',{'id':'giftList'}).tr`</br>\n",
    "</br>Even if it looks like there’s just one table (or other target tag) on the page, it’s easy to miss things. In addition, page layouts change all the time. What was once the first of its kind on the page might someday be the second or third tag of that type found on the page. To make your scrapers more robust, it’s best to be as specific as possible when making tag selections. **Take advantage of tag attributes when they are available.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2425e368-c962-4fef-bfb4-f1bf57647711",
   "metadata": {},
   "source": [
    "### `previous_siblings()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c91d6e6-1894-4400-992f-942a133d3523",
   "metadata": {},
   "source": [
    "The `previous_siblings` function can often be helpful if there is an easily selectable tag at the end of a list of sibling tags that you would like to get.\n",
    "\n",
    "And, of course, there are the `next_sibling` and `previous_sibling` functions, which perform nearly the same function as `next_siblings` and `previous_siblings`, except they return a single tag rather than a list of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075821f3-635a-428a-9fa6-f260e1d2dc4b",
   "metadata": {},
   "source": [
    "## Parents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4090dc-3089-40b7-ae6e-8e0b0f2990cd",
   "metadata": {},
   "source": [
    "When scraping pages, you will likely discover that you need to find parents of tags less frequently than you need to find their children or siblings. Typically, when you look at HTML pages with the goal of crawling them, you start by looking at the top layer of tags, and then figure out how to drill your way down into the exact piece of data that you want. Occasionally, however, you can find yourself in odd situations that require BeautifulSoup’s parent-finding functions, `.parent` and `.parents`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c22bdba-6b25-4881-a79e-7fc81a88d562",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T19:42:23.002213Z",
     "iopub.status.busy": "2024-01-21T19:42:23.000599Z",
     "iopub.status.idle": "2024-01-21T19:42:23.989702Z",
     "shell.execute_reply": "2024-01-21T19:42:23.986736Z",
     "shell.execute_reply.started": "2024-01-21T19:42:23.002149Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "$15.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def main():\n",
    "    html = urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
    "    bs = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    print(bs.find(\n",
    "            'img', {'src': '../img/gifts/img1.jpg'}\n",
    "                ).parent.previous_sibling.get_text())\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fce609a-badb-4ee7-b1d8-a4b4cf32ef35",
   "metadata": {},
   "source": [
    "This code will print the price of the object represented by the image at the location `../img/gifts/img1.jpg` (in this case, the price is `$15.00`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81788b9d-2b2d-4d58-b2dc-333b1cb0217b",
   "metadata": {},
   "source": [
    "![](./data/images/Screenshot_20240122_004505.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d324a4cf-c5cd-42fc-aba6-245edc7b8d8c",
   "metadata": {},
   "source": [
    "1. The image tag where `src=\"../img/gifts/img1.jpg\"` is first selected.\n",
    "1. You select the parent of that tag (in this case, the `td` tag).\n",
    "1. You select the `previous_sibling` of the `td` tag (in this case, the `td` tag that contains the dollar value of the product).\n",
    "1. You select the text within that tag, `“$15.00”`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6ec00d-7f50-4c5e-981a-63763005ad5f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9045d0b1-1b03-4e04-bb80-0a73f74a0389",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1c7a626-c584-4d16-97f3-e3fc7aecb5ed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8117f083-3f39-4f6e-ad28-c5152d144c94",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e177307a-13aa-4bca-a26d-cfb4623c0cee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4b0231a-6a4c-4071-9723-d4b37a218df5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "759d37e9-7534-440b-bbd2-e862c2a1cc2c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f41cb3e2-2f69-4474-9eb9-a91fe2fae0db",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3787583-52f3-4f23-b771-fca08575d5b8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a346f9cf-7a69-44d0-a35a-c380526c0d89",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a68e8dc8-f91e-4ee2-818b-822eb79ea4eb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e304fe16-d8c9-420c-b97f-97ad8517b283",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd66e21e-d02c-4d61-9253-c45a6d5bfb7f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37ca01c1-d26c-4b9c-b6fa-e4a85b6af777",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6bdb847-f558-4f35-8289-572a23dab753",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "797c5c05-05fc-45f9-8b78-7085606bbd7f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e155417-f8b5-4462-a496-c5885e0e7451",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd0dd81e-8e4f-4c92-a398-cdee8131f147",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49f3c89f-889d-40ea-84ba-b499c57a3396",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb664e44-3b5f-422a-971d-dbbc161c0751",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1ab418d-26fd-4225-a7dc-4b42364edacd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b92a6c5-4a92-4faa-9d1a-6da89bf2cb1f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d91d975e-e021-4681-9fe1-065b520580ce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bf4e963-bf83-4b62-bc70-16d0146f6a32",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e10554e0-1232-499a-8b48-5059799cdddd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b77cb236-c0c4-406a-94a0-8a9ab77644ed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9de846c8-cac4-419e-8c5e-7df98228537f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cea22d7-63f2-427d-bd4b-6299db7ba403",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "382213a6-5cd0-45bc-b835-1ae6f4609ea6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d2f8c4b-d09b-49a1-9eb7-989c50b7b5fe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf8b8da3-f82e-47f9-8287-20e8b973e10b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f1caf49-49fe-4578-9f83-321634b185d5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a6e5b58-b300-4c9e-b2bd-3c0be981b20d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1b7c048-1689-4bf5-91f1-ba7505414ca3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc150350-aa93-4464-9c20-55ad8adabec4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "123ebe98-f03c-4b0f-b974-d8ccca46ffaa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d27a0f3-bbf4-4e5e-9024-3e5bbe80a771",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab8d069a-2544-4071-bcee-19e32a7e2e5a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1527fc2-d7e8-4cec-ad81-5bf1f2097b51",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5953872-2eae-4309-a263-354f0b881ca6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf75f2be-48be-459d-be16-6ff4631c8e78",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "746e92f3-f1fe-4926-9b22-1f01e66e00ed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf218179-3b29-4d58-bc46-0efcd27688e2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f912ae86-c106-4c3f-92b1-f9b3c09a06a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dac73d1c-79ce-4a57-be0a-6d89a3ad4860",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42330caa-9343-4cd5-8344-20a845f8ac08",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c00f720-29eb-4813-8c23-ec34ca6ed8e7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e418846-9870-4773-bc9b-8cc8fef4f7e0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cdd92a7-f637-4a16-9bc0-649bf38fa983",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b120d83d-1ff9-4839-bf20-3e2857788f8c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f754663-fcab-4eb1-a08f-5dd33f2fb1e7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5959a5e7-1ba9-471a-ba1e-c97bc6abe58e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4294c722-5785-4b4b-a55f-073d0f83aa71",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a92e83c-c5c5-4c52-89dd-019ec17ab2df",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "219de4d9-1c11-4e16-b7d3-38681bdcde9d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e29177f-3f2e-4fe7-bd59-6c4c5b19bd03",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24954f52-ac63-4efc-98a7-65c3d9f159a6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f393e081-2784-4186-8587-ccd38f2e7c5f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36407e8a-f9d8-483e-b7cb-ee990e9d3ce2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a781645-1e77-44bd-8111-68737a63d337",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a14f0b4-47e0-4733-9f29-dc2fd758570e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e8fbafc-5ef1-46a5-b060-0d9c0e5d7463",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da5182e4-4842-4e73-8881-26f54aa9390d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4b2359f-ba37-4a72-82ff-e7095416318c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "397c9395-1081-4e60-b4e5-aace93ab7218",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c79e117d-222d-41d7-8519-93060aa38b7a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b538d502-3d5d-4911-a2c3-888ed7719ee4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0618675f-7b7f-44e5-9312-6a803e4140c0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d4e175b-5775-4b7a-9890-e2c0ee3dc402",
   "metadata": {},
   "source": [
    "# <b>Additional</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b782ff3-f8ce-4c2e-8bd9-07f91b234d1e",
   "metadata": {},
   "source": [
    "|bash|description|\n",
    "|-|-|\n",
    "|`scrapy startproject <name>`|start a new scrapy project|\n",
    "|`scrapy genspider <spider_name> <domain>`|generate a spider in the `spider` dir|\n",
    "|`scrapy runspider <spider_file>.py`|start the crawler|\n",
    "|||\n",
    "|||\n",
    "|||"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd4821c-a2c9-4874-9578-b23ad9d75c28",
   "metadata": {},
   "source": [
    "# 3. Creating a Scrapy project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e5340e-a8d7-401a-99c4-2c6a0caf7358",
   "metadata": {},
   "source": [
    "You should work in the virtual environment.\n",
    "\n",
    "```sh\n",
    "pip install --upgrade pip\n",
    "pip install scrapy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b8b505-df5b-4b02-8027-0633c47ec204",
   "metadata": {},
   "source": [
    "A **spider** is a Scrapy project that, like its arachnid namesake, is designed to crawl webs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab2c4b8-8321-4978-9afc-f3778f508549",
   "metadata": {},
   "source": [
    "```sh\n",
    "$ scrapy startproject test1\n",
    "New Scrapy project 'test1', using template directory '/home/commi/venv/venv3.12/lib/python3.12/site-packages/scrapy/templates/project', created in:\n",
    "    /home/commi/Yandex.Disk/it_learning/08_parsing_data/data/test1\n",
    "\n",
    "You can start your first spider with:\n",
    "    cd test1\n",
    "    scrapy genspider example example.com\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c9a66a-1315-4604-9403-ee2ab3fd8e43",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Project dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f35688a-9383-4ac6-b7ea-e9bda127aade",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:30:54.703573Z",
     "iopub.status.busy": "2024-01-15T13:30:54.703002Z",
     "iopub.status.idle": "2024-01-15T13:30:54.808659Z",
     "shell.execute_reply": "2024-01-15T13:30:54.807658Z",
     "shell.execute_reply.started": "2024-01-15T13:30:54.703536Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cd /home/commi/Yandex.Disk/it_learning/08_parsing_data/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e38bec3b-71ae-4227-ab99-3851ad5cfd9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:30:55.263463Z",
     "iopub.status.busy": "2024-01-15T13:30:55.262717Z",
     "iopub.status.idle": "2024-01-15T13:30:55.381147Z",
     "shell.execute_reply": "2024-01-15T13:30:55.380041Z",
     "shell.execute_reply.started": "2024-01-15T13:30:55.263422Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtest1\u001b[0m\n",
      "├── scrapy.cfg\n",
      "└── \u001b[01;34mtest1\u001b[0m\n",
      "    ├── __init__.py\n",
      "    ├── items.py\n",
      "    ├── middlewares.py\n",
      "    ├── pipelines.py\n",
      "    ├── settings.py\n",
      "    └── \u001b[01;34mspiders\u001b[0m\n",
      "        └── __init__.py\n",
      "\n",
      "3 directories, 7 files\n"
     ]
    }
   ],
   "source": [
    "tree test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2ba45058-35a5-481c-9727-d97f30abcb79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:30:57.259154Z",
     "iopub.status.busy": "2024-01-15T13:30:57.258366Z",
     "iopub.status.idle": "2024-01-15T13:30:57.408565Z",
     "shell.execute_reply": "2024-01-15T13:30:57.407181Z",
     "shell.execute_reply.started": "2024-01-15T13:30:57.259088Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Automatically created by: scrapy startproject\n",
      "#\n",
      "# For more information about the [deploy] section see:\n",
      "# https://scrapyd.readthedocs.io/en/latest/deploy.html\n",
      "\n",
      "[settings]\n",
      "default = test1.settings\n",
      "\n",
      "[deploy]\n",
      "#url = http://localhost:6800/\n",
      "project = test1\n"
     ]
    }
   ],
   "source": [
    "cat test1/scrapy.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd4c637-61f2-4121-8594-8c821279fd75",
   "metadata": {},
   "source": [
    "### Deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e0aed8cd-a851-4c16-9d5c-268a8a05f9cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:30:59.654728Z",
     "iopub.status.busy": "2024-01-15T13:30:59.653435Z",
     "iopub.status.idle": "2024-01-15T13:30:59.765945Z",
     "shell.execute_reply": "2024-01-15T13:30:59.764669Z",
     "shell.execute_reply.started": "2024-01-15T13:30:59.654676Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtest1/test1\u001b[0m\n",
      "├── __init__.py\n",
      "├── items.py\n",
      "├── middlewares.py\n",
      "├── pipelines.py\n",
      "├── settings.py\n",
      "└── \u001b[01;34mspiders\u001b[0m\n",
      "    └── __init__.py\n",
      "\n",
      "2 directories, 6 files\n"
     ]
    }
   ],
   "source": [
    "tree test1/test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "abbefcac-efcb-4e21-ab76-50ccf5437ef5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:31:02.723037Z",
     "iopub.status.busy": "2024-01-15T13:31:02.722598Z",
     "iopub.status.idle": "2024-01-15T13:31:02.836435Z",
     "shell.execute_reply": "2024-01-15T13:31:02.835624Z",
     "shell.execute_reply.started": "2024-01-15T13:31:02.723003Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Define here the models for your scraped items\n",
      "#\n",
      "# See documentation in:\n",
      "# https://docs.scrapy.org/en/latest/topics/items.html\n",
      "\n",
      "import scrapy\n",
      "\n",
      "\n",
      "class Test1Item(scrapy.Item):\n",
      "    # define the fields for your item here like:\n",
      "    # name = scrapy.Field()\n",
      "    pass\n"
     ]
    }
   ],
   "source": [
    "cat test1/test1/items.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "08b14b2f-e21c-47c4-906b-17bffe22852e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:31:05.807978Z",
     "iopub.status.busy": "2024-01-15T13:31:05.807297Z",
     "iopub.status.idle": "2024-01-15T13:31:06.006539Z",
     "shell.execute_reply": "2024-01-15T13:31:06.005002Z",
     "shell.execute_reply.started": "2024-01-15T13:31:05.807945Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Define here the models for your spider middleware\n",
      "#\n",
      "# See documentation in:\n",
      "# https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
      "\n",
      "from scrapy import signals\n",
      "\n",
      "# useful for handling different item types with a single interface\n",
      "from itemadapter import is_item, ItemAdapter\n",
      "\n",
      "\n",
      "class Test1SpiderMiddleware:\n",
      "    # Not all methods need to be defined. If a method is not defined,\n",
      "    # scrapy acts as if the spider middleware does not modify the\n",
      "    # passed objects.\n",
      "\n",
      "    @classmethod\n",
      "    def from_crawler(cls, crawler):\n",
      "        # This method is used by Scrapy to create your spiders.\n",
      "        s = cls()\n",
      "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
      "        return s\n",
      "\n",
      "    def process_spider_input(self, response, spider):\n",
      "        # Called for each response that goes through the spider\n",
      "        # middleware and into the spider.\n",
      "\n",
      "        # Should return None or raise an exception.\n",
      "        return None\n",
      "\n",
      "    def process_spider_output(self, response, result, spider):\n",
      "        # Called with the results returned from the Spider, after\n",
      "        # it has processed the response.\n",
      "\n",
      "        # Must return an iterable of Request, or item objects.\n",
      "        for i in result:\n",
      "            yield i\n",
      "\n",
      "    def process_spider_exception(self, response, exception, spider):\n",
      "        # Called when a spider or process_spider_input() method\n",
      "        # (from other spider middleware) raises an exception.\n",
      "\n",
      "        # Should return either None or an iterable of Request or item objects.\n",
      "        pass\n",
      "\n",
      "    def process_start_requests(self, start_requests, spider):\n",
      "        # Called with the start requests of the spider, and works\n",
      "        # similarly to the process_spider_output() method, except\n",
      "        # that it doesn’t have a response associated.\n",
      "\n",
      "        # Must return only requests (not items).\n",
      "        for r in start_requests:\n",
      "            yield r\n",
      "\n",
      "    def spider_opened(self, spider):\n",
      "        spider.logger.info(\"Spider opened: %s\" % spider.name)\n",
      "\n",
      "\n",
      "class Test1DownloaderMiddleware:\n",
      "    # Not all methods need to be defined. If a method is not defined,\n",
      "    # scrapy acts as if the downloader middleware does not modify the\n",
      "    # passed objects.\n",
      "\n",
      "    @classmethod\n",
      "    def from_crawler(cls, crawler):\n",
      "        # This method is used by Scrapy to create your spiders.\n",
      "        s = cls()\n",
      "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
      "        return s\n",
      "\n",
      "    def process_request(self, request, spider):\n",
      "        # Called for each request that goes through the downloader\n",
      "        # middleware.\n",
      "\n",
      "        # Must either:\n",
      "        # - return None: continue processing this request\n",
      "        # - or return a Response object\n",
      "        # - or return a Request object\n",
      "        # - or raise IgnoreRequest: process_exception() methods of\n",
      "        #   installed downloader middleware will be called\n",
      "        return None\n",
      "\n",
      "    def process_response(self, request, response, spider):\n",
      "        # Called with the response returned from the downloader.\n",
      "\n",
      "        # Must either;\n",
      "        # - return a Response object\n",
      "        # - return a Request object\n",
      "        # - or raise IgnoreRequest\n",
      "        return response\n",
      "\n",
      "    def process_exception(self, request, exception, spider):\n",
      "        # Called when a download handler or a process_request()\n",
      "        # (from other downloader middleware) raises an exception.\n",
      "\n",
      "        # Must either:\n",
      "        # - return None: continue processing this exception\n",
      "        # - return a Response object: stops process_exception() chain\n",
      "        # - return a Request object: stops process_exception() chain\n",
      "        pass\n",
      "\n",
      "    def spider_opened(self, spider):\n",
      "        spider.logger.info(\"Spider opened: %s\" % spider.name)\n"
     ]
    }
   ],
   "source": [
    "cat test1/test1/middlewares.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "68a6820f-f86a-4468-9837-125c9f3971d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:31:08.926648Z",
     "iopub.status.busy": "2024-01-15T13:31:08.925824Z",
     "iopub.status.idle": "2024-01-15T13:31:09.038088Z",
     "shell.execute_reply": "2024-01-15T13:31:09.036692Z",
     "shell.execute_reply.started": "2024-01-15T13:31:08.926579Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Define your item pipelines here\n",
      "#\n",
      "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
      "# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
      "\n",
      "\n",
      "# useful for handling different item types with a single interface\n",
      "from itemadapter import ItemAdapter\n",
      "\n",
      "\n",
      "class Test1Pipeline:\n",
      "    def process_item(self, item, spider):\n",
      "        return item\n"
     ]
    }
   ],
   "source": [
    "cat test1/test1/pipelines.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b59259f4-a695-4a77-8386-73bd2a16ba9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:31:09.533481Z",
     "iopub.status.busy": "2024-01-15T13:31:09.532629Z",
     "iopub.status.idle": "2024-01-15T13:31:09.721049Z",
     "shell.execute_reply": "2024-01-15T13:31:09.719769Z",
     "shell.execute_reply.started": "2024-01-15T13:31:09.533446Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Scrapy settings for test1 project\n",
      "#\n",
      "# For simplicity, this file contains only settings considered important or\n",
      "# commonly used. You can find more settings consulting the documentation:\n",
      "#\n",
      "#     https://docs.scrapy.org/en/latest/topics/settings.html\n",
      "#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
      "#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
      "\n",
      "BOT_NAME = \"test1\"\n",
      "\n",
      "SPIDER_MODULES = [\"test1.spiders\"]\n",
      "NEWSPIDER_MODULE = \"test1.spiders\"\n",
      "\n",
      "\n",
      "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
      "#USER_AGENT = \"test1 (+http://www.yourdomain.com)\"\n",
      "\n",
      "# Obey robots.txt rules\n",
      "ROBOTSTXT_OBEY = True\n",
      "\n",
      "# Configure maximum concurrent requests performed by Scrapy (default: 16)\n",
      "#CONCURRENT_REQUESTS = 32\n",
      "\n",
      "# Configure a delay for requests for the same website (default: 0)\n",
      "# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n",
      "# See also autothrottle settings and docs\n",
      "#DOWNLOAD_DELAY = 3\n",
      "# The download delay setting will honor only one of:\n",
      "#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n",
      "#CONCURRENT_REQUESTS_PER_IP = 16\n",
      "\n",
      "# Disable cookies (enabled by default)\n",
      "#COOKIES_ENABLED = False\n",
      "\n",
      "# Disable Telnet Console (enabled by default)\n",
      "#TELNETCONSOLE_ENABLED = False\n",
      "\n",
      "# Override the default request headers:\n",
      "#DEFAULT_REQUEST_HEADERS = {\n",
      "#    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
      "#    \"Accept-Language\": \"en\",\n",
      "#}\n",
      "\n",
      "# Enable or disable spider middlewares\n",
      "# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
      "#SPIDER_MIDDLEWARES = {\n",
      "#    \"test1.middlewares.Test1SpiderMiddleware\": 543,\n",
      "#}\n",
      "\n",
      "# Enable or disable downloader middlewares\n",
      "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
      "#DOWNLOADER_MIDDLEWARES = {\n",
      "#    \"test1.middlewares.Test1DownloaderMiddleware\": 543,\n",
      "#}\n",
      "\n",
      "# Enable or disable extensions\n",
      "# See https://docs.scrapy.org/en/latest/topics/extensions.html\n",
      "#EXTENSIONS = {\n",
      "#    \"scrapy.extensions.telnet.TelnetConsole\": None,\n",
      "#}\n",
      "\n",
      "# Configure item pipelines\n",
      "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
      "#ITEM_PIPELINES = {\n",
      "#    \"test1.pipelines.Test1Pipeline\": 300,\n",
      "#}\n",
      "\n",
      "# Enable and configure the AutoThrottle extension (disabled by default)\n",
      "# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n",
      "#AUTOTHROTTLE_ENABLED = True\n",
      "# The initial download delay\n",
      "#AUTOTHROTTLE_START_DELAY = 5\n",
      "# The maximum download delay to be set in case of high latencies\n",
      "#AUTOTHROTTLE_MAX_DELAY = 60\n",
      "# The average number of requests Scrapy should be sending in parallel to\n",
      "# each remote server\n",
      "#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n",
      "# Enable showing throttling stats for every response received:\n",
      "#AUTOTHROTTLE_DEBUG = False\n",
      "\n",
      "# Enable and configure HTTP caching (disabled by default)\n",
      "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n",
      "#HTTPCACHE_ENABLED = True\n",
      "#HTTPCACHE_EXPIRATION_SECS = 0\n",
      "#HTTPCACHE_DIR = \"httpcache\"\n",
      "#HTTPCACHE_IGNORE_HTTP_CODES = []\n",
      "#HTTPCACHE_STORAGE = \"scrapy.extensions.httpcache.FilesystemCacheStorage\"\n",
      "\n",
      "# Set settings whose default value is deprecated to a future-proof value\n",
      "REQUEST_FINGERPRINTER_IMPLEMENTATION = \"2.7\"\n",
      "TWISTED_REACTOR = \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n",
      "FEED_EXPORT_ENCODING = \"utf-8\"\n"
     ]
    }
   ],
   "source": [
    "cat test1/test1/settings.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5853d2-e675-45a1-884b-c2473f1c96cd",
   "metadata": {},
   "source": [
    "### Even deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "65300daf-62e3-43c5-8e4c-603123ce7bf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:31:15.439231Z",
     "iopub.status.busy": "2024-01-15T13:31:15.438518Z",
     "iopub.status.idle": "2024-01-15T13:31:15.547558Z",
     "shell.execute_reply": "2024-01-15T13:31:15.546528Z",
     "shell.execute_reply.started": "2024-01-15T13:31:15.439172Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtest1/test1/spiders\u001b[0m\n",
      "└── __init__.py\n",
      "\n",
      "1 directory, 1 file\n"
     ]
    }
   ],
   "source": [
    "tree test1/test1/spiders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e0cdb6af-f380-4379-92cb-cafdc242794d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:31:19.282686Z",
     "iopub.status.busy": "2024-01-15T13:31:19.281995Z",
     "iopub.status.idle": "2024-01-15T13:31:19.394365Z",
     "shell.execute_reply": "2024-01-15T13:31:19.393434Z",
     "shell.execute_reply.started": "2024-01-15T13:31:19.282629Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# This package will contain the spiders of your Scrapy project\n",
      "#\n",
      "# Please refer to the documentation for information on how to create and manage\n",
      "# your spiders.\n"
     ]
    }
   ],
   "source": [
    "cat test1/test1/spiders/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119e9ee5-08d9-403c-828b-28543fc41dd8",
   "metadata": {},
   "source": [
    "# 4. Write a Simple Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd957a70-b5cc-4db5-9d57-b6c5fb2f65d1",
   "metadata": {},
   "source": [
    "To create a crawler, you will add a new file inside the spiders directory at test1/test1/spiders/bookspider.py."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd1f15f-df3d-4899-ba0f-4881519966b2",
   "metadata": {},
   "source": [
    "```sh\n",
    "$ cd test1/test1/spiders/\n",
    "$ scrapy genspider bookspider books.toscrape.com\n",
    "```\n",
    "```\n",
    "Created spider 'bookspider' using template 'basic' in module:\n",
    "  test1.spiders.bookspider\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fc5b19cd-0d50-4b28-a16c-7241b19ae388",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:31:48.403396Z",
     "iopub.status.busy": "2024-01-15T13:31:48.402628Z",
     "iopub.status.idle": "2024-01-15T13:31:48.514078Z",
     "shell.execute_reply": "2024-01-15T13:31:48.512387Z",
     "shell.execute_reply.started": "2024-01-15T13:31:48.403331Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mtest1/test1/spiders/\u001b[0m\n",
      "├── bookspider.py\n",
      "├── __init__.py\n",
      "└── \u001b[01;34m__pycache__\u001b[0m\n",
      "    └── __init__.cpython-312.pyc\n",
      "\n",
      "2 directories, 3 files\n"
     ]
    }
   ],
   "source": [
    "tree test1/test1/spiders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f67a0079-49f9-46ff-a7d6-2ceec81aa424",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T13:32:14.207312Z",
     "iopub.status.busy": "2024-01-15T13:32:14.206579Z",
     "iopub.status.idle": "2024-01-15T13:32:14.318134Z",
     "shell.execute_reply": "2024-01-15T13:32:14.315643Z",
     "shell.execute_reply.started": "2024-01-15T13:32:14.207255Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import scrapy\n",
      "\n",
      "\n",
      "class BookspiderSpider(scrapy.Spider):\n",
      "    name = \"bookspider\"\n",
      "    allowed_domains = [\"books.toscrape.com\"]\n",
      "    start_urls = [\"https://books.toscrape.com\"]\n",
      "\n",
      "    def parse(self, response):\n",
      "        pass\n"
     ]
    }
   ],
   "source": [
    "cat test1/test1/spiders/bookspider.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777bb048-6f59-479c-81ff-130adf5bc8fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4213a5c4-2996-4b87-9dcb-887aafe38278",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b408de-deab-4ba8-8d37-4c04789dc630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ece7ef9-b6d5-43ea-b027-39bb2b5e4488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1ac3a9-e8d6-4a32-b2e1-34987c6d60da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012ea6a7-ef5f-4d68-a79e-21ef9fdcb594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56f6207-f44d-428c-b89d-46b74bc86728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a10f5e-a1cc-4061-869d-395ab8d6c579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fca0a4-ecaf-4fc6-8cb0-42ab2e5a4e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06223cab-2047-442e-aafa-94952089e7e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3431da-8015-4f6f-9cd1-cee174f5bd4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1729e66-af10-411c-8320-fe88f49d6bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d26c8d3-a7ec-4d03-9810-0d8d377f676b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c23e6c-adcf-4686-89d7-3542609759b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3bcc53-7bfb-4bd8-b743-e1e356cf8452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999ea350-415e-4e91-8a66-e34e126d2fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec79599-7086-44f5-80e0-6a1f041bb59f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fcbe05-5959-4a73-9f07-5e2c2f4f2d08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604828cd-83f1-4725-a606-1beedcd71d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84a20a8-cb07-4354-89d7-dc94b60bc377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14e5940-28c8-44fe-aa49-ca5d4cbd3e81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7fdddb-f1d2-48b6-ae3e-ba6d438dc5b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704d9191-4a10-4d13-adbe-0b6e3de40e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335d1ae0-ffce-4871-8b0a-186239a52998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d011a9-70d8-4b84-8a04-f715098b868e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc66afc-07a9-4476-b195-678183e38edd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e5cab7-6292-4118-ac7f-040d31b5d573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43193bc4-fd7e-4420-8e4a-ccffa864a10c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44f82da-b4ac-4a65-bce0-2ae307652d82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986bfc11-bcf1-463c-b057-fd4725098d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24217a6-f119-4c08-afc5-ded77ed85819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bdfdf2-c6a6-4295-bebd-0fab657c0905",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4e4c4e-f1c7-4651-8323-81b73a6ca3a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a439272-4f68-4dd6-b896-689f5f5c5b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3.12",
   "language": "python",
   "name": "venv3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
